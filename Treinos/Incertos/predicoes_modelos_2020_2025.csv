treino_id,unique_id,ds,y,y_pred,diferença_%,flag,dataset,modelo,comentario,data_treino
cluster 0 (2020-225),Agua_Boa,2012-12-31T00:00:00,1.12,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Agua_Boa,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Agua_Boa,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Agua_Boa,2015-12-31T00:00:00,1.08,1.1613385677337646,7.53,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Agua_Boa,2016-12-31T00:00:00,1.08,1.0853763818740845,0.5,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Agua_Boa,2017-12-31T00:00:00,1.11,1.0237057209014893,7.77,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Agua_Boa,2018-12-31T00:00:00,1.2,1.1119346618652344,7.34,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Aguas_Vermelhas,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Aguas_Vermelhas,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Aguas_Vermelhas,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Aguas_Vermelhas,2015-12-31T00:00:00,1.2,1.6296582221984863,35.8,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Aguas_Vermelhas,2016-12-31T00:00:00,3.6,1.5317291021347046,57.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Aguas_Vermelhas,2017-12-31T00:00:00,2.6,1.8789385557174683,27.73,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Aguas_Vermelhas,2018-12-31T00:00:00,3.0,2.7351768016815186,8.83,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Alto_Caparao,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Alto_Caparao,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Alto_Caparao,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Alto_Caparao,2015-12-31T00:00:00,1.2,1.551392674446106,29.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Alto_Caparao,2016-12-31T00:00:00,2.1,1.5467698574066162,26.34,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Alto_Caparao,2017-12-31T00:00:00,1.37,1.48233962059021,8.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Alto_Caparao,2018-12-31T00:00:00,1.5,2.1673660278320312,44.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Araponga,2012-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Araponga,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Araponga,2014-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Araponga,2015-12-31T00:00:00,1.26,1.303446650505066,3.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Araponga,2016-12-31T00:00:00,1.32,1.2809624671936035,2.96,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Araponga,2017-12-31T00:00:00,1.5,1.180795669555664,21.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Araponga,2018-12-31T00:00:00,1.38,1.5133922100067139,9.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Areado,2012-12-31T00:00:00,1.95,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Areado,2013-12-31T00:00:00,1.86,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Areado,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Areado,2015-12-31T00:00:00,1.68,1.783054232597351,6.13,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Areado,2016-12-31T00:00:00,2.04,1.8554866313934326,9.04,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Areado,2017-12-31T00:00:00,1.68,1.4736663103103638,12.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Areado,2018-12-31T00:00:00,1.5,1.7629481554031372,17.53,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ataleia,2012-12-31T00:00:00,0.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ataleia,2013-12-31T00:00:00,0.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ataleia,2014-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ataleia,2015-12-31T00:00:00,0.91,0.7653098702430725,15.9,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ataleia,2016-12-31T00:00:00,0.91,0.8738313317298889,3.97,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ataleia,2017-12-31T00:00:00,0.65,0.7810875177383423,20.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ataleia,2018-12-31T00:00:00,0.9,0.9723994135856628,8.04,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Bom_Jesus_do_Galho,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Bom_Jesus_do_Galho,2013-12-31T00:00:00,0.78,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Bom_Jesus_do_Galho,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Bom_Jesus_do_Galho,2015-12-31T00:00:00,0.96,0.9059910774230957,5.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Bom_Jesus_do_Galho,2016-12-31T00:00:00,1.8,0.9040595293045044,49.77,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Bom_Jesus_do_Galho,2017-12-31T00:00:00,0.82,1.1387994289398193,38.88,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Bom_Jesus_do_Galho,2018-12-31T00:00:00,1.2,2.112943172454834,76.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caiana,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caiana,2013-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caiana,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caiana,2015-12-31T00:00:00,0.96,1.1097357273101807,15.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caiana,2016-12-31T00:00:00,1.2,1.1595934629440308,3.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caiana,2017-12-31T00:00:00,1.2,1.1082786321640015,7.64,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caiana,2018-12-31T00:00:00,1.5,1.3957287073135376,6.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Cajuri,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Cajuri,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Cajuri,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Cajuri,2015-12-31T00:00:00,1.32,1.7812888622283936,34.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Cajuri,2016-12-31T00:00:00,1.2,1.2234054803848267,1.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Cajuri,2017-12-31T00:00:00,1.2,1.2740089893341064,6.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Cajuri,2018-12-31T00:00:00,1.8,1.2955607175827026,28.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Canaa,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Canaa,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Canaa,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Canaa,2015-12-31T00:00:00,1.02,1.2933344841003418,26.8,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Canaa,2016-12-31T00:00:00,1.2,1.2068477869033813,0.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Canaa,2017-12-31T00:00:00,1.5,1.116276741027832,25.58,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Canaa,2018-12-31T00:00:00,1.5,1.5491564273834229,3.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caparao,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caparao,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caparao,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caparao,2015-12-31T00:00:00,1.02,1.4285130500793457,40.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caparao,2016-12-31T00:00:00,1.92,1.2291135787963867,35.98,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caparao,2017-12-31T00:00:00,1.45,1.267555832862854,12.58,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caparao,2018-12-31T00:00:00,1.8,1.9903125762939453,10.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caputira,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caputira,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caputira,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caputira,2015-12-31T00:00:00,1.5,1.3658119440078735,8.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caputira,2016-12-31T00:00:00,1.26,1.4076271057128906,11.72,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caputira,2017-12-31T00:00:00,0.77,1.2882719039916992,67.31,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caputira,2018-12-31T00:00:00,1.2,1.627044677734375,35.59,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carai,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carai,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carai,2014-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carai,2015-12-31T00:00:00,0.96,0.9015203714370728,6.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carai,2016-12-31T00:00:00,0.96,0.982577383518219,2.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carai,2017-12-31T00:00:00,0.54,0.784501850605011,45.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carai,2018-12-31T00:00:00,0.84,1.103897213935852,31.42,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carangola,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carangola,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carangola,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carangola,2015-12-31T00:00:00,0.9,1.0136502981185913,12.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carangola,2016-12-31T00:00:00,1.2,4.126408100128174,243.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carangola,2017-12-31T00:00:00,1.1,0.9742884039878845,11.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carangola,2018-12-31T00:00:00,1.56,1.2697113752365112,18.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caratinga,2012-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caratinga,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caratinga,2014-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caratinga,2015-12-31T00:00:00,1.08,1.1227785348892212,3.96,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caratinga,2016-12-31T00:00:00,1.56,1.1006306409835815,29.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caratinga,2017-12-31T00:00:00,1.46,1.1526198387145996,21.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caratinga,2018-12-31T00:00:00,1.8,1.813787579536438,0.77,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Chale,2012-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Chale,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Chale,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Chale,2015-12-31T00:00:00,1.38,1.6163537502288818,17.13,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Chale,2016-12-31T00:00:00,1.42,1.5997254848480225,12.66,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Chale,2017-12-31T00:00:00,0.93,1.4179017543792725,52.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Chale,2018-12-31T00:00:00,1.38,1.6867196559906006,22.23,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Conceicao_de_Ipanema,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Conceicao_de_Ipanema,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Conceicao_de_Ipanema,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Conceicao_de_Ipanema,2015-12-31T00:00:00,1.14,1.1538447141647339,1.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Conceicao_de_Ipanema,2016-12-31T00:00:00,1.44,1.153890609741211,19.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Conceicao_de_Ipanema,2017-12-31T00:00:00,1.44,1.1968098878860474,16.89,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Conceicao_de_Ipanema,2018-12-31T00:00:00,1.56,1.611042857170105,3.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Datas,2012-12-31T00:00:00,1.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Datas,2013-12-31T00:00:00,1.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Datas,2014-12-31T00:00:00,0.79,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Datas,2015-12-31T00:00:00,1.4,1.0621048212051392,24.14,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Datas,2016-12-31T00:00:00,1.2,1.120627999305725,6.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Datas,2017-12-31T00:00:00,1.5,1.1263080835342407,24.91,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Datas,2018-12-31T00:00:00,3.0,1.6231801509857178,45.89,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Divino,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Divino,2013-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Divino,2014-12-31T00:00:00,0.89,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Divino,2015-12-31T00:00:00,1.2,1.2965540885925293,8.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Divino,2016-12-31T00:00:00,1.2,1.3797495365142822,14.98,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Divino,2017-12-31T00:00:00,1.19,1.0747359991073608,9.69,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Divino,2018-12-31T00:00:00,1.32,1.2098156213760376,8.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Durande,2012-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Durande,2013-12-31T00:00:00,2.22,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Durande,2014-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Durande,2015-12-31T00:00:00,1.68,1.8526684045791626,10.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Durande,2016-12-31T00:00:00,1.8,1.7934978008270264,0.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Durande,2017-12-31T00:00:00,1.33,1.5794434547424316,18.76,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Durande,2018-12-31T00:00:00,2.1,2.128512144088745,1.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Entre_Folhas,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Entre_Folhas,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Entre_Folhas,2014-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Entre_Folhas,2015-12-31T00:00:00,1.0,1.059966802597046,6.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Entre_Folhas,2016-12-31T00:00:00,1.2,1.086715579032898,9.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Entre_Folhas,2017-12-31T00:00:00,0.86,1.1476552486419678,33.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Entre_Folhas,2018-12-31T00:00:00,1.2,1.2110753059387207,0.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ervalia,2012-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ervalia,2013-12-31T00:00:00,1.92,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ervalia,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ervalia,2015-12-31T00:00:00,1.5,1.6754460334777832,11.7,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ervalia,2016-12-31T00:00:00,1.32,1.4392504692077637,9.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ervalia,2017-12-31T00:00:00,1.36,1.2793909311294556,5.93,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ervalia,2018-12-31T00:00:00,1.8,1.5274139642715454,15.14,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Espera_Feliz,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Espera_Feliz,2013-12-31T00:00:00,1.92,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Espera_Feliz,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Espera_Feliz,2015-12-31T00:00:00,1.38,1.3814563751220703,0.11,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Espera_Feliz,2016-12-31T00:00:00,1.68,1.7170300483703613,2.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Espera_Feliz,2017-12-31T00:00:00,1.21,1.3187546730041504,8.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Espera_Feliz,2018-12-31T00:00:00,1.92,1.9568867683410645,1.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Faria_Lemos,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Faria_Lemos,2013-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Faria_Lemos,2014-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Faria_Lemos,2015-12-31T00:00:00,1.32,1.1085608005523682,16.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Faria_Lemos,2016-12-31T00:00:00,1.38,1.266720175743103,8.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Faria_Lemos,2017-12-31T00:00:00,0.97,1.2101366519927979,24.76,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Faria_Lemos,2018-12-31T00:00:00,1.2,1.5269360542297363,27.24,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ferros,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ferros,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ferros,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ferros,2015-12-31T00:00:00,1.08,1.3148109912872314,21.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ferros,2016-12-31T00:00:00,1.2,1.3173143863677979,9.78,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ferros,2017-12-31T00:00:00,0.59,1.1504818201065063,95.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ferros,2018-12-31T00:00:00,0.84,1.0053489208221436,19.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Franciscopolis,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Franciscopolis,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Franciscopolis,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Franciscopolis,2015-12-31T00:00:00,1.08,3.314145565032959,206.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Franciscopolis,2016-12-31T00:00:00,1.2,1.0610787868499756,11.58,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Franciscopolis,2017-12-31T00:00:00,0.52,1.0020923614501953,92.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Franciscopolis,2018-12-31T00:00:00,1.8,1.3374518156051636,25.7,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Itanhomi,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Itanhomi,2013-12-31T00:00:00,1.35,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Itanhomi,2014-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Itanhomi,2015-12-31T00:00:00,1.2,1.3101189136505127,9.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Itanhomi,2016-12-31T00:00:00,1.14,1.2287538051605225,7.79,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Itanhomi,2017-12-31T00:00:00,1.0,1.0912079811096191,9.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Itanhomi,2018-12-31T00:00:00,1.62,1.2166273593902588,24.9,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Formosa,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Formosa,2013-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Formosa,2014-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Formosa,2015-12-31T00:00:00,2.4,2.4532480239868164,2.22,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Formosa,2016-12-31T00:00:00,1.8,2.305006742477417,28.06,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Formosa,2017-12-31T00:00:00,1.92,1.9487758874893188,1.5,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Formosa,2018-12-31T00:00:00,1.92,2.04522442817688,6.52,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Grande,2012-12-31T00:00:00,1.43,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Grande,2013-12-31T00:00:00,3.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Grande,2014-12-31T00:00:00,1.79,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Grande,2015-12-31T00:00:00,2.38,2.730877161026001,14.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Grande,2016-12-31T00:00:00,2.1,2.4926717281341553,18.7,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Grande,2017-12-31T00:00:00,2.1,1.9640157222747803,6.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Grande,2018-12-31T00:00:00,2.1,2.162933349609375,3.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lajinha,2012-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lajinha,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lajinha,2014-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lajinha,2015-12-31T00:00:00,1.44,1.566648244857788,8.8,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lajinha,2016-12-31T00:00:00,1.68,1.387444019317627,17.41,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lajinha,2017-12-31T00:00:00,1.04,1.2270835638046265,17.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lajinha,2018-12-31T00:00:00,1.68,1.817848563194275,8.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Luisburgo,2012-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Luisburgo,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Luisburgo,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Luisburgo,2015-12-31T00:00:00,1.5,1.3386058807373047,10.76,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Luisburgo,2016-12-31T00:00:00,1.32,1.483224868774414,12.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Luisburgo,2017-12-31T00:00:00,1.53,1.3542118072509766,11.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Luisburgo,2018-12-31T00:00:00,1.8,1.7324626445770264,3.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Malacacheta,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Malacacheta,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Malacacheta,2014-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Malacacheta,2015-12-31T00:00:00,1.5,1.5176060199737549,1.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Malacacheta,2016-12-31T00:00:00,1.32,1.4923043251037598,13.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Malacacheta,2017-12-31T00:00:00,1.37,1.3565986156463623,0.98,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Malacacheta,2018-12-31T00:00:00,1.8,1.5284124612808228,15.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhuacu,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhuacu,2013-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhuacu,2014-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhuacu,2015-12-31T00:00:00,1.38,1.2341564893722534,10.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhuacu,2016-12-31T00:00:00,1.26,1.3003653287887573,3.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhuacu,2017-12-31T00:00:00,1.43,1.136244535446167,20.54,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhuacu,2018-12-31T00:00:00,1.44,1.4536473751068115,0.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhumirim,2012-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhumirim,2013-12-31T00:00:00,1.92,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhumirim,2014-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhumirim,2015-12-31T00:00:00,1.8,1.6110763549804688,10.5,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhumirim,2016-12-31T00:00:00,1.32,1.817826747894287,37.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhumirim,2017-12-31T00:00:00,1.62,1.3759125471115112,15.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhumirim,2018-12-31T00:00:00,1.56,1.7824922800064087,14.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Matipo,2012-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Matipo,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Matipo,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Matipo,2015-12-31T00:00:00,1.44,1.3336598873138428,7.38,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Matipo,2016-12-31T00:00:00,1.32,1.397702932357788,5.89,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Matipo,2017-12-31T00:00:00,0.9,1.290553092956543,43.39,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Matipo,2018-12-31T00:00:00,1.32,1.5263404846191406,15.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mirai,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mirai,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mirai,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mirai,2015-12-31T00:00:00,0.84,1.1099454164505005,32.14,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mirai,2016-12-31T00:00:00,1.2,0.9638305902481079,19.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mirai,2017-12-31T00:00:00,1.02,1.0080487728118896,1.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mirai,2018-12-31T00:00:00,1.2,1.158446192741394,3.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Muriae,2012-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Muriae,2013-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Muriae,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Muriae,2015-12-31T00:00:00,1.08,1.174833059310913,8.78,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Muriae,2016-12-31T00:00:00,1.32,1.1292760372161865,14.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Muriae,2017-12-31T00:00:00,1.44,1.0501904487609863,27.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Muriae,2018-12-31T00:00:00,1.44,1.478770136833191,2.69,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mutum,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mutum,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mutum,2014-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mutum,2015-12-31T00:00:00,1.15,1.3807287216186523,20.06,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mutum,2016-12-31T00:00:00,1.26,1.3213351964950562,4.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mutum,2017-12-31T00:00:00,1.45,1.185356855392456,18.25,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mutum,2018-12-31T00:00:00,1.5,1.5112457275390625,0.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Nova_Belem,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Nova_Belem,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Nova_Belem,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Nova_Belem,2015-12-31T00:00:00,0.9,2.5904579162597656,187.83,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Nova_Belem,2016-12-31T00:00:00,1.2,4.12603235244751,243.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Nova_Belem,2017-12-31T00:00:00,0.48,0.9754941463470459,103.23,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Nova_Belem,2018-12-31T00:00:00,1.2,1.2213550806045532,1.78,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Orizania,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Orizania,2013-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Orizania,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Orizania,2015-12-31T00:00:00,1.2,1.3107328414916992,9.23,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Orizania,2016-12-31T00:00:00,1.8,1.377801775932312,23.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Orizania,2017-12-31T00:00:00,1.47,1.3595480918884277,7.51,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Orizania,2018-12-31T00:00:00,1.56,1.8244297504425049,16.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Patis,2012-12-31T00:00:00,2.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Patis,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Patis,2014-12-31T00:00:00,1.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Patis,2015-12-31T00:00:00,2.8,1.4500970840454102,48.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Patis,2016-12-31T00:00:00,1.19,1.6394667625427246,37.77,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Patis,2017-12-31T00:00:00,1.8,1.7582145929336548,2.32,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Patis,2018-12-31T00:00:00,3.6,2.4971184730529785,30.64,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ponte_Nova,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ponte_Nova,2013-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ponte_Nova,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ponte_Nova,2015-12-31T00:00:00,1.26,1.222951054573059,2.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ponte_Nova,2016-12-31T00:00:00,1.2,1.1950680017471313,0.41,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ponte_Nova,2017-12-31T00:00:00,1.17,1.1680467128753662,0.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ponte_Nova,2018-12-31T00:00:00,1.5,1.3115837574005127,12.56,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Porto_Firme,2012-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Porto_Firme,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Porto_Firme,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Porto_Firme,2015-12-31T00:00:00,1.26,1.0958976745605469,13.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Porto_Firme,2016-12-31T00:00:00,1.2,1.2075588703155518,0.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Porto_Firme,2017-12-31T00:00:00,1.2,1.1762158870697021,1.98,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Porto_Firme,2018-12-31T00:00:00,1.2,1.261406660079956,5.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Raul_Soares,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Raul_Soares,2013-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Raul_Soares,2014-12-31T00:00:00,0.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Raul_Soares,2015-12-31T00:00:00,1.08,1.1106750965118408,2.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Raul_Soares,2016-12-31T00:00:00,1.08,1.0075660943984985,6.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Raul_Soares,2017-12-31T00:00:00,0.86,0.9527634978294373,10.79,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Raul_Soares,2018-12-31T00:00:00,1.2,1.243361234664917,3.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Reduto,2012-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Reduto,2013-12-31T00:00:00,1.62,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Reduto,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Reduto,2015-12-31T00:00:00,1.38,1.3772916793823242,0.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Reduto,2016-12-31T00:00:00,1.44,1.4297271966934204,0.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Reduto,2017-12-31T00:00:00,1.26,1.2366859912872314,1.85,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Reduto,2018-12-31T00:00:00,1.8,1.4734361171722412,18.14,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rio_Pardo_de_Minas,2012-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rio_Pardo_de_Minas,2013-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rio_Pardo_de_Minas,2014-12-31T00:00:00,2.65,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rio_Pardo_de_Minas,2015-12-31T00:00:00,2.28,2.2074339389801025,3.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rio_Pardo_de_Minas,2016-12-31T00:00:00,2.46,2.4208760261535645,1.59,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rio_Pardo_de_Minas,2017-12-31T00:00:00,2.18,2.462151050567627,12.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rio_Pardo_de_Minas,2018-12-31T00:00:00,2.63,2.377394437789917,9.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rosario_da_Limeira,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rosario_da_Limeira,2013-12-31T00:00:00,1.54,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rosario_da_Limeira,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rosario_da_Limeira,2015-12-31T00:00:00,1.14,1.4834718704223633,30.13,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rosario_da_Limeira,2016-12-31T00:00:00,1.26,1.3378101587295532,6.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rosario_da_Limeira,2017-12-31T00:00:00,0.78,1.2504829168319702,60.32,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rosario_da_Limeira,2018-12-31T00:00:00,1.2,1.3400259017944336,11.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Barbara_do_Leste,2012-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Barbara_do_Leste,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Barbara_do_Leste,2014-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Barbara_do_Leste,2015-12-31T00:00:00,1.08,1.1230040788650513,3.98,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Barbara_do_Leste,2016-12-31T00:00:00,1.02,1.096869945526123,7.54,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Barbara_do_Leste,2017-12-31T00:00:00,0.91,1.0027552843093872,10.19,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Barbara_do_Leste,2018-12-31T00:00:00,1.74,1.1595852375030518,33.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Margarida,2012-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Margarida,2013-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Margarida,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Margarida,2015-12-31T00:00:00,1.56,1.4551628828048706,6.72,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Margarida,2016-12-31T00:00:00,1.32,1.4880836009979248,12.73,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Margarida,2017-12-31T00:00:00,1.2,1.3324027061462402,11.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Margarida,2018-12-31T00:00:00,1.68,1.567725658416748,6.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Rita_de_Minas,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Rita_de_Minas,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Rita_de_Minas,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Rita_de_Minas,2015-12-31T00:00:00,1.2,1.1869263648986816,1.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Rita_de_Minas,2016-12-31T00:00:00,1.08,1.128062129020691,4.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Rita_de_Minas,2017-12-31T00:00:00,1.25,1.0419667959213257,16.64,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Rita_de_Minas,2018-12-31T00:00:00,1.5,1.3200931549072266,11.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santana_do_Manhuacu,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santana_do_Manhuacu,2013-12-31T00:00:00,1.62,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santana_do_Manhuacu,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santana_do_Manhuacu,2015-12-31T00:00:00,1.44,1.486217975616455,3.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santana_do_Manhuacu,2016-12-31T00:00:00,1.32,1.4459517002105713,9.54,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santana_do_Manhuacu,2017-12-31T00:00:00,1.28,1.2696146965026855,0.81,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santana_do_Manhuacu,2018-12-31T00:00:00,1.5,1.4610545635223389,2.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Francisco_do_Gloria,2012-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Francisco_do_Gloria,2013-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Francisco_do_Gloria,2014-12-31T00:00:00,1.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Francisco_do_Gloria,2015-12-31T00:00:00,1.32,1.2728763818740845,3.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Francisco_do_Gloria,2016-12-31T00:00:00,1.5,1.2953455448150635,13.64,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Francisco_do_Gloria,2017-12-31T00:00:00,0.8,1.3404589891433716,67.56,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Francisco_do_Gloria,2018-12-31T00:00:00,1.32,1.6061776876449585,21.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Joao_do_Manhuacu,2012-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Joao_do_Manhuacu,2013-12-31T00:00:00,1.74,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Joao_do_Manhuacu,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Joao_do_Manhuacu,2015-12-31T00:00:00,1.32,1.446770429611206,9.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Joao_do_Manhuacu,2016-12-31T00:00:00,1.32,1.3859888315200806,5.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Joao_do_Manhuacu,2017-12-31T00:00:00,1.7,1.2114733457565308,28.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Joao_do_Manhuacu,2018-12-31T00:00:00,1.62,1.8096330165863037,11.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Jose_do_Mantimento,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Jose_do_Mantimento,2013-12-31T00:00:00,1.42,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Jose_do_Mantimento,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Jose_do_Mantimento,2015-12-31T00:00:00,1.44,1.3148834705352783,8.69,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Jose_do_Mantimento,2016-12-31T00:00:00,1.2,1.3941680192947388,16.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Jose_do_Mantimento,2017-12-31T00:00:00,1.2,1.239254117012024,3.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Jose_do_Mantimento,2018-12-31T00:00:00,1.44,1.4909708499908447,3.54,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sericita,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sericita,2013-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sericita,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sericita,2015-12-31T00:00:00,1.2,1.321537971496582,10.13,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sericita,2016-12-31T00:00:00,1.2,1.271117925643921,5.93,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sericita,2017-12-31T00:00:00,1.12,1.1483405828475952,2.53,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sericita,2018-12-31T00:00:00,1.8,1.2667157649993896,29.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Setubinha,2012-12-31T00:00:00,0.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Setubinha,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Setubinha,2014-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Setubinha,2015-12-31T00:00:00,0.9,0.8322755098342896,7.52,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Setubinha,2016-12-31T00:00:00,1.5,0.9366511106491089,37.56,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Setubinha,2017-12-31T00:00:00,0.76,0.8822211623191833,16.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Setubinha,2018-12-31T00:00:00,0.9,1.5163419246673584,68.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Teofilo_Otoni,2012-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Teofilo_Otoni,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Teofilo_Otoni,2014-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Teofilo_Otoni,2015-12-31T00:00:00,0.9,0.8112048506736755,9.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Teofilo_Otoni,2016-12-31T00:00:00,0.9,0.875868558883667,2.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Teofilo_Otoni,2017-12-31T00:00:00,0.67,0.7611346244812012,13.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Teofilo_Otoni,2018-12-31T00:00:00,0.91,0.9175853729248047,0.83,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Tombos,2012-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Tombos,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Tombos,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Tombos,2015-12-31T00:00:00,1.2,1.1055364608764648,7.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Tombos,2016-12-31T00:00:00,0.96,1.1614786386489868,20.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Tombos,2017-12-31T00:00:00,0.99,1.0481163263320923,5.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Tombos,2018-12-31T00:00:00,1.44,1.1826143264770508,17.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ubaporanga,2012-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ubaporanga,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ubaporanga,2014-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ubaporanga,2015-12-31T00:00:00,1.2,1.313615322113037,9.47,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ubaporanga,2016-12-31T00:00:00,0.96,1.2191352844238281,26.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ubaporanga,2017-12-31T00:00:00,1.03,1.021189570426941,0.86,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ubaporanga,2018-12-31T00:00:00,1.5,1.5194331407546997,1.3,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Urucuia,2012-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Urucuia,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Urucuia,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Urucuia,2015-12-31T00:00:00,1.8,1.8638529777526855,3.55,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Urucuia,2016-12-31T00:00:00,2.4,2.3690199851989746,1.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Urucuia,2017-12-31T00:00:00,1.8,1.611828088760376,10.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Urucuia,2018-12-31T00:00:00,2.4,2.181640625,9.1,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vermelho_Novo,2012-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vermelho_Novo,2013-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vermelho_Novo,2014-12-31T00:00:00,0.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vermelho_Novo,2015-12-31T00:00:00,1.2,0.9487432241439819,20.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vermelho_Novo,2016-12-31T00:00:00,1.2,1.0060757398605347,16.16,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vermelho_Novo,2017-12-31T00:00:00,1.08,1.0266176462173462,4.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vermelho_Novo,2018-12-31T00:00:00,1.32,1.2885748147964478,2.38,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vicosa,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vicosa,2013-12-31T00:00:00,1.62,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vicosa,2014-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vicosa,2015-12-31T00:00:00,1.44,1.5062414407730103,4.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vicosa,2016-12-31T00:00:00,1.16,1.3771830797195435,18.72,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vicosa,2017-12-31T00:00:00,1.49,1.3873223066329956,6.89,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vicosa,2018-12-31T00:00:00,1.56,1.6414177417755127,5.22,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vieiras,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vieiras,2013-12-31T00:00:00,1.81,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vieiras,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vieiras,2015-12-31T00:00:00,1.32,1.6561667919158936,25.47,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vieiras,2016-12-31T00:00:00,1.2,1.6495883464813232,37.47,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vieiras,2017-12-31T00:00:00,1.13,1.5014126300811768,32.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vieiras,2018-12-31T00:00:00,1.26,1.3028450012207031,3.4,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Virginopolis,2012-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Virginopolis,2013-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Virginopolis,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Virginopolis,2015-12-31T00:00:00,1.32,1.2702043056488037,3.77,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Virginopolis,2016-12-31T00:00:00,1.0,1.2623488903045654,26.23,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Virginopolis,2017-12-31T00:00:00,1.37,1.1870696544647217,13.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Virginopolis,2018-12-31T00:00:00,1.5,1.477065920829773,1.53,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Agua_Boa,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Aguas_Vermelhas,2019-12-31T00:00:00,3.0,3,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Alto_Caparao,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Araponga,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Areado,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ataleia,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Bom_Jesus_do_Galho,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caiana,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Cajuri,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Canaa,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caparao,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caputira,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carai,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carangola,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caratinga,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Chale,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Conceicao_de_Ipanema,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Datas,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Divino,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Durande,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Entre_Folhas,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ervalia,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Espera_Feliz,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Faria_Lemos,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ferros,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Franciscopolis,2019-12-31T00:00:00,3.0,1,-66.67,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Itanhomi,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Formosa,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Grande,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lajinha,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Luisburgo,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Malacacheta,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhuacu,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhumirim,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Matipo,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mirai,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Muriae,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mutum,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Nova_Belem,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Orizania,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Patis,2019-12-31T00:00:00,3.0,2,-33.33,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ponte_Nova,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Porto_Firme,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Raul_Soares,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Reduto,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rio_Pardo_de_Minas,2019-12-31T00:00:00,3.0,2,-33.33,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rosario_da_Limeira,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Barbara_do_Leste,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Margarida,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Rita_de_Minas,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santana_do_Manhuacu,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Francisco_do_Gloria,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Joao_do_Manhuacu,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Jose_do_Mantimento,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sericita,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Setubinha,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Teofilo_Otoni,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Tombos,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ubaporanga,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Urucuia,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vermelho_Novo,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vicosa,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vieiras,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Virginopolis,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Agua_Boa,2020-12-31T00:00:00,1.33,1.2557251453399658,-5.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Aguas_Vermelhas,2020-12-31T00:00:00,3.6000000000000005,3.3031630516052246,-8.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Alto_Caparao,2020-12-31T00:00:00,1.92,1.4821419715881348,-22.81,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Araponga,2020-12-31T00:00:00,1.8000000000000005,1.529170274734497,-15.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Areado,2020-12-31T00:00:00,2.078746484531941,1.9631084203720093,-5.56,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ataleia,2020-12-31T00:00:00,1.204819277108434,0.954089879989624,-20.81,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Bom_Jesus_do_Galho,2020-12-31T00:00:00,1.44,1.1491153240203857,-20.2,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caiana,2020-12-31T00:00:00,1.5,1.5291638374328613,1.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Cajuri,2020-12-31T00:00:00,1.8000000000000005,1.840423822402954,2.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Canaa,2020-12-31T00:00:00,1.8000000000000005,1.537367343902588,-14.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caparao,2020-12-31T00:00:00,1.9199600798403191,1.729040265083313,-9.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caputira,2020-12-31T00:00:00,1.8000000000000005,1.2264323234558105,-31.86,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carai,2020-12-31T00:00:00,1.5225,1.2326676845550537,-19.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carangola,2020-12-31T00:00:00,1.5,1.5252281427383423,1.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caratinga,2020-12-31T00:00:00,2.040022111663903,1.747580885887146,-14.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Chale,2020-12-31T00:00:00,1.71015625,1.3044267892837524,-23.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Conceicao_de_Ipanema,2020-12-31T00:00:00,1.500533617929563,1.4826034307479858,-1.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Datas,2020-12-31T00:00:00,2.0,2.2776083946228027,13.88,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Divino,2020-12-31T00:00:00,1.8000483851457605,1.320742130279541,-26.63,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Durande,2020-12-31T00:00:00,2.4,2.070668935775757,-13.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Entre_Folhas,2020-12-31T00:00:00,1.26046511627907,1.1342676877975464,-10.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ervalia,2020-12-31T00:00:00,1.7999614197530858,1.6852545738220215,-6.37,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Espera_Feliz,2020-12-31T00:00:00,2.04,2.113980531692505,3.63,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Faria_Lemos,2020-12-31T00:00:00,1.44031007751938,1.191304326057434,-17.29,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ferros,2020-12-31T00:00:00,1.2,0.9024779796600342,-24.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Franciscopolis,2020-12-31T00:00:00,1.8000000000000005,4.370452880859375,142.8,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Itanhomi,2020-12-31T00:00:00,1.26,1.688717246055603,34.03,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Formosa,2020-12-31T00:00:00,2.5494505494505484,2.4490582942962646,-3.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Grande,2020-12-31T00:00:00,2.4,2.453397512435913,2.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lajinha,2020-12-31T00:00:00,1.9200446677833607,1.520205020904541,-20.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Luisburgo,2020-12-31T00:00:00,2.1,1.800461769104004,-14.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Malacacheta,2020-12-31T00:00:00,1.8000000000000005,1.9114978313446045,6.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhuacu,2020-12-31T00:00:00,1.6800182481751822,1.4624946117401123,-12.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhumirim,2020-12-31T00:00:00,1.68,1.6634783744812012,-0.98,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Matipo,2020-12-31T00:00:00,1.56,1.367431879043579,-12.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mirai,2020-12-31T00:00:00,1.5011494252873558,1.2166728973388672,-18.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Muriae,2020-12-31T00:00:00,1.56,1.5452592372894287,-0.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mutum,2020-12-31T00:00:00,1.65,1.4873971939086914,-9.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Nova_Belem,2020-12-31T00:00:00,1.1,1.1250945329666138,2.28,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Orizania,2020-12-31T00:00:00,1.679901960784314,1.5514330863952637,-7.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Patis,2020-12-31T00:00:00,3.6000000000000005,4.533916473388672,25.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ponte_Nova,2020-12-31T00:00:00,1.333333333333333,1.4146147966384888,6.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Porto_Firme,2020-12-31T00:00:00,1.56,1.2112298011779785,-22.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Raul_Soares,2020-12-31T00:00:00,1.320083682008368,1.2367041110992432,-6.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Reduto,2020-12-31T00:00:00,1.8000000000000005,1.7210084199905396,-4.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rio_Pardo_de_Minas,2020-12-31T00:00:00,3.4487179487179493,3.3500266075134277,-2.86,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rosario_da_Limeira,2020-12-31T00:00:00,1.5,1.2734043598175049,-15.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Barbara_do_Leste,2020-12-31T00:00:00,1.4400953029271608,1.5173900127410889,5.37,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Margarida,2020-12-31T00:00:00,1.8000000000000005,1.5408015251159668,-14.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Rita_de_Minas,2020-12-31T00:00:00,1.5,1.4232136011123657,-5.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santana_do_Manhuacu,2020-12-31T00:00:00,1.8000000000000005,1.5337209701538086,-14.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Francisco_do_Gloria,2020-12-31T00:00:00,1.679693486590038,1.2112452983856201,-27.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Joao_do_Manhuacu,2020-12-31T00:00:00,1.8000000000000005,1.6800107955932617,-6.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Jose_do_Mantimento,2020-12-31T00:00:00,1.8000000000000005,1.5085577964782715,-16.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sericita,2020-12-31T00:00:00,2.5200000000000005,1.7283822298049927,-31.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Setubinha,2020-12-31T00:00:00,1.0,0.953592836856842,-4.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Teofilo_Otoni,2020-12-31T00:00:00,1.09375,0.9659513235092163,-11.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Tombos,2020-12-31T00:00:00,1.5,1.2946093082427979,-13.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ubaporanga,2020-12-31T00:00:00,1.6200873362445412,1.4106056690216064,-12.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Urucuia,2020-12-31T00:00:00,2.4,2.486793041229248,3.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vermelho_Novo,2020-12-31T00:00:00,1.320080321285141,1.2377300262451172,-6.24,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vicosa,2020-12-31T00:00:00,1.8000000000000005,1.6381959915161133,-8.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vieiras,2020-12-31T00:00:00,1.8000000000000005,1.2198091745376587,-32.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Virginopolis,2020-12-31T00:00:00,1.626666666666667,1.6433117389678955,1.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Agua_Boa,2021-12-31T00:00:00,0.6171428571428572,1.173013687133789,90.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Aguas_Vermelhas,2021-12-31T00:00:00,3.0,2.9175865650177,-2.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Alto_Caparao,2021-12-31T00:00:00,1.08,1.5132973194122314,40.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Araponga,2021-12-31T00:00:00,1.2,1.2915977239608765,7.63,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Areado,2021-12-31T00:00:00,1.5228426395939092,1.809368371963501,18.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ataleia,2021-12-31T00:00:00,0.6266666666666667,0.9280019998550415,48.09,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Bom_Jesus_do_Galho,2021-12-31T00:00:00,1.079802955665025,1.1556296348571777,7.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caiana,2021-12-31T00:00:00,0.9,1.2791767120361328,42.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Cajuri,2021-12-31T00:00:00,1.2602564102564102,1.606358289718628,27.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Canaa,2021-12-31T00:00:00,1.2,1.49620521068573,24.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caparao,2021-12-31T00:00:00,0.9,1.5945801734924316,77.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caputira,2021-12-31T00:00:00,1.38008658008658,1.3898316621780396,0.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carai,2021-12-31T00:00:00,1.218,1.256406307220459,3.15,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carangola,2021-12-31T00:00:00,1.02,1.399984359741211,37.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caratinga,2021-12-31T00:00:00,1.26,1.1968967914581299,-5.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Chale,2021-12-31T00:00:00,1.26016713091922,1.2854387760162354,2.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Conceicao_de_Ipanema,2021-12-31T00:00:00,1.29,1.3117094039916992,1.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Datas,2021-12-31T00:00:00,1.0,2.1126537322998047,111.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Divino,2021-12-31T00:00:00,1.08,1.2951226234436035,19.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Durande,2021-12-31T00:00:00,1.32,1.6803853511810303,27.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Entre_Folhas,2021-12-31T00:00:00,0.8400000000000001,1.096851110458374,30.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ervalia,2021-12-31T00:00:00,1.08,1.5230759382247925,41.03,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Espera_Feliz,2021-12-31T00:00:00,0.78,1.361438274383545,74.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Faria_Lemos,2021-12-31T00:00:00,0.7199999999999999,1.1941641569137573,65.86,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ferros,2021-12-31T00:00:00,1.205128205128205,0.9033645987510681,-25.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Franciscopolis,2021-12-31T00:00:00,1.2,2.039508819580078,69.96,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Itanhomi,2021-12-31T00:00:00,0.9606060606060604,1.1613595485687256,20.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Formosa,2021-12-31T00:00:00,1.4830188679245278,2.7369608879089355,84.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Grande,2021-12-31T00:00:00,0.6000000000000001,1.5744987726211548,162.42,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lajinha,2021-12-31T00:00:00,1.260014255167498,1.3282380104064941,5.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Luisburgo,2021-12-31T00:00:00,1.3799999999999997,1.8245816230773926,32.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Malacacheta,2021-12-31T00:00:00,1.8000000000000005,1.64011549949646,-8.88,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhuacu,2021-12-31T00:00:00,0.9,1.2713384628295898,41.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhumirim,2021-12-31T00:00:00,1.2,1.4303793907165527,19.2,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Matipo,2021-12-31T00:00:00,1.07995337995338,1.3738301992416382,27.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mirai,2021-12-31T00:00:00,1.019512195121951,1.2554404735565186,23.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Muriae,2021-12-31T00:00:00,0.9,1.2162160873413086,35.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mutum,2021-12-31T00:00:00,1.319934249850568,1.4997926950454712,13.63,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Nova_Belem,2021-12-31T00:00:00,1.0,1.0289523601531982,2.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Orizania,2021-12-31T00:00:00,1.08,1.539139747619629,42.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Patis,2021-12-31T00:00:00,2.111111111111112,3.2591469287872314,54.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ponte_Nova,2021-12-31T00:00:00,1.2,1.2949506044387817,7.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Porto_Firme,2021-12-31T00:00:00,1.319607843137255,1.2823150157928467,-2.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Raul_Soares,2021-12-31T00:00:00,1.320079522862823,1.2195950746536255,-7.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Reduto,2021-12-31T00:00:00,1.2,1.4774236679077148,23.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rio_Pardo_de_Minas,2021-12-31T00:00:00,2.764006791171477,2.874255657196045,3.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rosario_da_Limeira,2021-12-31T00:00:00,0.9,1.2650917768478394,40.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Barbara_do_Leste,2021-12-31T00:00:00,1.319917440660475,1.2772300243377686,-3.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Margarida,2021-12-31T00:00:00,0.9,1.5381848812103271,70.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Rita_de_Minas,2021-12-31T00:00:00,1.2,1.3564352989196777,13.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santana_do_Manhuacu,2021-12-31T00:00:00,1.32,1.5109593868255615,14.47,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Francisco_do_Gloria,2021-12-31T00:00:00,0.78,1.1578993797302246,48.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Joao_do_Manhuacu,2021-12-31T00:00:00,0.9,1.5817667245864868,75.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Jose_do_Mantimento,2021-12-31T00:00:00,1.500917431192661,1.5875022411346436,5.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sericita,2021-12-31T00:00:00,0.9,1.9324886798858643,114.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Setubinha,2021-12-31T00:00:00,0.9329787234042556,0.9152268171310425,-1.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Teofilo_Otoni,2021-12-31T00:00:00,0.6800000000000002,0.9500925540924072,39.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Tombos,2021-12-31T00:00:00,0.8497959183673469,1.1486973762512207,35.17,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ubaporanga,2021-12-31T00:00:00,1.319867549668874,1.3498378992080688,2.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Urucuia,2021-12-31T00:00:00,1.799276672694394,2.4002296924591064,33.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vermelho_Novo,2021-12-31T00:00:00,0.96,1.1894553899765015,23.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vicosa,2021-12-31T00:00:00,0.9006711409395973,1.3191417455673218,46.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vieiras,2021-12-31T00:00:00,0.78,1.2714099884033203,63.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Virginopolis,2021-12-31T00:00:00,1.202380952380952,1.533062219619751,27.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Agua_Boa,2022-12-31T00:00:00,0.9136363636363636,0.9837000966072083,7.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Aguas_Vermelhas,2022-12-31T00:00:00,3.0,3.032529830932617,1.08,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Alto_Caparao,2022-12-31T00:00:00,1.5,1.2660026550292969,-15.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Araponga,2022-12-31T00:00:00,1.260041407867495,0.9097451567649841,-27.8,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Areado,2022-12-31T00:00:00,0.9076923076923076,1.8101756572723389,99.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ataleia,2022-12-31T00:00:00,0.7142857142857143,0.7999357581138611,11.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Bom_Jesus_do_Galho,2022-12-31T00:00:00,0.7801587301587303,1.146430253982544,46.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caiana,2022-12-31T00:00:00,0.9,0.998170018196106,10.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Cajuri,2022-12-31T00:00:00,1.050602409638554,1.2443033456802368,18.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Canaa,2022-12-31T00:00:00,1.320338983050847,1.2801846265792847,-3.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caparao,2022-12-31T00:00:00,1.32,1.1291319131851196,-14.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caputira,2022-12-31T00:00:00,1.440044247787611,1.3716291189193726,-4.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carai,2022-12-31T00:00:00,1.2,1.2657721042633057,5.48,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carangola,2022-12-31T00:00:00,1.02,1.1016676425933838,8.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caratinga,2022-12-31T00:00:00,1.620022123893805,1.2488123178482056,-22.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Chale,2022-12-31T00:00:00,1.44,1.4719703197479248,2.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Conceicao_de_Ipanema,2022-12-31T00:00:00,1.5,1.3284988403320312,-11.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Datas,2022-12-31T00:00:00,0.75,1.428877353668213,90.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Divino,2022-12-31T00:00:00,1.08,1.0843489170074463,0.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Durande,2022-12-31T00:00:00,1.5,1.5437259674072266,2.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Entre_Folhas,2022-12-31T00:00:00,1.2,0.8135480880737305,-32.2,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ervalia,2022-12-31T00:00:00,1.2,1.1730382442474365,-2.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Espera_Feliz,2022-12-31T00:00:00,1.2,0.8599786162376404,-28.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Faria_Lemos,2022-12-31T00:00:00,1.200305810397553,0.8388909101486206,-30.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ferros,2022-12-31T00:00:00,1.230769230769231,1.0051698684692383,-18.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Franciscopolis,2022-12-31T00:00:00,1.255555555555556,1.736546516418457,38.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Itanhomi,2022-12-31T00:00:00,1.081818181818182,1.0284503698349,-4.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Formosa,2022-12-31T00:00:00,2.088607594936709,1.9212374687194824,-8.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Grande,2022-12-31T00:00:00,2.4,1.2537370920181274,-47.76,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lajinha,2022-12-31T00:00:00,1.419975786924939,1.274712324142456,-10.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Luisburgo,2022-12-31T00:00:00,1.5,1.5403549671173096,2.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Malacacheta,2022-12-31T00:00:00,1.5595238095238098,1.657529354095459,6.28,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhuacu,2022-12-31T00:00:00,1.44,1.111379861831665,-22.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhumirim,2022-12-31T00:00:00,1.3799999999999997,1.2794973850250244,-7.28,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Matipo,2022-12-31T00:00:00,1.26,1.2440208196640015,-1.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mirai,2022-12-31T00:00:00,1.2,1.1114708185195923,-7.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Muriae,2022-12-31T00:00:00,0.9897142857142858,1.0404891967773438,5.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mutum,2022-12-31T00:00:00,1.379940564635958,1.3272887468338013,-3.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Nova_Belem,2022-12-31T00:00:00,0.96,0.9807199835777283,2.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Orizania,2022-12-31T00:00:00,1.5,1.2248655557632446,-18.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Patis,2022-12-31T00:00:00,2.044444444444444,2.6269447803497314,28.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ponte_Nova,2022-12-31T00:00:00,1.25,1.2175953388214111,-2.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Porto_Firme,2022-12-31T00:00:00,1.32,1.2188053131103516,-7.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Raul_Soares,2022-12-31T00:00:00,0.8399548532731377,1.2543222904205322,49.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Reduto,2022-12-31T00:00:00,1.260059171597633,1.3655141592025757,8.37,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rio_Pardo_de_Minas,2022-12-31T00:00:00,2.515254237288135,2.917025089263916,15.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rosario_da_Limeira,2022-12-31T00:00:00,1.02,1.0284056663513184,0.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Barbara_do_Leste,2022-12-31T00:00:00,1.05015873015873,1.2607109546661377,20.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Margarida,2022-12-31T00:00:00,1.3799999999999997,1.1254783868789673,-18.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Rita_de_Minas,2022-12-31T00:00:00,1.019909502262443,1.2798398733139038,25.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santana_do_Manhuacu,2022-12-31T00:00:00,1.26,1.5108426809310913,19.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Francisco_do_Gloria,2022-12-31T00:00:00,0.9601476014760146,0.853904128074646,-11.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Joao_do_Manhuacu,2022-12-31T00:00:00,1.2,1.1720709800720215,-2.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Jose_do_Mantimento,2022-12-31T00:00:00,1.499236641221374,1.6438285112380981,9.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sericita,2022-12-31T00:00:00,1.3799999999999997,1.0933572053909302,-20.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Setubinha,2022-12-31T00:00:00,1.079761904761905,0.9397911429405212,-12.96,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Teofilo_Otoni,2022-12-31T00:00:00,0.8181818181818183,0.8224025368690491,0.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Tombos,2022-12-31T00:00:00,1.2,0.9351243376731873,-22.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ubaporanga,2022-12-31T00:00:00,1.2,1.3670146465301514,13.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Urucuia,2022-12-31T00:00:00,1.9193245778611627,2.120194911956787,10.47,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vermelho_Novo,2022-12-31T00:00:00,0.9598214285714286,1.0792741775512695,12.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vicosa,2022-12-31T00:00:00,1.079768786127168,0.9101079702377319,-15.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vieiras,2022-12-31T00:00:00,1.32,0.941681981086731,-28.66,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Virginopolis,2022-12-31T00:00:00,1.314606741573034,1.3548601865768433,3.06,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Agua_Boa,2023-12-31T00:00:00,1.213636363636364,0.8256810307502747,-31.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Aguas_Vermelhas,2023-12-31T00:00:00,3.0,3.1659555435180664,5.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Alto_Caparao,2023-12-31T00:00:00,1.32,1.4057978391647339,6.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Araponga,2023-12-31T00:00:00,1.5,1.3645093441009521,-9.03,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Areado,2023-12-31T00:00:00,1.590034364261168,1.5014725923538208,-5.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ataleia,2023-12-31T00:00:00,0.6833333333333333,0.8506358861923218,24.48,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Bom_Jesus_do_Galho,2023-12-31T00:00:00,1.2003192338387871,1.0529999732971191,-12.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caiana,2023-12-31T00:00:00,1.08,1.0569119453430176,-2.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Cajuri,2023-12-31T00:00:00,0.8200000000000001,1.3804935216903687,68.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Canaa,2023-12-31T00:00:00,1.560135135135135,1.423039197921753,-8.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caparao,2023-12-31T00:00:00,1.32,1.288364052772522,-2.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caputira,2023-12-31T00:00:00,1.380065005417118,1.5068508386611938,9.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carai,2023-12-31T00:00:00,1.2,1.2704441547393799,5.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carangola,2023-12-31T00:00:00,1.140118343195266,1.181028962135315,3.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caratinga,2023-12-31T00:00:00,1.3799999999999997,1.5813405513763428,14.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Chale,2023-12-31T00:00:00,1.3799999999999997,1.4308407306671143,3.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Conceicao_de_Ipanema,2023-12-31T00:00:00,1.5,1.40500009059906,-6.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Datas,2023-12-31T00:00:00,0.6666666666666667,1.1259254217147827,68.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Divino,2023-12-31T00:00:00,1.02,1.2963459491729736,27.09,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Durande,2023-12-31T00:00:00,1.3799999999999997,1.5823581218719482,14.66,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Entre_Folhas,2023-12-31T00:00:00,1.3210526315789468,1.0860748291015625,-17.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ervalia,2023-12-31T00:00:00,1.380044843049327,1.3270633220672607,-3.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Espera_Feliz,2023-12-31T00:00:00,1.08,1.2039669752120972,11.48,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Faria_Lemos,2023-12-31T00:00:00,1.380152671755725,1.115354299545288,-19.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ferros,2023-12-31T00:00:00,1.17948717948718,1.209876298904419,2.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Franciscopolis,2023-12-31T00:00:00,1.2,1.3806486129760742,15.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Itanhomi,2023-12-31T00:00:00,1.078787878787879,1.0829102993011475,0.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Formosa,2023-12-31T00:00:00,1.978723404255319,2.112377643585205,6.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Grande,2023-12-31T00:00:00,2.4,1.3329871892929077,-44.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lajinha,2023-12-31T00:00:00,1.4399536768963517,1.430774211883545,-0.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Luisburgo,2023-12-31T00:00:00,1.44,1.6064238548278809,11.56,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Malacacheta,2023-12-31T00:00:00,1.619230769230769,1.699800729751587,4.98,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhuacu,2023-12-31T00:00:00,1.2,1.2234456539154053,1.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhumirim,2023-12-31T00:00:00,1.44,1.347348690032959,-6.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Matipo,2023-12-31T00:00:00,1.25990675990676,1.2833504676818848,1.86,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mirai,2023-12-31T00:00:00,1.08,1.167971134185791,8.15,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Muriae,2023-12-31T00:00:00,1.057297297297297,1.0824637413024902,2.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mutum,2023-12-31T00:00:00,1.019985196150999,1.3703100681304932,34.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Nova_Belem,2023-12-31T00:00:00,0.96,1.0104453563690186,5.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Orizania,2023-12-31T00:00:00,1.2,1.3854999542236328,15.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Patis,2023-12-31T00:00:00,1.2,2.4463818073272705,103.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ponte_Nova,2023-12-31T00:00:00,1.333333333333333,1.2540242671966553,-5.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Porto_Firme,2023-12-31T00:00:00,1.200764818355641,1.381263256072998,15.03,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Raul_Soares,2023-12-31T00:00:00,1.5,1.144949197769165,-23.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Reduto,2023-12-31T00:00:00,0.9,1.3313226699829102,47.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rio_Pardo_de_Minas,2023-12-31T00:00:00,2.568224299065421,2.7409117221832275,6.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rosario_da_Limeira,2023-12-31T00:00:00,1.2,1.0665127038955688,-11.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Barbara_do_Leste,2023-12-31T00:00:00,1.08,1.2309964895248413,13.98,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Margarida,2023-12-31T00:00:00,1.32,1.2787951231002808,-3.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Rita_de_Minas,2023-12-31T00:00:00,1.019930675909879,1.2034157514572144,17.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santana_do_Manhuacu,2023-12-31T00:00:00,1.32,1.3739076852798462,4.08,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Francisco_do_Gloria,2023-12-31T00:00:00,1.08,1.1223596334457397,3.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Joao_do_Manhuacu,2023-12-31T00:00:00,1.2,1.2305222749710083,2.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Jose_do_Mantimento,2023-12-31T00:00:00,1.5,1.5680482387542725,4.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sericita,2023-12-31T00:00:00,1.26,1.4931719303131104,18.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Setubinha,2023-12-31T00:00:00,1.2,0.9914721250534058,-17.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Teofilo_Otoni,2023-12-31T00:00:00,1.2,0.8378086686134338,-30.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Tombos,2023-12-31T00:00:00,1.32,1.2712446451187134,-3.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ubaporanga,2023-12-31T00:00:00,1.5,1.3377492427825928,-10.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Urucuia,2023-12-31T00:00:00,2.0994575045207946,1.9799493551254272,-5.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vermelho_Novo,2023-12-31T00:00:00,1.5,1.043277382850647,-30.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vicosa,2023-12-31T00:00:00,1.5004329004329,1.1818770170211792,-21.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vieiras,2023-12-31T00:00:00,1.5,1.259666919708252,-16.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Virginopolis,2023-12-31T00:00:00,1.3258426966292132,1.366795301437378,3.09,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Agua_Boa,2024-12-31T00:00:00,1.7064935064935067,1.1603567600250244,-32.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Aguas_Vermelhas,2024-12-31T00:00:00,3.0,258.5863037109375,8519.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Alto_Caparao,2024-12-31T00:00:00,1.14,1.3464345932006836,18.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Araponga,2024-12-31T00:00:00,1.5,1.4385457038879395,-4.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Areado,2024-12-31T00:00:00,1.162105263157895,1.3236753940582275,13.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ataleia,2024-12-31T00:00:00,0.7529411764705883,0.7389971017837524,-1.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Bom_Jesus_do_Galho,2024-12-31T00:00:00,0.9,1.067195177078247,18.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caiana,2024-12-31T00:00:00,1.14,0.9898263216018677,-13.17,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Cajuri,2024-12-31T00:00:00,1.32,1.1823394298553467,-10.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Canaa,2024-12-31T00:00:00,1.319791666666667,1.4462782144546509,9.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caparao,2024-12-31T00:00:00,1.6799283154121862,1.2618632316589355,-24.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caputira,2024-12-31T00:00:00,1.38008658008658,1.4185841083526611,2.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carai,2024-12-31T00:00:00,1.2,1.2158598899841309,1.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carangola,2024-12-31T00:00:00,1.44,1.0769500732421875,-25.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caratinga,2024-12-31T00:00:00,1.6200335289186922,1.4452930688858032,-10.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Chale,2024-12-31T00:00:00,1.320168067226891,1.3891464471817017,5.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Conceicao_de_Ipanema,2024-12-31T00:00:00,1.2901734104046243,1.4633400440216064,13.42,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Datas,2024-12-31T00:00:00,1.0,0.9550356268882751,-4.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Divino,2024-12-31T00:00:00,1.2,1.0692338943481445,-10.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Durande,2024-12-31T00:00:00,1.380095693779904,1.4301159381866455,3.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Entre_Folhas,2024-12-31T00:00:00,1.2,1.2145962715148926,1.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ervalia,2024-12-31T00:00:00,1.32,1.5934958457946777,20.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Espera_Feliz,2024-12-31T00:00:00,1.44,1.0763026475906372,-25.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Faria_Lemos,2024-12-31T00:00:00,1.37956204379562,1.1809204816818237,-14.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ferros,2024-12-31T00:00:00,1.205128205128205,1.2150003910064697,0.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Franciscopolis,2024-12-31T00:00:00,0.9,1.2538764476776123,39.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Itanhomi,2024-12-31T00:00:00,1.02089552238806,1.1080724000930786,8.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Formosa,2024-12-31T00:00:00,1.8142857142857145,2.080212116241455,14.66,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Grande,2024-12-31T00:00:00,2.4,3.3741252422332764,40.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lajinha,2024-12-31T00:00:00,1.564997053624043,1.4032433032989502,-10.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Luisburgo,2024-12-31T00:00:00,1.2,1.4570574760437012,21.42,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Malacacheta,2024-12-31T00:00:00,1.5,1.7158081531524658,14.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhuacu,2024-12-31T00:00:00,1.5,1.6279900074005127,8.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhumirim,2024-12-31T00:00:00,1.56,1.6316992044448853,4.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Matipo,2024-12-31T00:00:00,1.2,1.2690660953521729,5.76,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mirai,2024-12-31T00:00:00,1.2,1.189749002456665,-0.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Muriae,2024-12-31T00:00:00,1.170285714285714,0.9966264963150024,-14.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mutum,2024-12-31T00:00:00,1.15997150997151,1.2514629364013672,7.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Nova_Belem,2024-12-31T00:00:00,0.9,0.99993896484375,11.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Orizania,2024-12-31T00:00:00,1.5,1.4936131238937378,-0.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Patis,2024-12-31T00:00:00,0.9,2.708495855331421,200.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ponte_Nova,2024-12-31T00:00:00,2.0,1.3402125835418701,-32.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Porto_Firme,2024-12-31T00:00:00,1.3810526315789473,1.3262988328933716,-3.96,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Raul_Soares,2024-12-31T00:00:00,1.5,1.3411540985107422,-10.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Reduto,2024-12-31T00:00:00,1.2,1.370082139968872,14.17,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rio_Pardo_de_Minas,2024-12-31T00:00:00,3.064285714285715,2.7601256370544434,-9.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rosario_da_Limeira,2024-12-31T00:00:00,1.259493670886076,1.272317886352539,1.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Barbara_do_Leste,2024-12-31T00:00:00,1.4101226993865028,1.1712846755981445,-16.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Margarida,2024-12-31T00:00:00,1.5,1.3955720663070679,-6.96,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Rita_de_Minas,2024-12-31T00:00:00,1.200173310225303,1.0994796752929688,-8.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santana_do_Manhuacu,2024-12-31T00:00:00,1.08,1.3475714921951294,24.78,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Francisco_do_Gloria,2024-12-31T00:00:00,1.25,1.0091572999954224,-19.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Joao_do_Manhuacu,2024-12-31T00:00:00,1.32,1.2026864290237427,-8.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Jose_do_Mantimento,2024-12-31T00:00:00,1.44,1.5003087520599365,4.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sericita,2024-12-31T00:00:00,1.3799999999999997,1.2929290533065796,-6.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Setubinha,2024-12-31T00:00:00,1.020238095238095,1.2127299308776855,18.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Teofilo_Otoni,2024-12-31T00:00:00,1.05,1.026991367340088,-2.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Tombos,2024-12-31T00:00:00,1.5,1.2262921333312988,-18.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ubaporanga,2024-12-31T00:00:00,1.440056417489422,1.3724796772003174,-4.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Urucuia,2024-12-31T00:00:00,1.920433996383363,2.089486837387085,8.8,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vermelho_Novo,2024-12-31T00:00:00,1.8000000000000005,1.2921562194824219,-28.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vicosa,2024-12-31T00:00:00,1.32,1.3014566898345947,-1.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vieiras,2024-12-31T00:00:00,1.44,1.4178013801574707,-1.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Virginopolis,2024-12-31T00:00:00,1.325,1.3691271543502808,3.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Agua_Boa,2025-12-31T00:00:00,0.0,1.2015471458435059,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Aguas_Vermelhas,2025-12-31T00:00:00,0.0,2.3105955123901367,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Alto_Caparao,2025-12-31T00:00:00,0.0,1.3537366390228271,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Araponga,2025-12-31T00:00:00,0.0,1.4384691715240479,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Areado,2025-12-31T00:00:00,0.0,1.2909300327301025,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ataleia,2025-12-31T00:00:00,0.0,0.7085819244384766,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Bom_Jesus_do_Galho,2025-12-31T00:00:00,0.0,0.9742202758789062,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caiana,2025-12-31T00:00:00,0.0,1.054349660873413,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Cajuri,2025-12-31T00:00:00,0.0,1.0771493911743164,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Canaa,2025-12-31T00:00:00,0.0,1.4138166904449463,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caparao,2025-12-31T00:00:00,0.0,1.4587767124176025,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caputira,2025-12-31T00:00:00,0.0,1.4237768650054932,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carai,2025-12-31T00:00:00,0.0,1.1186102628707886,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Carangola,2025-12-31T00:00:00,0.0,1.2426221370697021,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Caratinga,2025-12-31T00:00:00,0.0,1.5629960298538208,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Chale,2025-12-31T00:00:00,0.0,1.382973313331604,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Conceicao_de_Ipanema,2025-12-31T00:00:00,0.0,1.430751085281372,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Datas,2025-12-31T00:00:00,0.0,0.9048478603363037,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Divino,2025-12-31T00:00:00,0.0,1.1141093969345093,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Durande,2025-12-31T00:00:00,0.0,1.4160809516906738,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Entre_Folhas,2025-12-31T00:00:00,0.0,1.251522183418274,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ervalia,2025-12-31T00:00:00,0.0,1.3093427419662476,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Espera_Feliz,2025-12-31T00:00:00,0.0,1.2576924562454224,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Faria_Lemos,2025-12-31T00:00:00,0.0,1.334792971611023,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ferros,2025-12-31T00:00:00,0.0,1.2096861600875854,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Franciscopolis,2025-12-31T00:00:00,0.0,1.090124249458313,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Itanhomi,2025-12-31T00:00:00,0.0,1.0547069311141968,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Formosa,2025-12-31T00:00:00,0.0,1.9799983501434326,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lagoa_Grande,2025-12-31T00:00:00,0.0,5.0397868156433105,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Lajinha,2025-12-31T00:00:00,0.0,1.480830430984497,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Luisburgo,2025-12-31T00:00:00,0.0,1.4194412231445312,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Malacacheta,2025-12-31T00:00:00,0.0,1.5542950630187988,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhuacu,2025-12-31T00:00:00,0.0,1.4331865310668945,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Manhumirim,2025-12-31T00:00:00,0.0,1.4898420572280884,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Matipo,2025-12-31T00:00:00,0.0,1.2440900802612305,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mirai,2025-12-31T00:00:00,0.0,1.1737116575241089,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Muriae,2025-12-31T00:00:00,0.0,1.0955190658569336,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Mutum,2025-12-31T00:00:00,0.0,1.1654542684555054,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Nova_Belem,2025-12-31T00:00:00,0.0,0.9386270642280579,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Orizania,2025-12-31T00:00:00,0.0,1.4534878730773926,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Patis,2025-12-31T00:00:00,0.0,1.3186533451080322,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ponte_Nova,2025-12-31T00:00:00,0.0,1.5564638376235962,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Porto_Firme,2025-12-31T00:00:00,0.0,1.3069558143615723,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Raul_Soares,2025-12-31T00:00:00,0.0,1.325526237487793,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Reduto,2025-12-31T00:00:00,0.0,1.1349817514419556,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rio_Pardo_de_Minas,2025-12-31T00:00:00,0.0,2.61246919631958,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Rosario_da_Limeira,2025-12-31T00:00:00,0.0,1.17136812210083,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Barbara_do_Leste,2025-12-31T00:00:00,0.0,1.2342232465744019,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Margarida,2025-12-31T00:00:00,0.0,1.4363054037094116,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santa_Rita_de_Minas,2025-12-31T00:00:00,0.0,1.1141908168792725,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Santana_do_Manhuacu,2025-12-31T00:00:00,0.0,1.2449493408203125,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Francisco_do_Gloria,2025-12-31T00:00:00,0.0,1.120492696762085,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Joao_do_Manhuacu,2025-12-31T00:00:00,0.0,1.2676756381988525,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sao_Jose_do_Mantimento,2025-12-31T00:00:00,0.0,1.481005072593689,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Sericita,2025-12-31T00:00:00,0.0,1.3537030220031738,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Setubinha,2025-12-31T00:00:00,0.0,1.087822437286377,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Teofilo_Otoni,2025-12-31T00:00:00,0.0,1.0061932802200317,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Tombos,2025-12-31T00:00:00,0.0,1.3489995002746582,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Ubaporanga,2025-12-31T00:00:00,0.0,1.4113490581512451,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Urucuia,2025-12-31T00:00:00,0.0,1.957287311553955,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vermelho_Novo,2025-12-31T00:00:00,0.0,1.5037175416946411,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vicosa,2025-12-31T00:00:00,0.0,1.3405563831329346,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Vieiras,2025-12-31T00:00:00,0.0,1.4314467906951904,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 0 (2020-225),Virginopolis,2025-12-31T00:00:00,0.0,1.321561336517334,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0019397366519644814
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 100
",2025-09-23T22:55:31
cluster 1 (2020-225),Abre_Campo,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Abre_Campo,2013-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Abre_Campo,2014-12-31T00:00:00,0.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Abre_Campo,2015-12-31T00:00:00,1.2,0.993851363658905,17.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Abre_Campo,2016-12-31T00:00:00,1.2,1.0725986957550049,10.62,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Abre_Campo,2017-12-31T00:00:00,0.96,1.108498215675354,15.47,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Abre_Campo,2018-12-31T00:00:00,1.2,1.2300697565078735,2.51,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aimores,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aimores,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aimores,2014-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aimores,2015-12-31T00:00:00,1.08,1.2405723333358765,14.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aimores,2016-12-31T00:00:00,1.2,1.1976548433303833,0.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aimores,2017-12-31T00:00:00,1.23,1.2224606275558472,0.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aimores,2018-12-31T00:00:00,1.2,1.2193877696990967,1.62,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Alvarenga,2012-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Alvarenga,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Alvarenga,2014-12-31T00:00:00,0.84,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Alvarenga,2015-12-31T00:00:00,0.78,0.9016880989074707,15.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Alvarenga,2016-12-31T00:00:00,0.74,0.8508825302124023,14.98,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Alvarenga,2017-12-31T00:00:00,0.63,0.7966284155845642,26.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Alvarenga,2018-12-31T00:00:00,0.72,0.771660566329956,7.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Amparo_do_Serra,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Amparo_do_Serra,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Amparo_do_Serra,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Amparo_do_Serra,2015-12-31T00:00:00,1.08,0.9765912294387817,9.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Amparo_do_Serra,2016-12-31T00:00:00,1.2,1.0895957946777344,9.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Amparo_do_Serra,2017-12-31T00:00:00,1.29,1.1069527864456177,14.19,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Amparo_do_Serra,2018-12-31T00:00:00,1.5,1.2861651182174683,14.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Angelandia,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Angelandia,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Angelandia,2014-12-31T00:00:00,1.15,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Angelandia,2015-12-31T00:00:00,0.96,1.1504894495010376,19.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Angelandia,2016-12-31T00:00:00,0.96,1.1455010175704956,19.32,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Angelandia,2017-12-31T00:00:00,0.94,1.0368157625198364,10.3,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Angelandia,2018-12-31T00:00:00,1.68,0.9574247002601624,43.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Dias,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Dias,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Dias,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Dias,2015-12-31T00:00:00,1.08,1.102483868598938,2.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Dias,2016-12-31T00:00:00,0.9,1.0881520509719849,20.91,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Dias,2017-12-31T00:00:00,1.0,0.9745687246322632,2.54,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Dias,2018-12-31T00:00:00,0.8,1.0664726495742798,33.31,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Prado_de_Minas,2012-12-31T00:00:00,0.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Prado_de_Minas,2013-12-31T00:00:00,0.69,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Prado_de_Minas,2014-12-31T00:00:00,0.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Prado_de_Minas,2015-12-31T00:00:00,1.08,0.7096351385116577,34.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Prado_de_Minas,2016-12-31T00:00:00,0.9,0.8428934812545776,6.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Prado_de_Minas,2017-12-31T00:00:00,0.85,0.9283255338668823,9.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Prado_de_Minas,2018-12-31T00:00:00,1.2,1.0252909660339355,14.56,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aricanduva,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aricanduva,2013-12-31T00:00:00,0.87,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aricanduva,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aricanduva,2015-12-31T00:00:00,1.2,0.8923090100288391,25.64,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aricanduva,2016-12-31T00:00:00,1.08,1.060394525527954,1.82,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aricanduva,2017-12-31T00:00:00,0.87,1.0756906270980835,23.64,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aricanduva,2018-12-31T00:00:00,0.96,1.121495008468628,16.82,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bandeira,2012-12-31T00:00:00,0.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bandeira,2013-12-31T00:00:00,0.84,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bandeira,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bandeira,2015-12-31T00:00:00,0.96,0.9046852588653564,5.76,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bandeira,2016-12-31T00:00:00,0.9,1.0408059358596802,15.65,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bandeira,2017-12-31T00:00:00,0.9,1.0179625749588013,13.11,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bandeira,2018-12-31T00:00:00,0.9,0.939746618270874,4.42,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berilo,2012-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berilo,2013-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berilo,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berilo,2015-12-31T00:00:00,0.9,0.9439396262168884,4.88,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berilo,2016-12-31T00:00:00,0.6,0.9256218075752258,54.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berilo,2017-12-31T00:00:00,1.0,0.8282909393310547,17.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berilo,2018-12-31T00:00:00,0.71,0.9201989769935608,29.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berizal,2012-12-31T00:00:00,0.62,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berizal,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berizal,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berizal,2015-12-31T00:00:00,1.08,0.8019779920578003,25.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berizal,2016-12-31T00:00:00,3.0,1.0172901153564453,66.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berizal,2017-12-31T00:00:00,0.55,1.515988826751709,175.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berizal,2018-12-31T00:00:00,1.2,1.7554819583892822,46.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bocaiuva,2012-12-31T00:00:00,2.7,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bocaiuva,2013-12-31T00:00:00,2.7,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bocaiuva,2014-12-31T00:00:00,2.7,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bocaiuva,2015-12-31T00:00:00,1.8,2.857618808746338,58.76,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bocaiuva,2016-12-31T00:00:00,1.8,2.6205763816833496,45.59,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bocaiuva,2017-12-31T00:00:00,1.62,2.1430771350860596,32.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bocaiuva,2018-12-31T00:00:00,1.8,1.7895252704620361,0.58,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bom_Jesus_do_Amparo,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bom_Jesus_do_Amparo,2013-12-31T00:00:00,0.78,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bom_Jesus_do_Amparo,2014-12-31T00:00:00,0.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bom_Jesus_do_Amparo,2015-12-31T00:00:00,0.9,0.844684362411499,6.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bom_Jesus_do_Amparo,2016-12-31T00:00:00,1.2,0.8139121532440186,32.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bom_Jesus_do_Amparo,2017-12-31T00:00:00,1.0,0.9689761996269226,3.1,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bom_Jesus_do_Amparo,2018-12-31T00:00:00,2.2,1.1837348937988281,46.19,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capela_Nova,2012-12-31T00:00:00,1.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capela_Nova,2013-12-31T00:00:00,1.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capela_Nova,2014-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capela_Nova,2015-12-31T00:00:00,1.5,1.8310701847076416,22.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capela_Nova,2016-12-31T00:00:00,2.04,1.8475227355957031,9.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capela_Nova,2017-12-31T00:00:00,1.84,1.9246870279312134,4.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capela_Nova,2018-12-31T00:00:00,2.1,1.970410943031311,6.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capelinha,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capelinha,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capelinha,2014-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capelinha,2015-12-31T00:00:00,0.9,1.4414933919906616,60.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capelinha,2016-12-31T00:00:00,1.08,1.4681694507598877,35.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capelinha,2017-12-31T00:00:00,1.16,1.0989267826080322,5.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capelinha,2018-12-31T00:00:00,1.26,1.0991617441177368,12.76,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caranaiba,2012-12-31T00:00:00,0.33,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caranaiba,2013-12-31T00:00:00,0.33,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caranaiba,2014-12-31T00:00:00,0.33,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caranaiba,2015-12-31T00:00:00,2.47,0.21855130791664124,91.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caranaiba,2016-12-31T00:00:00,2.47,0.9987336993217468,59.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caranaiba,2017-12-31T00:00:00,0.45,1.7218272686004639,282.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caranaiba,2018-12-31T00:00:00,0.61,2.4649624824523926,304.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carmo_da_Mata,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carmo_da_Mata,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carmo_da_Mata,2014-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carmo_da_Mata,2015-12-31T00:00:00,1.2,1.4574003219604492,21.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carmo_da_Mata,2016-12-31T00:00:00,1.2,1.407206654548645,17.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carmo_da_Mata,2017-12-31T00:00:00,1.23,1.2917828559875488,5.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carmo_da_Mata,2018-12-31T00:00:00,1.5,1.222875714302063,18.47,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carrancas,2012-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carrancas,2013-12-31T00:00:00,1.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carrancas,2014-12-31T00:00:00,1.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carrancas,2015-12-31T00:00:00,0.7,1.4509375095367432,107.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carrancas,2016-12-31T00:00:00,1.2,1.2613195180892944,5.11,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carrancas,2017-12-31T00:00:00,1.96,1.000084400177002,48.98,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carrancas,2018-12-31T00:00:00,1.5,1.630631685256958,8.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Casa_Grande,2012-12-31T00:00:00,1.23,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Casa_Grande,2013-12-31T00:00:00,1.23,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Casa_Grande,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Casa_Grande,2015-12-31T00:00:00,1.32,1.2210735082626343,7.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Casa_Grande,2016-12-31T00:00:00,1.51,1.2474853992462158,17.39,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Casa_Grande,2017-12-31T00:00:00,0.71,1.3632581233978271,92.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Casa_Grande,2018-12-31T00:00:00,1.21,1.4598758220672607,20.65,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cataguases,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cataguases,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cataguases,2014-12-31T00:00:00,0.87,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cataguases,2015-12-31T00:00:00,0.62,0.8753901720046997,41.19,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cataguases,2016-12-31T00:00:00,0.67,0.8358043432235718,24.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cataguases,2017-12-31T00:00:00,2.0,0.7505085468292236,62.47,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cataguases,2018-12-31T00:00:00,0.5,1.46254563331604,192.51,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catas_Altas_da_Noruega,2012-12-31T00:00:00,1.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catas_Altas_da_Noruega,2013-12-31T00:00:00,1.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catas_Altas_da_Noruega,2014-12-31T00:00:00,1.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catas_Altas_da_Noruega,2015-12-31T00:00:00,1.0,1.1456151008605957,14.56,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catas_Altas_da_Noruega,2016-12-31T00:00:00,1.2,1.4207733869552612,18.4,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catas_Altas_da_Noruega,2017-12-31T00:00:00,0.87,1.0949949026107788,25.86,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catas_Altas_da_Noruega,2018-12-31T00:00:00,0.62,1.13588285446167,83.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catuji,2012-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catuji,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catuji,2014-12-31T00:00:00,0.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catuji,2015-12-31T00:00:00,0.9,0.7399082183837891,17.79,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catuji,2016-12-31T00:00:00,1.2,0.8670943379402161,27.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catuji,2017-12-31T00:00:00,0.47,0.9643027186393738,105.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catuji,2018-12-31T00:00:00,0.9,1.0175474882125854,13.06,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caxambu,2012-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caxambu,2013-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caxambu,2014-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caxambu,2015-12-31T00:00:00,1.2,1.265114426612854,5.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caxambu,2016-12-31T00:00:00,1.5,1.2724778652191162,15.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caxambu,2017-12-31T00:00:00,0.59,1.3625274896621704,130.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caxambu,2018-12-31T00:00:00,0.9,1.320541262626648,46.73,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Claudio,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Claudio,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Claudio,2014-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Claudio,2015-12-31T00:00:00,0.9,1.4571915864944458,61.91,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Claudio,2016-12-31T00:00:00,1.2,1.3365275859832764,11.38,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Claudio,2017-12-31T00:00:00,1.42,1.2324450016021729,13.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Claudio,2018-12-31T00:00:00,1.5,1.3123583793640137,12.51,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coimbra,2012-12-31T00:00:00,1.53,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coimbra,2013-12-31T00:00:00,1.53,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coimbra,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coimbra,2015-12-31T00:00:00,1.5,1.3880289793014526,7.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coimbra,2016-12-31T00:00:00,1.32,1.4376789331436157,8.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coimbra,2017-12-31T00:00:00,1.86,1.368202805519104,26.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coimbra,2018-12-31T00:00:00,1.8,1.8497813940048218,2.77,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Conselheiro_Pena,2012-12-31T00:00:00,1.48,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Conselheiro_Pena,2013-12-31T00:00:00,1.3,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Conselheiro_Pena,2014-12-31T00:00:00,1.3,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Conselheiro_Pena,2015-12-31T00:00:00,1.06,1.3615992069244385,28.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Conselheiro_Pena,2016-12-31T00:00:00,1.18,1.245009183883667,5.51,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Conselheiro_Pena,2017-12-31T00:00:00,1.29,1.2100181579589844,6.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Conselheiro_Pena,2018-12-31T00:00:00,1.62,1.2517201900482178,22.73,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coroaci,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coroaci,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coroaci,2014-12-31T00:00:00,1.04,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coroaci,2015-12-31T00:00:00,1.32,0.9495272636413574,28.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coroaci,2016-12-31T00:00:00,1.74,1.1498570442199707,33.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coroaci,2017-12-31T00:00:00,1.73,1.3417000770568848,22.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coroaci,2018-12-31T00:00:00,1.56,1.7867724895477295,14.54,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Corrego_Novo,2012-12-31T00:00:00,1.17,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Corrego_Novo,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Corrego_Novo,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Corrego_Novo,2015-12-31T00:00:00,1.2,1.1477545499801636,4.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Corrego_Novo,2016-12-31T00:00:00,0.96,1.1775047779083252,22.66,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Corrego_Novo,2017-12-31T00:00:00,0.5,1.099082350730896,119.82,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Corrego_Novo,2018-12-31T00:00:00,0.66,1.1715879440307617,77.51,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cruzilia,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cruzilia,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cruzilia,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cruzilia,2015-12-31T00:00:00,1.12,0.7267627716064453,35.11,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cruzilia,2016-12-31T00:00:00,1.5,1.185738444328308,20.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cruzilia,2017-12-31T00:00:00,0.9,1.3061732053756714,45.13,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cruzilia,2018-12-31T00:00:00,1.2,1.3328661918640137,11.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cuparaque,2012-12-31T00:00:00,1.06,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cuparaque,2013-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cuparaque,2014-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cuparaque,2015-12-31T00:00:00,0.84,1.054893970489502,25.58,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cuparaque,2016-12-31T00:00:00,1.02,1.0047242641448975,1.5,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cuparaque,2017-12-31T00:00:00,0.74,0.9817155003547668,32.66,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cuparaque,2018-12-31T00:00:00,1.02,0.9758036136627197,4.33,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Diamantina,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Diamantina,2013-12-31T00:00:00,1.31,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Diamantina,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Diamantina,2015-12-31T00:00:00,1.87,1.4360041618347168,23.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Diamantina,2016-12-31T00:00:00,1.8,1.601273775100708,11.04,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Diamantina,2017-12-31T00:00:00,1.42,1.761022686958313,24.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Diamantina,2018-12-31T00:00:00,2.0,1.8304239511489868,8.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Divisopolis,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Divisopolis,2013-12-31T00:00:00,1.74,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Divisopolis,2014-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Divisopolis,2015-12-31T00:00:00,0.66,1.5523078441619873,135.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Divisopolis,2016-12-31T00:00:00,0.72,1.4932255744934082,107.39,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Divisopolis,2017-12-31T00:00:00,1.08,0.9666924476623535,10.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Divisopolis,2018-12-31T00:00:00,1.2,0.9034034013748169,24.72,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Dores_do_Turvo,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Dores_do_Turvo,2013-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Dores_do_Turvo,2014-12-31T00:00:00,1.3,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Dores_do_Turvo,2015-12-31T00:00:00,1.32,1.3109705448150635,0.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Dores_do_Turvo,2016-12-31T00:00:00,1.32,1.3170197010040283,0.23,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Dores_do_Turvo,2017-12-31T00:00:00,0.6,1.3162589073181152,119.38,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Dores_do_Turvo,2018-12-31T00:00:00,0.89,1.4087965488433838,58.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Esmeraldas,2012-12-31T00:00:00,2.41,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Esmeraldas,2013-12-31T00:00:00,1.51,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Esmeraldas,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Esmeraldas,2015-12-31T00:00:00,1.5,1.5359830856323242,2.4,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Esmeraldas,2016-12-31T00:00:00,1.5,1.4326038360595703,4.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Esmeraldas,2017-12-31T00:00:00,2.27,1.401095986366272,38.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Esmeraldas,2018-12-31T00:00:00,2.1,2.060256004333496,1.89,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Espirito_Santo_do_Dourado,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Espirito_Santo_do_Dourado,2013-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Espirito_Santo_do_Dourado,2014-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Espirito_Santo_do_Dourado,2015-12-31T00:00:00,1.02,1.0582154989242554,3.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Espirito_Santo_do_Dourado,2016-12-31T00:00:00,1.14,1.0069663524627686,11.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Espirito_Santo_do_Dourado,2017-12-31T00:00:00,1.44,1.0550920963287354,26.73,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Espirito_Santo_do_Dourado,2018-12-31T00:00:00,1.8,1.3270058631896973,26.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Eugenopolis,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Eugenopolis,2013-12-31T00:00:00,1.71,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Eugenopolis,2014-12-31T00:00:00,1.71,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Eugenopolis,2015-12-31T00:00:00,1.5,1.4131839275360107,5.79,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Eugenopolis,2016-12-31T00:00:00,1.2,1.649468183517456,37.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Eugenopolis,2017-12-31T00:00:00,1.02,1.5213704109191895,49.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Eugenopolis,2018-12-31T00:00:00,1.2,1.3952484130859375,16.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Felicio_dos_Santos,2012-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Felicio_dos_Santos,2013-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Felicio_dos_Santos,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Felicio_dos_Santos,2015-12-31T00:00:00,1.81,1.6273077726364136,10.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Felicio_dos_Santos,2016-12-31T00:00:00,1.8,1.7002673149108887,5.54,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Felicio_dos_Santos,2017-12-31T00:00:00,1.25,1.735877513885498,38.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Felicio_dos_Santos,2018-12-31T00:00:00,1.3,1.795318365097046,38.1,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fervedouro,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fervedouro,2013-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fervedouro,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fervedouro,2015-12-31T00:00:00,1.08,1.2771648168563843,18.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fervedouro,2016-12-31T00:00:00,1.14,1.2290629148483276,7.81,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fervedouro,2017-12-31T00:00:00,1.06,1.1523197889328003,8.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fervedouro,2018-12-31T00:00:00,1.32,1.1239675283432007,14.85,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formiga,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formiga,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formiga,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formiga,2015-12-31T00:00:00,1.2,1.1999393701553345,0.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formiga,2016-12-31T00:00:00,1.5,1.199922800064087,20.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formiga,2017-12-31T00:00:00,1.8,1.3569345474243164,24.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formiga,2018-12-31T00:00:00,2.4,1.6667284965515137,30.55,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formoso,2012-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formoso,2013-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formoso,2014-12-31T00:00:00,2.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formoso,2015-12-31T00:00:00,1.8,2.528590440750122,40.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formoso,2016-12-31T00:00:00,3.0,2.3597750663757324,21.34,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formoso,2017-12-31T00:00:00,2.45,2.472128391265869,0.9,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formoso,2018-12-31T00:00:00,2.7,2.7411181926727295,1.52,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fortaleza_de_Minas,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fortaleza_de_Minas,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fortaleza_de_Minas,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fortaleza_de_Minas,2015-12-31T00:00:00,0.9,1.2107396125793457,34.53,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fortaleza_de_Minas,2016-12-31T00:00:00,1.0,1.1244590282440186,12.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fortaleza_de_Minas,2017-12-31T00:00:00,1.28,1.0051296949386597,21.47,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fortaleza_de_Minas,2018-12-31T00:00:00,1.8,1.141139030456543,36.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guaraciaba,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guaraciaba,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guaraciaba,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guaraciaba,2015-12-31T00:00:00,1.2,1.0301752090454102,14.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guaraciaba,2016-12-31T00:00:00,1.2,1.1616395711898804,3.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guaraciaba,2017-12-31T00:00:00,1.14,1.1363451480865479,0.32,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guaraciaba,2018-12-31T00:00:00,1.2,1.1979089975357056,0.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guiricema,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guiricema,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guiricema,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guiricema,2015-12-31T00:00:00,1.5,0.6903524994850159,53.98,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guiricema,2016-12-31T00:00:00,1.5,1.733543872833252,15.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guiricema,2017-12-31T00:00:00,1.9,1.6306140422821045,14.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guiricema,2018-12-31T00:00:00,1.8,1.8100453615188599,0.56,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Iapu,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Iapu,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Iapu,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Iapu,2015-12-31T00:00:00,1.2,0.6259428858757019,47.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Iapu,2016-12-31T00:00:00,1.2,1.029357671737671,14.22,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Iapu,2017-12-31T00:00:00,1.28,1.1428802013397217,10.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Iapu,2018-12-31T00:00:00,1.5,1.2593275308609009,16.04,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Imbe_de_Minas,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Imbe_de_Minas,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Imbe_de_Minas,2014-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Imbe_de_Minas,2015-12-31T00:00:00,1.2,1.0872735977172852,9.39,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Imbe_de_Minas,2016-12-31T00:00:00,0.9,1.1553316116333008,28.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Imbe_de_Minas,2017-12-31T00:00:00,1.29,1.0793545246124268,16.33,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Imbe_de_Minas,2018-12-31T00:00:00,1.5,1.2773430347442627,14.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Inhapim,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Inhapim,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Inhapim,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Inhapim,2015-12-31T00:00:00,1.21,1.371886134147644,13.38,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Inhapim,2016-12-31T00:00:00,1.5,1.3201078176498413,11.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Inhapim,2017-12-31T00:00:00,1.12,1.3511216640472412,20.64,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Inhapim,2018-12-31T00:00:00,1.5,1.4138989448547363,5.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Irai_de_Minas,2012-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Irai_de_Minas,2013-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Irai_de_Minas,2014-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Irai_de_Minas,2015-12-31T00:00:00,1.2,2.1642842292785645,80.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Irai_de_Minas,2016-12-31T00:00:00,1.8,2.030552625656128,12.81,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Irai_de_Minas,2017-12-31T00:00:00,0.87,1.9136549234390259,119.96,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Irai_de_Minas,2018-12-31T00:00:00,1.8,1.608062744140625,10.66,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itaipe,2012-12-31T00:00:00,0.84,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itaipe,2013-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itaipe,2014-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itaipe,2015-12-31T00:00:00,0.84,0.8336474895477295,0.76,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itaipe,2016-12-31T00:00:00,0.72,0.8909665942192078,23.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itaipe,2017-12-31T00:00:00,0.69,0.7477984428405762,8.38,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itaipe,2018-12-31T00:00:00,0.96,0.7878053784370422,17.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarandiba,2012-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarandiba,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarandiba,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarandiba,2015-12-31T00:00:00,1.2,1.0276665687561035,14.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarandiba,2016-12-31T00:00:00,1.2,1.1815632581710815,1.54,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarandiba,2017-12-31T00:00:00,1.64,1.1297473907470703,31.11,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarandiba,2018-12-31T00:00:00,1.8,1.4792394638061523,17.82,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarati_de_Minas,2012-12-31T00:00:00,1.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarati_de_Minas,2013-12-31T00:00:00,1.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarati_de_Minas,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarati_de_Minas,2015-12-31T00:00:00,0.9,0.9495118260383606,5.5,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarati_de_Minas,2016-12-31T00:00:00,0.9,0.9510564804077148,5.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarati_de_Minas,2017-12-31T00:00:00,0.74,2.0155558586120605,172.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarati_de_Minas,2018-12-31T00:00:00,0.9,0.8918212652206421,0.91,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequeri,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequeri,2013-12-31T00:00:00,1.86,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequeri,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequeri,2015-12-31T00:00:00,1.26,1.5627727508544922,24.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequeri,2016-12-31T00:00:00,1.2,1.4934661388397217,24.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequeri,2017-12-31T00:00:00,1.28,1.2281867265701294,4.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequeri,2018-12-31T00:00:00,1.5,1.284001111984253,14.4,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequitinhonha,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequitinhonha,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequitinhonha,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequitinhonha,2015-12-31T00:00:00,0.9,0.5554659366607666,38.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequitinhonha,2016-12-31T00:00:00,1.1,1.5358508825302124,39.62,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequitinhonha,2017-12-31T00:00:00,1.44,0.9774627089500427,32.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequitinhonha,2018-12-31T00:00:00,2.4,1.2974519729614258,45.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jose_Goncalves_de_Minas,2012-12-31T00:00:00,0.84,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jose_Goncalves_de_Minas,2013-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jose_Goncalves_de_Minas,2014-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jose_Goncalves_de_Minas,2015-12-31T00:00:00,0.9,0.6971635818481445,22.54,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jose_Goncalves_de_Minas,2016-12-31T00:00:00,1.2,0.7483630180358887,37.64,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jose_Goncalves_de_Minas,2017-12-31T00:00:00,0.6,0.9141895174980164,52.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jose_Goncalves_de_Minas,2018-12-31T00:00:00,1.2,1.0078963041305542,16.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Lamim,2012-12-31T00:00:00,1.05,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Lamim,2013-12-31T00:00:00,1.05,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Lamim,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Lamim,2015-12-31T00:00:00,1.8,1.2816007137298584,28.8,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Lamim,2016-12-31T00:00:00,1.05,1.5739037990570068,49.9,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Lamim,2017-12-31T00:00:00,1.2,1.6280437707901,35.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Lamim,2018-12-31T00:00:00,1.19,1.6010706424713135,34.54,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mantena,2012-12-31T00:00:00,0.42,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mantena,2013-12-31T00:00:00,0.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mantena,2014-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mantena,2015-12-31T00:00:00,1.01,0.5689107775688171,43.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mantena,2016-12-31T00:00:00,0.6,0.8143877387046814,35.73,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mantena,2017-12-31T00:00:00,0.64,0.778743326663971,21.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mantena,2018-12-31T00:00:00,0.9,0.9299324750900269,3.33,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mar_de_Espanha,2012-12-31T00:00:00,2.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mar_de_Espanha,2013-12-31T00:00:00,2.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mar_de_Espanha,2014-12-31T00:00:00,2.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mar_de_Espanha,2015-12-31T00:00:00,1.26,0.9622461199760437,23.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mar_de_Espanha,2016-12-31T00:00:00,1.2,1.7471446990966797,45.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mar_de_Espanha,2017-12-31T00:00:00,2.19,1.5945231914520264,27.19,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mar_de_Espanha,2018-12-31T00:00:00,1.8,1.7518224716186523,2.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mata_Verde,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mata_Verde,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mata_Verde,2014-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mata_Verde,2015-12-31T00:00:00,1.44,1.2537126541137695,12.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mata_Verde,2016-12-31T00:00:00,0.96,1.351069450378418,40.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mata_Verde,2017-12-31T00:00:00,0.6,1.2089530229568481,101.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mata_Verde,2018-12-31T00:00:00,1.5,1.1638821363449097,22.41,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Matutina,2012-12-31T00:00:00,1.92,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Matutina,2013-12-31T00:00:00,1.92,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Matutina,2014-12-31T00:00:00,1.53,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Matutina,2015-12-31T00:00:00,1.5,1.753277063369751,16.89,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Matutina,2016-12-31T00:00:00,1.5,1.6445794105529785,9.64,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Matutina,2017-12-31T00:00:00,1.75,1.5123118162155151,13.58,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Matutina,2018-12-31T00:00:00,2.1,1.679990291595459,20.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Minas_Novas,2012-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Minas_Novas,2013-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Minas_Novas,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Minas_Novas,2015-12-31T00:00:00,1.2,0.9448376893997192,21.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Minas_Novas,2016-12-31T00:00:00,1.2,1.0298091173171997,14.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Minas_Novas,2017-12-31T00:00:00,0.69,1.1128942966461182,61.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Minas_Novas,2018-12-31T00:00:00,0.9,1.141135334968567,26.79,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Monte_Santo_de_Minas,2012-12-31T00:00:00,1.89,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Monte_Santo_de_Minas,2013-12-31T00:00:00,1.62,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Monte_Santo_de_Minas,2014-12-31T00:00:00,1.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Monte_Santo_de_Minas,2015-12-31T00:00:00,1.35,1.6646984815597534,23.31,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Monte_Santo_de_Minas,2016-12-31T00:00:00,2.04,1.5691556930541992,23.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Monte_Santo_de_Minas,2017-12-31T00:00:00,1.08,1.681565523147583,55.7,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Monte_Santo_de_Minas,2018-12-31T00:00:00,1.88,1.9035612344741821,1.25,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Nova_Ponte,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Nova_Ponte,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Nova_Ponte,2014-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Nova_Ponte,2015-12-31T00:00:00,1.2,1.5816493034362793,31.8,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Nova_Ponte,2016-12-31T00:00:00,1.8,1.52180814743042,15.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Nova_Ponte,2017-12-31T00:00:00,1.07,1.6052236557006836,50.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Nova_Ponte,2018-12-31T00:00:00,1.8,1.566992998123169,12.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novo_Cruzeiro,2012-12-31T00:00:00,0.84,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novo_Cruzeiro,2013-12-31T00:00:00,0.84,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novo_Cruzeiro,2014-12-31T00:00:00,0.84,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novo_Cruzeiro,2015-12-31T00:00:00,0.9,0.9931807518005371,10.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novo_Cruzeiro,2016-12-31T00:00:00,0.84,0.8708741664886475,3.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novo_Cruzeiro,2017-12-31T00:00:00,1.05,0.868557333946228,17.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novo_Cruzeiro,2018-12-31T00:00:00,1.14,0.9982379078865051,12.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novorizonte,2012-12-31T00:00:00,1.48,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novorizonte,2013-12-31T00:00:00,0.93,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novorizonte,2014-12-31T00:00:00,0.91,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novorizonte,2015-12-31T00:00:00,0.87,1.1113418340682983,27.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novorizonte,2016-12-31T00:00:00,1.2,0.9231802225112915,23.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novorizonte,2017-12-31T00:00:00,0.71,1.0036358833312988,41.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novorizonte,2018-12-31T00:00:00,0.91,1.0073106288909912,10.69,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ouro_Verde_de_Minas,2012-12-31T00:00:00,0.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ouro_Verde_de_Minas,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ouro_Verde_de_Minas,2014-12-31T00:00:00,0.73,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ouro_Verde_de_Minas,2015-12-31T00:00:00,0.9,0.7742805480957031,13.97,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ouro_Verde_de_Minas,2016-12-31T00:00:00,1.2,0.8770950436592102,26.91,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ouro_Verde_de_Minas,2017-12-31T00:00:00,0.86,1.0009528398513794,16.39,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ouro_Verde_de_Minas,2018-12-31T00:00:00,1.2,1.1023920774459839,8.13,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Passos,2012-12-31T00:00:00,2.16,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Passos,2013-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Passos,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Passos,2015-12-31T00:00:00,0.9,1.3869744539260864,54.11,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Passos,2016-12-31T00:00:00,1.68,1.1570266485214233,31.13,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Passos,2017-12-31T00:00:00,1.92,1.2984719276428223,32.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Passos,2018-12-31T00:00:00,2.1,1.7581899166107178,16.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Paula_Candido,2012-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Paula_Candido,2013-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Paula_Candido,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Paula_Candido,2015-12-31T00:00:00,1.32,1.3431553840637207,1.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Paula_Candido,2016-12-31T00:00:00,1.26,1.343997836112976,6.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Paula_Candido,2017-12-31T00:00:00,1.37,1.274524211883545,6.97,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Paula_Candido,2018-12-31T00:00:00,1.5,1.3673498630523682,8.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pecanha,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pecanha,2013-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pecanha,2014-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pecanha,2015-12-31T00:00:00,0.91,0.7091258764266968,22.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pecanha,2016-12-31T00:00:00,0.91,0.765507161617279,15.88,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pecanha,2017-12-31T00:00:00,1.33,0.8243277072906494,38.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pecanha,2018-12-31T00:00:00,1.44,1.2038146257400513,16.4,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Azul,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Azul,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Azul,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Azul,2015-12-31T00:00:00,0.9,1.1753045320510864,30.59,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Azul,2016-12-31T00:00:00,0.9,1.6720163822174072,85.78,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Azul,2017-12-31T00:00:00,1.5,1.0215911865234375,31.89,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Azul,2018-12-31T00:00:00,1.2,1.199041485786438,0.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Bonita,2012-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Bonita,2013-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Bonita,2014-12-31T00:00:00,0.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Bonita,2015-12-31T00:00:00,0.6,0.9474573135375977,57.91,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Bonita,2016-12-31T00:00:00,1.32,0.8304988145828247,37.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Bonita,2017-12-31T00:00:00,0.86,0.9398409724235535,9.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Bonita,2018-12-31T00:00:00,1.2,1.2651499509811401,5.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Dourada,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Dourada,2013-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Dourada,2014-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Dourada,2015-12-31T00:00:00,0.96,0.9990370273590088,4.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Dourada,2016-12-31T00:00:00,0.96,1.3757845163345337,43.31,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Dourada,2017-12-31T00:00:00,1.34,1.5525038242340088,15.86,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Dourada,2018-12-31T00:00:00,1.2,1.2743717432022095,6.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_do_Anta,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_do_Anta,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_do_Anta,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_do_Anta,2015-12-31T00:00:00,1.26,1.1514177322387695,8.62,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_do_Anta,2016-12-31T00:00:00,1.2,1.199384331703186,0.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_do_Anta,2017-12-31T00:00:00,1.18,1.2093583345413208,2.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_do_Anta,2018-12-31T00:00:00,1.2,1.250293493270874,4.19,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Piedade_de_Caratinga,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Piedade_de_Caratinga,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Piedade_de_Caratinga,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Piedade_de_Caratinga,2015-12-31T00:00:00,1.2,1.1494944095611572,4.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Piedade_de_Caratinga,2016-12-31T00:00:00,1.2,1.1767035722732544,1.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Piedade_de_Caratinga,2017-12-31T00:00:00,1.59,1.179337978363037,25.83,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Piedade_de_Caratinga,2018-12-31T00:00:00,1.68,1.4661388397216797,12.73,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pocrane,2012-12-31T00:00:00,1.21,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pocrane,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pocrane,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pocrane,2015-12-31T00:00:00,1.08,1.1654725074768066,7.91,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pocrane,2016-12-31T00:00:00,1.09,1.1314585208892822,3.8,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pocrane,2017-12-31T00:00:00,1.32,1.0839974880218506,17.88,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pocrane,2018-12-31T00:00:00,1.32,1.25358247756958,5.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ponto_dos_Volantes,2012-12-31T00:00:00,0.7,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ponto_dos_Volantes,2013-12-31T00:00:00,0.3,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ponto_dos_Volantes,2014-12-31T00:00:00,0.7,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ponto_dos_Volantes,2015-12-31T00:00:00,0.6,0.5261203050613403,12.31,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ponto_dos_Volantes,2016-12-31T00:00:00,0.6,0.5477436184883118,8.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ponto_dos_Volantes,2017-12-31T00:00:00,0.38,0.6449247598648071,69.72,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ponto_dos_Volantes,2018-12-31T00:00:00,0.6,0.6102915406227112,1.72,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pote,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pote,2013-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pote,2014-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pote,2015-12-31T00:00:00,0.9,0.6923621892929077,23.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pote,2016-12-31T00:00:00,0.9,0.745530903339386,17.16,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pote,2017-12-31T00:00:00,0.6,0.8400402665138245,40.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pote,2018-12-31T00:00:00,0.73,0.8981629610061646,23.04,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pratinha,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pratinha,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pratinha,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pratinha,2015-12-31T00:00:00,1.5,1.5518888235092163,3.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pratinha,2016-12-31T00:00:00,1.5,1.6063534021377563,7.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pratinha,2017-12-31T00:00:00,1.72,1.628227949142456,5.34,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pratinha,2018-12-31T00:00:00,1.8,1.645289421081543,8.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Quartel_Geral,2012-12-31T00:00:00,4.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Quartel_Geral,2013-12-31T00:00:00,3.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Quartel_Geral,2014-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Quartel_Geral,2015-12-31T00:00:00,1.8,3.3510782718658447,86.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Quartel_Geral,2016-12-31T00:00:00,2.7,2.5916848182678223,4.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Quartel_Geral,2017-12-31T00:00:00,4.12,2.328472137451172,43.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Quartel_Geral,2018-12-31T00:00:00,4.5,3.7199618816375732,17.33,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ritapolis,2012-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ritapolis,2013-12-31T00:00:00,1.85,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ritapolis,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ritapolis,2015-12-31T00:00:00,1.5,1.773834228515625,18.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ritapolis,2016-12-31T00:00:00,1.8,1.7505522966384888,2.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ritapolis,2017-12-31T00:00:00,1.81,1.7195912599563599,4.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ritapolis,2018-12-31T00:00:00,1.8,1.836233377456665,2.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sabinopolis,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sabinopolis,2013-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sabinopolis,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sabinopolis,2015-12-31T00:00:00,1.5,0.8039053678512573,46.41,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sabinopolis,2016-12-31T00:00:00,0.6,1.1306654214859009,88.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sabinopolis,2017-12-31T00:00:00,1.25,1.0237423181533813,18.1,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sabinopolis,2018-12-31T00:00:00,1.2,1.4425338506698608,20.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Barbara,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Barbara,2013-12-31T00:00:00,0.98,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Barbara,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Barbara,2015-12-31T00:00:00,1.2,1.0097601413726807,15.85,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Barbara,2016-12-31T00:00:00,0.91,1.055369257926941,15.97,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Barbara,2017-12-31T00:00:00,1.17,1.0339568853378296,11.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Barbara,2018-12-31T00:00:00,1.21,1.2460176944732666,2.98,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Maria_do_Suacui,2012-12-31T00:00:00,0.91,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Maria_do_Suacui,2013-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Maria_do_Suacui,2014-12-31T00:00:00,0.66,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Maria_do_Suacui,2015-12-31T00:00:00,0.9,0.7386122345924377,17.93,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Maria_do_Suacui,2016-12-31T00:00:00,0.83,0.7989198565483093,3.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Maria_do_Suacui,2017-12-31T00:00:00,0.67,0.8012943863868713,19.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Maria_do_Suacui,2018-12-31T00:00:00,0.8,0.8641281127929688,8.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_de_Caldas,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_de_Caldas,2013-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_de_Caldas,2014-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_de_Caldas,2015-12-31T00:00:00,1.5,1.7076749801635742,13.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_de_Caldas,2016-12-31T00:00:00,1.8,1.647554636001587,8.47,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_de_Caldas,2017-12-31T00:00:00,2.18,1.700409173965454,22.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_de_Caldas,2018-12-31T00:00:00,1.8,1.9635998010635376,9.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_do_Itueto,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_do_Itueto,2013-12-31T00:00:00,1.17,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_do_Itueto,2014-12-31T00:00:00,1.11,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_do_Itueto,2015-12-31T00:00:00,0.92,1.1510814428329468,25.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_do_Itueto,2016-12-31T00:00:00,1.2,1.07827889919281,10.14,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_do_Itueto,2017-12-31T00:00:00,1.32,1.1089345216751099,15.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_do_Itueto,2018-12-31T00:00:00,1.35,1.2740286588668823,5.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santo_Antonio_do_Retiro,2012-12-31T00:00:00,1.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santo_Antonio_do_Retiro,2013-12-31T00:00:00,1.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santo_Antonio_do_Retiro,2014-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santo_Antonio_do_Retiro,2015-12-31T00:00:00,1.2,0.9836621284484863,18.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santo_Antonio_do_Retiro,2016-12-31T00:00:00,0.9,1.1404012441635132,26.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santo_Antonio_do_Retiro,2017-12-31T00:00:00,0.93,1.0668162107467651,14.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santo_Antonio_do_Retiro,2018-12-31T00:00:00,1.08,1.0614091157913208,1.72,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Domingos_do_Prata,2012-12-31T00:00:00,0.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Domingos_do_Prata,2013-12-31T00:00:00,0.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Domingos_do_Prata,2014-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Domingos_do_Prata,2015-12-31T00:00:00,0.9,0.7296257019042969,18.93,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Domingos_do_Prata,2016-12-31T00:00:00,0.84,0.7872977256774902,6.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Domingos_do_Prata,2017-12-31T00:00:00,0.7,0.7986651062965393,14.1,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Domingos_do_Prata,2018-12-31T00:00:00,1.2,0.9032864570617676,24.73,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Francisco_de_Paula,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Francisco_de_Paula,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Francisco_de_Paula,2014-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Francisco_de_Paula,2015-12-31T00:00:00,0.9,1.134568214416504,26.06,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Francisco_de_Paula,2016-12-31T00:00:00,1.56,1.0787689685821533,30.85,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Francisco_de_Paula,2017-12-31T00:00:00,1.56,1.2461382150650024,20.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Francisco_de_Paula,2018-12-31T00:00:00,1.8,1.5722882747650146,12.65,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Geraldo,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Geraldo,2013-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Geraldo,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Geraldo,2015-12-31T00:00:00,0.75,1.0799992084503174,44.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Geraldo,2016-12-31T00:00:00,0.75,1.008142352104187,34.42,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Geraldo,2017-12-31T00:00:00,0.59,0.9092633724212646,54.11,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Geraldo,2018-12-31T00:00:00,1.2,0.7822914719581604,34.81,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Goncalo_do_Rio_Preto,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Goncalo_do_Rio_Preto,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Goncalo_do_Rio_Preto,2014-12-31T00:00:00,1.25,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Goncalo_do_Rio_Preto,2015-12-31T00:00:00,1.3,1.218902826309204,6.24,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Goncalo_do_Rio_Preto,2016-12-31T00:00:00,1.25,1.2630670070648193,1.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Goncalo_do_Rio_Preto,2017-12-31T00:00:00,4.77,1.2701478004455566,73.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Goncalo_do_Rio_Preto,2018-12-31T00:00:00,1.79,3.198430299758911,78.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Manteninha,2012-12-31T00:00:00,0.48,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Manteninha,2013-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Manteninha,2014-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Manteninha,2015-12-31T00:00:00,0.6,0.8344407081604004,39.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Manteninha,2016-12-31T00:00:00,0.6,1.0285818576812744,71.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Manteninha,2017-12-31T00:00:00,0.65,0.9781136512756348,50.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Manteninha,2018-12-31T00:00:00,0.6,0.6391436457633972,6.52,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Paraiso,2012-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Paraiso,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Paraiso,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Paraiso,2015-12-31T00:00:00,1.32,1.2210818529129028,7.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Paraiso,2016-12-31T00:00:00,2.1,1.2893718481063843,38.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Paraiso,2017-12-31T00:00:00,0.59,1.5638017654418945,165.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Paraiso,2018-12-31T00:00:00,2.96,1.5439562797546387,47.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_da_Barra,2012-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_da_Barra,2013-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_da_Barra,2014-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_da_Barra,2015-12-31T00:00:00,1.2,1.4948101043701172,24.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_da_Barra,2016-12-31T00:00:00,1.2,1.4484394788742065,20.7,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_da_Barra,2017-12-31T00:00:00,1.91,1.3055768013000488,31.65,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_da_Barra,2018-12-31T00:00:00,2.1,1.6264654397964478,22.55,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_do_Alegre,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_do_Alegre,2013-12-31T00:00:00,1.09,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_do_Alegre,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_do_Alegre,2015-12-31T00:00:00,1.2,1.170281171798706,2.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_do_Alegre,2016-12-31T00:00:00,1.51,1.1562505960464478,23.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_do_Alegre,2017-12-31T00:00:00,1.1,1.2881782054901123,17.11,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_do_Alegre,2018-12-31T00:00:00,1.5,1.3481988906860352,10.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Miguel_do_Anta,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Miguel_do_Anta,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Miguel_do_Anta,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Miguel_do_Anta,2015-12-31T00:00:00,1.32,1.111235499382019,15.82,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Miguel_do_Anta,2016-12-31T00:00:00,1.2,1.22249174118042,1.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Miguel_do_Anta,2017-12-31T00:00:00,1.02,1.2242469787597656,20.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Miguel_do_Anta,2018-12-31T00:00:00,1.44,1.321465015411377,8.23,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_da_Vargem_Alegre,2012-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_da_Vargem_Alegre,2013-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_da_Vargem_Alegre,2014-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_da_Vargem_Alegre,2015-12-31T00:00:00,1.02,0.21946856379508972,78.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_da_Vargem_Alegre,2016-12-31T00:00:00,1.02,1.626579761505127,59.47,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_da_Vargem_Alegre,2017-12-31T00:00:00,1.27,1.5980286598205566,25.83,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_da_Vargem_Alegre,2018-12-31T00:00:00,1.02,1.2144261598587036,19.06,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Anta,2012-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Anta,2013-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Anta,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Anta,2015-12-31T00:00:00,1.2,1.303267002105713,8.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Anta,2016-12-31T00:00:00,1.2,1.2752430438995361,6.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Anta,2017-12-31T00:00:00,1.78,2.1885151863098145,22.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Anta,2018-12-31T00:00:00,1.64,1.60575270652771,2.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Maranhao,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Maranhao,2013-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Maranhao,2014-12-31T00:00:00,0.66,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Maranhao,2015-12-31T00:00:00,0.9,0.7353065013885498,18.3,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Maranhao,2016-12-31T00:00:00,0.9,0.7853050827980042,12.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Maranhao,2017-12-31T00:00:00,0.88,0.8236968517303467,6.4,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Maranhao,2018-12-31T00:00:00,0.9,0.8996536731719971,0.04,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senador_Modestino_Goncalves,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senador_Modestino_Goncalves,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senador_Modestino_Goncalves,2014-12-31T00:00:00,1.55,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senador_Modestino_Goncalves,2015-12-31T00:00:00,1.8,1.2224984169006348,32.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senador_Modestino_Goncalves,2016-12-31T00:00:00,1.8,1.5280323028564453,15.11,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senador_Modestino_Goncalves,2017-12-31T00:00:00,1.14,1.7313295602798462,51.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senador_Modestino_Goncalves,2018-12-31T00:00:00,1.5,1.8088741302490234,20.59,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senhora_dos_Remedios,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senhora_dos_Remedios,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senhora_dos_Remedios,2014-12-31T00:00:00,1.51,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senhora_dos_Remedios,2015-12-31T00:00:00,1.32,1.6761300563812256,26.98,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senhora_dos_Remedios,2016-12-31T00:00:00,1.2,1.571088433265686,30.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senhora_dos_Remedios,2017-12-31T00:00:00,1.07,1.368090271949768,27.86,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senhora_dos_Remedios,2018-12-31T00:00:00,1.5,1.2684111595153809,15.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Simonesia,2012-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Simonesia,2013-12-31T00:00:00,1.62,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Simonesia,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Simonesia,2015-12-31T00:00:00,1.44,1.456970453262329,1.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Simonesia,2016-12-31T00:00:00,1.32,1.461923599243164,10.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Simonesia,2017-12-31T00:00:00,1.14,1.3387234210968018,17.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Simonesia,2018-12-31T00:00:00,1.2,1.4267113208770752,18.89,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Taiobeiras,2012-12-31T00:00:00,3.3,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Taiobeiras,2013-12-31T00:00:00,3.3,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Taiobeiras,2014-12-31T00:00:00,3.3,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Taiobeiras,2015-12-31T00:00:00,3.3,3.3005261421203613,0.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Taiobeiras,2016-12-31T00:00:00,1.08,3.3005800247192383,205.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Taiobeiras,2017-12-31T00:00:00,1.45,2.4819798469543457,71.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Taiobeiras,2018-12-31T00:00:00,2.22,2.2948830127716064,3.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tapira,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tapira,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tapira,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tapira,2015-12-31T00:00:00,1.5,1.798313021659851,19.89,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tapira,2016-12-31T00:00:00,1.5,1.6914827823638916,12.77,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tapira,2017-12-31T00:00:00,2.7,1.6236519813537598,39.86,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tapira,2018-12-31T00:00:00,1.8,2.2230005264282227,23.5,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Teixeiras,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Teixeiras,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Teixeiras,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Teixeiras,2015-12-31T00:00:00,1.26,1.1098368167877197,11.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Teixeiras,2016-12-31T00:00:00,1.2,1.197959542274475,0.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Teixeiras,2017-12-31T00:00:00,0.86,1.1997578144073486,39.51,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Teixeiras,2018-12-31T00:00:00,1.32,1.3240145444869995,0.3,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tocos_do_Moji,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tocos_do_Moji,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tocos_do_Moji,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tocos_do_Moji,2015-12-31T00:00:00,1.2,1.2875221967697144,7.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tocos_do_Moji,2016-12-31T00:00:00,1.08,1.304814338684082,20.82,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tocos_do_Moji,2017-12-31T00:00:00,1.74,1.1330676078796387,34.88,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tocos_do_Moji,2018-12-31T00:00:00,1.8,1.490919589996338,17.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tupaciguara,2012-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tupaciguara,2013-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tupaciguara,2014-12-31T00:00:00,2.28,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tupaciguara,2015-12-31T00:00:00,2.1,2.126966953277588,1.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tupaciguara,2016-12-31T00:00:00,2.1,2.15691876411438,2.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tupaciguara,2017-12-31T00:00:00,2.1,2.1161699295043945,0.77,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tupaciguara,2018-12-31T00:00:00,2.4,2.100491523742676,12.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Uberaba,2012-12-31T00:00:00,2.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Uberaba,2013-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Uberaba,2014-12-31T00:00:00,2.12,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Uberaba,2015-12-31T00:00:00,1.86,2.1458234786987305,15.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Uberaba,2016-12-31T00:00:00,1.08,2.0424857139587402,89.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Uberaba,2017-12-31T00:00:00,1.99,1.7184185981750488,13.65,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Uberaba,2018-12-31T00:00:00,2.48,1.876227617263794,24.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Vargem_Grande_do_Rio_Pardo,2012-12-31T00:00:00,0.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Vargem_Grande_do_Rio_Pardo,2013-12-31T00:00:00,0.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Vargem_Grande_do_Rio_Pardo,2014-12-31T00:00:00,0.7,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Vargem_Grande_do_Rio_Pardo,2015-12-31T00:00:00,1.22,0.716201901435852,41.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Vargem_Grande_do_Rio_Pardo,2016-12-31T00:00:00,1.2,1.0488522052764893,12.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Vargem_Grande_do_Rio_Pardo,2017-12-31T00:00:00,1.57,1.0840083360671997,30.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Vargem_Grande_do_Rio_Pardo,2018-12-31T00:00:00,1.8,1.4012510776519775,22.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Visconde_do_Rio_Branco,2012-12-31T00:00:00,0.78,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Visconde_do_Rio_Branco,2013-12-31T00:00:00,0.78,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Visconde_do_Rio_Branco,2014-12-31T00:00:00,1.54,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Visconde_do_Rio_Branco,2015-12-31T00:00:00,0.83,0.9650997519493103,16.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Visconde_do_Rio_Branco,2016-12-31T00:00:00,0.83,1.14451265335083,37.89,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Visconde_do_Rio_Branco,2017-12-31T00:00:00,1.17,1.1235331296920776,3.97,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Visconde_do_Rio_Branco,2018-12-31T00:00:00,0.83,1.0804506540298462,30.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Abre_Campo,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aimores,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Alvarenga,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Amparo_do_Serra,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Angelandia,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Dias,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Prado_de_Minas,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aricanduva,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bandeira,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berilo,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berizal,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bocaiuva,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bom_Jesus_do_Amparo,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capela_Nova,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capelinha,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caranaiba,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carmo_da_Mata,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carrancas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Casa_Grande,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cataguases,2019-12-31T00:00:00,0.0,1,inf,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catas_Altas_da_Noruega,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catuji,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caxambu,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Claudio,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coimbra,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Conselheiro_Pena,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coroaci,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Corrego_Novo,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cruzilia,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cuparaque,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Diamantina,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Divisopolis,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Dores_do_Turvo,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Esmeraldas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Espirito_Santo_do_Dourado,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Eugenopolis,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Felicio_dos_Santos,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fervedouro,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formiga,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formoso,2019-12-31T00:00:00,3.0,3,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fortaleza_de_Minas,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guaraciaba,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guiricema,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Iapu,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Imbe_de_Minas,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Inhapim,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Irai_de_Minas,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itaipe,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarandiba,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarati_de_Minas,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequeri,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequitinhonha,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jose_Goncalves_de_Minas,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Lamim,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mantena,2019-12-31T00:00:00,0.0,1,inf,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mar_de_Espanha,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mata_Verde,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Matutina,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Minas_Novas,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Monte_Santo_de_Minas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Nova_Ponte,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novo_Cruzeiro,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novorizonte,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ouro_Verde_de_Minas,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Passos,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Paula_Candido,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pecanha,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Azul,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Bonita,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Dourada,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_do_Anta,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Piedade_de_Caratinga,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pocrane,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ponto_dos_Volantes,2019-12-31T00:00:00,1.0,0,-100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pote,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pratinha,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Quartel_Geral,2019-12-31T00:00:00,3.0,3,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ritapolis,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sabinopolis,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Barbara,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Maria_do_Suacui,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_de_Caldas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_do_Itueto,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santo_Antonio_do_Retiro,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Domingos_do_Prata,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Francisco_de_Paula,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Geraldo,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Goncalo_do_Rio_Preto,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Manteninha,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Paraiso,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_da_Barra,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_do_Alegre,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Miguel_do_Anta,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_da_Vargem_Alegre,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Anta,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Maranhao,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senador_Modestino_Goncalves,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senhora_dos_Remedios,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Simonesia,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Taiobeiras,2019-12-31T00:00:00,3.0,2,-33.33,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tapira,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Teixeiras,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tocos_do_Moji,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tupaciguara,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Uberaba,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Vargem_Grande_do_Rio_Pardo,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Visconde_do_Rio_Branco,2019-12-31T00:00:00,0.0,1,inf,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Abre_Campo,2020-12-31T00:00:00,1.5,1.3641232252120972,-9.06,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aimores,2020-12-31T00:00:00,2.063157894736842,1.392021656036377,-32.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Alvarenga,2020-12-31T00:00:00,0.9,0.6930219531059265,-23.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Amparo_do_Serra,2020-12-31T00:00:00,1.4392156862745098,1.4537850618362427,1.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Angelandia,2020-12-31T00:00:00,1.8400349650349648,1.6048489809036255,-12.78,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Dias,2020-12-31T00:00:00,1.333333333333333,0.9930306673049927,-25.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Prado_de_Minas,2020-12-31T00:00:00,1.5,1.1066738367080688,-26.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aricanduva,2020-12-31T00:00:00,1.3196428571428571,0.9573830366134644,-27.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bandeira,2020-12-31T00:00:00,1.083333333333333,2.0848734378814697,92.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berilo,2020-12-31T00:00:00,0.8428571428571429,0.866960346698761,2.86,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berizal,2020-12-31T00:00:00,3.340236686390532,1.0777138471603394,-67.74,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bocaiuva,2020-12-31T00:00:00,1.622950819672131,1.7187621593475342,5.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bom_Jesus_do_Amparo,2020-12-31T00:00:00,1.5,1.8673161268234253,24.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capela_Nova,2020-12-31T00:00:00,1.8000000000000005,2.044074296951294,13.56,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capelinha,2020-12-31T00:00:00,1.434123222748815,1.2139294147491455,-15.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caranaiba,2020-12-31T00:00:00,1.645161290322581,1.055107831954956,-35.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carmo_da_Mata,2020-12-31T00:00:00,1.68,1.4204683303833008,-15.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carrancas,2020-12-31T00:00:00,1.785714285714286,2.0457403659820557,14.56,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Casa_Grande,2020-12-31T00:00:00,1.553846153846154,1.2702456712722778,-18.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cataguases,2020-12-31T00:00:00,1.25,1.3968830108642578,11.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catas_Altas_da_Noruega,2020-12-31T00:00:00,1.0,0.7996975183486938,-20.03,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catuji,2020-12-31T00:00:00,1.197183098591549,1.0866928100585938,-9.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caxambu,2020-12-31T00:00:00,1.3265306122448983,0.8832285404205322,-33.42,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Claudio,2020-12-31T00:00:00,2.76,1.4846621751785278,-46.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coimbra,2020-12-31T00:00:00,1.8000000000000005,1.7719168663024902,-1.56,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Conselheiro_Pena,2020-12-31T00:00:00,1.55981308411215,1.5015082359313965,-3.74,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coroaci,2020-12-31T00:00:00,1.8000000000000005,1.667032241821289,-7.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Corrego_Novo,2020-12-31T00:00:00,1.2545454545454553,0.9912320971488953,-20.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cruzilia,2020-12-31T00:00:00,1.2,1.1953740119934082,-0.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cuparaque,2020-12-31T00:00:00,1.018518518518519,1.3201968669891357,29.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Diamantina,2020-12-31T00:00:00,2.3453815261044184,1.7256531715393066,-26.42,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Divisopolis,2020-12-31T00:00:00,1.34029484029484,1.151207685470581,-14.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Dores_do_Turvo,2020-12-31T00:00:00,1.625,1.268039345741272,-21.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Esmeraldas,2020-12-31T00:00:00,2.096153846153846,2.2258729934692383,6.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Espirito_Santo_do_Dourado,2020-12-31T00:00:00,1.619148936170213,1.609588384628296,-0.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Eugenopolis,2020-12-31T00:00:00,1.8000000000000005,1.0201236009597778,-43.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Felicio_dos_Santos,2020-12-31T00:00:00,3.548821548821549,1.633517861366272,-53.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fervedouro,2020-12-31T00:00:00,1.5,1.241700291633606,-17.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formiga,2020-12-31T00:00:00,1.441791044776119,2.211427688598633,53.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formoso,2020-12-31T00:00:00,3.0,2.8806779384613037,-3.98,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fortaleza_de_Minas,2020-12-31T00:00:00,1.68,1.587404727935791,-5.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guaraciaba,2020-12-31T00:00:00,1.2,1.2029122114181519,0.24,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guiricema,2020-12-31T00:00:00,2.4,1.8612830638885498,-22.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Iapu,2020-12-31T00:00:00,1.0,1.5375094413757324,53.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Imbe_de_Minas,2020-12-31T00:00:00,1.439877300613497,1.3867754936218262,-3.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Inhapim,2020-12-31T00:00:00,1.8000000000000005,1.4281103610992432,-20.66,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Irai_de_Minas,2020-12-31T00:00:00,3.3595092024539883,1.773064136505127,-47.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itaipe,2020-12-31T00:00:00,0.9594594594594597,0.9751939177513123,1.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarandiba,2020-12-31T00:00:00,1.7196261682242993,1.7005646228790283,-1.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarati_de_Minas,2020-12-31T00:00:00,1.2,0.9096072912216187,-24.2,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequeri,2020-12-31T00:00:00,2.1,1.458767294883728,-30.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequitinhonha,2020-12-31T00:00:00,2.4,2.063941717147827,-14.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jose_Goncalves_de_Minas,2020-12-31T00:00:00,1.5,1.0446714162826538,-30.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Lamim,2020-12-31T00:00:00,2.387096774193548,1.375391960144043,-42.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mantena,2020-12-31T00:00:00,0.7,0.7827680110931396,11.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mar_de_Espanha,2020-12-31T00:00:00,1.8000000000000005,2.083883285522461,15.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mata_Verde,2020-12-31T00:00:00,1.319688109161793,1.187232494354248,-10.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Matutina,2020-12-31T00:00:00,1.82995951417004,1.9934390783309937,8.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Minas_Novas,2020-12-31T00:00:00,1.0,0.8607650399208069,-13.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Monte_Santo_de_Minas,2020-12-31T00:00:00,2.123987903619159,1.7568320035934448,-17.29,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Nova_Ponte,2020-12-31T00:00:00,1.8591549295774648,1.777411699295044,-4.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novo_Cruzeiro,2020-12-31T00:00:00,1.19727047146402,1.1172525882720947,-6.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novorizonte,2020-12-31T00:00:00,1.235294117647059,1.2329109907150269,-0.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ouro_Verde_de_Minas,2020-12-31T00:00:00,1.2,1.4176452159881592,18.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Passos,2020-12-31T00:00:00,1.62,1.9841150045394897,22.48,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Paula_Candido,2020-12-31T00:00:00,1.62,1.4477643966674805,-10.63,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pecanha,2020-12-31T00:00:00,2.0,1.43523371219635,-28.24,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Azul,2020-12-31T00:00:00,1.0,1.344963550567627,34.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Bonita,2020-12-31T00:00:00,1.7400000000000002,1.2022260427474976,-30.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Dourada,2020-12-31T00:00:00,1.44,1.2830952405929565,-10.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_do_Anta,2020-12-31T00:00:00,1.5,1.2931041717529297,-13.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Piedade_de_Caratinga,2020-12-31T00:00:00,1.8000000000000005,1.6569565534591675,-7.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pocrane,2020-12-31T00:00:00,1.258064516129032,1.3137617111206055,4.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ponto_dos_Volantes,2020-12-31T00:00:00,0.631578947368421,0.5931974649429321,-6.08,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pote,2020-12-31T00:00:00,0.6571428571428573,0.7335613369941711,11.63,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pratinha,2020-12-31T00:00:00,2.1,1.7868402004241943,-14.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Quartel_Geral,2020-12-31T00:00:00,3.6000000000000005,4.3403849601745605,20.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ritapolis,2020-12-31T00:00:00,2.101769911504425,1.8071904182434082,-14.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sabinopolis,2020-12-31T00:00:00,1.0,1.2381603717803955,23.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Barbara,2020-12-31T00:00:00,1.142857142857143,1.1627452373504639,1.74,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Maria_do_Suacui,2020-12-31T00:00:00,0.8,0.8121915459632874,1.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_de_Caldas,2020-12-31T00:00:00,1.8000000000000005,2.1085309982299805,17.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_do_Itueto,2020-12-31T00:00:00,1.77,1.5812256336212158,-10.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santo_Antonio_do_Retiro,2020-12-31T00:00:00,0.975,1.5467462539672852,58.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Domingos_do_Prata,2020-12-31T00:00:00,0.8846153846153846,1.0666660070419312,20.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Francisco_de_Paula,2020-12-31T00:00:00,1.62,1.7348754405975342,7.09,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Geraldo,2020-12-31T00:00:00,1.8000000000000005,0.766167938709259,-57.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Goncalo_do_Rio_Preto,2020-12-31T00:00:00,3.6000000000000005,2.5436296463012695,-29.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Manteninha,2020-12-31T00:00:00,0.9166666666666664,0.8368211984634399,-8.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Paraiso,2020-12-31T00:00:00,3.146938775510204,1.8974888324737549,-39.7,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_da_Barra,2020-12-31T00:00:00,2.537024221453287,2.0140280723571777,-20.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_do_Alegre,2020-12-31T00:00:00,1.359090909090909,1.4630497694015503,7.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Miguel_do_Anta,2020-12-31T00:00:00,1.5,1.439233422279358,-4.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_da_Vargem_Alegre,2020-12-31T00:00:00,1.8000000000000005,1.1891919374465942,-33.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Anta,2020-12-31T00:00:00,1.8000000000000005,1.7091541290283203,-5.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Maranhao,2020-12-31T00:00:00,0.8148148148148149,0.9152469038963318,12.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senador_Modestino_Goncalves,2020-12-31T00:00:00,3.6000000000000005,1.5837626457214355,-56.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senhora_dos_Remedios,2020-12-31T00:00:00,1.8000000000000005,1.7625174522399902,-2.08,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Simonesia,2020-12-31T00:00:00,1.8000000000000005,1.269371509552002,-29.48,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Taiobeiras,2020-12-31T00:00:00,3.240083507306889,2.2774946689605713,-29.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tapira,2020-12-31T00:00:00,2.1013698630137,2.531254529953003,20.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Teixeiras,2020-12-31T00:00:00,1.5,1.258988380432129,-16.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tocos_do_Moji,2020-12-31T00:00:00,1.441340782122905,1.7375400066375732,20.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tupaciguara,2020-12-31T00:00:00,2.1,2.3810207843780518,13.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Uberaba,2020-12-31T00:00:00,2.5325581395348844,2.4651803970336914,-2.66,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Vargem_Grande_do_Rio_Pardo,2020-12-31T00:00:00,1.8000000000000005,1.9140805006027222,6.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Visconde_do_Rio_Branco,2020-12-31T00:00:00,1.0,0.8952898979187012,-10.47,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Abre_Campo,2021-12-31T00:00:00,1.2,1.375239610671997,14.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aimores,2021-12-31T00:00:00,1.634328358208955,1.5420572757720947,-5.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Alvarenga,2021-12-31T00:00:00,0.6304878048780488,0.7310112118721008,15.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Amparo_do_Serra,2021-12-31T00:00:00,1.2,1.3493726253509521,12.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Angelandia,2021-12-31T00:00:00,1.5133433283358322,1.6634286642074585,9.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Dias,2021-12-31T00:00:00,1.5,1.003403663635254,-33.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Prado_de_Minas,2021-12-31T00:00:00,0.9,1.0459785461425781,16.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aricanduva,2021-12-31T00:00:00,0.8394736842105264,1.1179066896438599,33.17,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bandeira,2021-12-31T00:00:00,0.96,0.9839592576026917,2.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berilo,2021-12-31T00:00:00,1.8017241379310345,0.7820684909820557,-56.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berizal,2021-12-31T00:00:00,3.541935483870968,2.1503701210021973,-39.29,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bocaiuva,2021-12-31T00:00:00,1.7407407407407405,1.7046589851379395,-2.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bom_Jesus_do_Amparo,2021-12-31T00:00:00,1.2,1.8044904470443726,50.37,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capela_Nova,2021-12-31T00:00:00,1.8000000000000005,1.8353662490844727,1.96,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capelinha,2021-12-31T00:00:00,1.4350318471337582,1.2814091444015503,-10.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caranaiba,2021-12-31T00:00:00,2.28,1.0587222576141357,-53.56,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carmo_da_Mata,2021-12-31T00:00:00,1.32,1.3376624584197998,1.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carrancas,2021-12-31T00:00:00,1.214285714285714,1.642512559890747,35.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Casa_Grande,2021-12-31T00:00:00,1.3230769230769233,1.3167256116867065,-0.48,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cataguases,2021-12-31T00:00:00,1.25,0.5933926701545715,-52.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catas_Altas_da_Noruega,2021-12-31T00:00:00,1.0,0.7538806200027466,-24.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catuji,2021-12-31T00:00:00,1.384615384615385,1.1345787048339844,-18.06,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caxambu,2021-12-31T00:00:00,1.4387755102040822,1.0299404859542847,-28.42,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Claudio,2021-12-31T00:00:00,1.800653594771242,1.7519121170043945,-2.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coimbra,2021-12-31T00:00:00,1.501182033096927,1.6372967958450317,9.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Conselheiro_Pena,2021-12-31T00:00:00,1.2898734177215188,1.4732369184494019,14.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coroaci,2021-12-31T00:00:00,1.5,1.5793800354003906,5.29,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Corrego_Novo,2021-12-31T00:00:00,1.8000000000000005,0.9872198104858398,-45.15,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cruzilia,2021-12-31T00:00:00,1.5027322404371577,1.157005786895752,-23.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cuparaque,2021-12-31T00:00:00,1.32,1.1909756660461426,-9.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Diamantina,2021-12-31T00:00:00,2.9196787148594376,1.949378490447998,-33.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Divisopolis,2021-12-31T00:00:00,0.9595505617977528,1.2217880487442017,27.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Dores_do_Turvo,2021-12-31T00:00:00,1.5,1.109786033630371,-26.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Esmeraldas,2021-12-31T00:00:00,1.7115384615384608,2.114546775817871,23.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Espirito_Santo_do_Dourado,2021-12-31T00:00:00,1.200854700854701,1.5121753215789795,25.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Eugenopolis,2021-12-31T00:00:00,1.5,1.0737777948379517,-28.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Felicio_dos_Santos,2021-12-31T00:00:00,3.542087542087541,2.083026885986328,-41.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fervedouro,2021-12-31T00:00:00,1.15006090133983,1.3082674741744995,13.76,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formiga,2021-12-31T00:00:00,1.347576301615799,1.2936068773269653,-4.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formoso,2021-12-31T00:00:00,3.0,2.91174578666687,-2.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fortaleza_de_Minas,2021-12-31T00:00:00,1.26,1.6330006122589111,29.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guaraciaba,2021-12-31T00:00:00,2.4,1.1989412307739258,-50.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guiricema,2021-12-31T00:00:00,2.1,1.6696332693099976,-20.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Iapu,2021-12-31T00:00:00,1.5578947368421048,1.2619776725769043,-18.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Imbe_de_Minas,2021-12-31T00:00:00,1.2,1.259141206741333,4.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Inhapim,2021-12-31T00:00:00,1.2,1.1105290651321411,-7.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Irai_de_Minas,2021-12-31T00:00:00,2.472300469483568,2.183825731277466,-11.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itaipe,2021-12-31T00:00:00,0.7792792792792793,0.9602375626564026,23.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarandiba,2021-12-31T00:00:00,1.632,1.6475639343261719,0.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarati_de_Minas,2021-12-31T00:00:00,1.2,0.9563291072845459,-20.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequeri,2021-12-31T00:00:00,1.5,1.4825611114501953,-1.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequitinhonha,2021-12-31T00:00:00,1.8000000000000005,2.320899724960327,28.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jose_Goncalves_de_Minas,2021-12-31T00:00:00,1.502673796791444,1.284881830215454,-14.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Lamim,2021-12-31T00:00:00,1.2800000000000002,1.4642813205718994,14.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mantena,2021-12-31T00:00:00,0.7289999999999999,0.6385289430618286,-12.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mar_de_Espanha,2021-12-31T00:00:00,1.4199999999999997,2.181185722351074,53.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mata_Verde,2021-12-31T00:00:00,1.319277108433735,1.349492073059082,2.29,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Matutina,2021-12-31T00:00:00,1.502024291497976,1.8626794815063477,24.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Minas_Novas,2021-12-31T00:00:00,1.7848101265822778,0.9444769024848938,-47.08,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Monte_Santo_de_Minas,2021-12-31T00:00:00,1.200018932222643,1.755131483078003,46.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Nova_Ponte,2021-12-31T00:00:00,2.5482352941176467,1.7576830387115479,-31.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novo_Cruzeiro,2021-12-31T00:00:00,1.226392251815981,1.1372015476226807,-7.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novorizonte,2021-12-31T00:00:00,1.307692307692308,1.448087215423584,10.74,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ouro_Verde_de_Minas,2021-12-31T00:00:00,1.942857142857143,1.2965295314788818,-33.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Passos,2021-12-31T00:00:00,0.99,1.605142593383789,62.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Paula_Candido,2021-12-31T00:00:00,1.5,1.4681217670440674,-2.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pecanha,2021-12-31T00:00:00,1.807692307692308,1.6016392707824707,-11.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Azul,2021-12-31T00:00:00,1.2,1.1644179821014404,-2.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Bonita,2021-12-31T00:00:00,1.08,1.2732187509536743,17.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Dourada,2021-12-31T00:00:00,0.96,1.1557449102401733,20.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_do_Anta,2021-12-31T00:00:00,0.9,1.278994083404541,42.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Piedade_de_Caratinga,2021-12-31T00:00:00,1.2,1.5832579135894775,31.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pocrane,2021-12-31T00:00:00,1.3228346456692908,1.1740031242370605,-11.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ponto_dos_Volantes,2021-12-31T00:00:00,0.5375,0.6160327792167664,14.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pote,2021-12-31T00:00:00,0.6000000000000001,0.7099188566207886,18.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pratinha,2021-12-31T00:00:00,1.35,1.8198590278625488,34.8,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Quartel_Geral,2021-12-31T00:00:00,2.4571428571428573,3.325455665588379,35.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ritapolis,2021-12-31T00:00:00,1.5,1.8268862962722778,21.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sabinopolis,2021-12-31T00:00:00,1.2,1.0922086238861084,-8.98,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Barbara,2021-12-31T00:00:00,0.9333333333333332,1.143951177597046,22.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Maria_do_Suacui,2021-12-31T00:00:00,0.8,1.1492326259613037,43.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_de_Caldas,2021-12-31T00:00:00,1.682051282051282,1.841661810874939,9.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_do_Itueto,2021-12-31T00:00:00,1.3799999999999997,1.5085277557373047,9.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santo_Antonio_do_Retiro,2021-12-31T00:00:00,1.2,1.5561881065368652,29.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Domingos_do_Prata,2021-12-31T00:00:00,0.903846153846154,0.8890887498855591,-1.63,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Francisco_de_Paula,2021-12-31T00:00:00,1.3200867052023115,1.6719613075256348,26.66,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Geraldo,2021-12-31T00:00:00,1.5,1.603041648864746,6.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Goncalo_do_Rio_Preto,2021-12-31T00:00:00,3.36,2.151122570037842,-35.98,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Manteninha,2021-12-31T00:00:00,0.7,0.8177600502967834,16.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Paraiso,2021-12-31T00:00:00,3.0614754098360666,2.854118585586548,-6.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_da_Barra,2021-12-31T00:00:00,1.682068965517241,1.8549355268478394,10.28,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_do_Alegre,2021-12-31T00:00:00,1.7823529411764714,1.4434221982955933,-19.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Miguel_do_Anta,2021-12-31T00:00:00,1.2603053435114495,1.3902766704559326,10.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_da_Vargem_Alegre,2021-12-31T00:00:00,1.020238095238095,1.0578327178955078,3.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Anta,2021-12-31T00:00:00,1.5,1.6232388019561768,8.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Maranhao,2021-12-31T00:00:00,0.9655172413793104,0.8913266062736511,-7.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senador_Modestino_Goncalves,2021-12-31T00:00:00,2.7000000000000006,2.4085516929626465,-10.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senhora_dos_Remedios,2021-12-31T00:00:00,1.5,1.6515605449676514,10.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Simonesia,2021-12-31T00:00:00,1.08,1.4083356857299805,30.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Taiobeiras,2021-12-31T00:00:00,3.49869451697128,2.902057409286499,-17.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tapira,2021-12-31T00:00:00,0.6000000000000001,1.8815374374389648,213.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Teixeiras,2021-12-31T00:00:00,1.32,1.1734174489974976,-11.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tocos_do_Moji,2021-12-31T00:00:00,1.259776536312849,1.5166343450546265,20.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tupaciguara,2021-12-31T00:00:00,1.887700534759358,2.277139186859131,20.63,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Uberaba,2021-12-31T00:00:00,2.0302325581395344,2.4906342029571533,22.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Vargem_Grande_do_Rio_Pardo,2021-12-31T00:00:00,1.644444444444444,1.9631328582763672,19.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Visconde_do_Rio_Branco,2021-12-31T00:00:00,1.25,0.6376771330833435,-48.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Abre_Campo,2022-12-31T00:00:00,1.2,1.4471609592437744,20.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aimores,2022-12-31T00:00:00,2.332558139534884,1.7746572494506836,-23.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Alvarenga,2022-12-31T00:00:00,0.9,0.7952006459236145,-11.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Amparo_do_Serra,2022-12-31T00:00:00,1.8000000000000005,1.3158856630325317,-26.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Angelandia,2022-12-31T00:00:00,1.9799126637554585,1.6305150985717773,-17.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Dias,2022-12-31T00:00:00,1.5,1.3516342639923096,-9.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Prado_de_Minas,2022-12-31T00:00:00,1.2,1.0459489822387695,-12.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aricanduva,2022-12-31T00:00:00,1.2,1.116410732269287,-6.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bandeira,2022-12-31T00:00:00,1.5,0.9974433183670044,-33.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berilo,2022-12-31T00:00:00,1.5625,1.1527235507965088,-26.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berizal,2022-12-31T00:00:00,3.570967741935484,3.3592734336853027,-5.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bocaiuva,2022-12-31T00:00:00,3.6000000000000005,1.7029876708984375,-52.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bom_Jesus_do_Amparo,2022-12-31T00:00:00,1.8000000000000005,1.623826503753662,-9.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capela_Nova,2022-12-31T00:00:00,2.1016393442622947,4.494520664215088,113.86,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capelinha,2022-12-31T00:00:00,1.576751592356688,1.3977429866790771,-11.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caranaiba,2022-12-31T00:00:00,3.405405405405405,2.0758018493652344,-39.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carmo_da_Mata,2022-12-31T00:00:00,1.56,1.4907439947128296,-4.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carrancas,2022-12-31T00:00:00,1.6785714285714293,1.8396844863891602,9.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Casa_Grande,2022-12-31T00:00:00,2.107692307692308,1.5387048721313477,-27.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cataguases,2022-12-31T00:00:00,1.5,0.860957682132721,-42.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catas_Altas_da_Noruega,2022-12-31T00:00:00,1.0,0.9881815910339355,-1.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catuji,2022-12-31T00:00:00,1.3230769230769233,1.3096604347229004,-1.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caxambu,2022-12-31T00:00:00,1.4387755102040822,1.3131225109100342,-8.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Claudio,2022-12-31T00:00:00,1.140522875816993,2.325329303741455,103.88,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coimbra,2022-12-31T00:00:00,1.799054373522459,1.5565557479858398,-13.48,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Conselheiro_Pena,2022-12-31T00:00:00,1.8000000000000005,1.4230855703353882,-20.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coroaci,2022-12-31T00:00:00,2.1,1.5677669048309326,-25.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Corrego_Novo,2022-12-31T00:00:00,1.377777777777778,1.5406488180160522,11.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cruzilia,2022-12-31T00:00:00,1.5027322404371577,1.3801624774932861,-8.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cuparaque,2022-12-31T00:00:00,1.02,1.4435844421386719,41.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Diamantina,2022-12-31T00:00:00,2.965183752417796,2.62003231048584,-11.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Divisopolis,2022-12-31T00:00:00,1.259523809523809,1.173493504524231,-6.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Dores_do_Turvo,2022-12-31T00:00:00,1.2,1.5584077835083008,29.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Esmeraldas,2022-12-31T00:00:00,2.111111111111112,2.1016461849212646,-0.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Espirito_Santo_do_Dourado,2022-12-31T00:00:00,1.319148936170213,1.521438717842102,15.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Eugenopolis,2022-12-31T00:00:00,1.8000000000000005,1.217645287513733,-32.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Felicio_dos_Santos,2022-12-31T00:00:00,3.306397306397306,3.288614511489868,-0.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fervedouro,2022-12-31T00:00:00,1.4399108138238568,1.339248776435852,-6.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formiga,2022-12-31T00:00:00,1.8000000000000005,1.355027437210083,-24.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formoso,2022-12-31T00:00:00,3.3607142857142858,8.647439956665039,157.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fortaleza_de_Minas,2022-12-31T00:00:00,1.5,1.5165618658065796,1.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guaraciaba,2022-12-31T00:00:00,1.8000000000000005,1.9471197128295898,8.17,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guiricema,2022-12-31T00:00:00,2.4,1.823016881942749,-24.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Iapu,2022-12-31T00:00:00,1.5614035087719298,1.4834730625152588,-4.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Imbe_de_Minas,2022-12-31T00:00:00,1.3799999999999997,1.3024033308029175,-5.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Inhapim,2022-12-31T00:00:00,1.8000000000000005,1.574531078338623,-12.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Irai_de_Minas,2022-12-31T00:00:00,2.386666666666667,2.841060161590576,19.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itaipe,2022-12-31T00:00:00,0.9604651162790696,0.9415664076805115,-1.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarandiba,2022-12-31T00:00:00,2.136,1.626028299331665,-23.88,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarati_de_Minas,2022-12-31T00:00:00,1.261538461538461,1.1267693042755127,-10.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequeri,2022-12-31T00:00:00,1.8000000000000005,1.683213472366333,-6.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequitinhonha,2022-12-31T00:00:00,2.4,2.1575002670288086,-10.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jose_Goncalves_de_Minas,2022-12-31T00:00:00,1.5,1.2430088520050049,-17.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Lamim,2022-12-31T00:00:00,2.4,2.054987907409668,-14.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mantena,2022-12-31T00:00:00,0.75,0.8938450217247009,19.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mar_de_Espanha,2022-12-31T00:00:00,2.1,1.7415401935577393,-17.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mata_Verde,2022-12-31T00:00:00,1.6792452830188678,1.233798623085022,-26.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Matutina,2022-12-31T00:00:00,1.8097165991902835,1.7733451128005981,-2.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Minas_Novas,2022-12-31T00:00:00,1.7848101265822778,1.3743735551834106,-23.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Monte_Santo_de_Minas,2022-12-31T00:00:00,1.4699677072120565,1.7449607849121094,18.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Nova_Ponte,2022-12-31T00:00:00,2.710714285714286,2.2324140071868896,-17.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novo_Cruzeiro,2022-12-31T00:00:00,1.180722891566265,1.1963306665420532,1.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novorizonte,2022-12-31T00:00:00,1.461538461538461,1.3932409286499023,-4.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ouro_Verde_de_Minas,2022-12-31T00:00:00,2.0,1.6535848379135132,-17.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Passos,2022-12-31T00:00:00,1.290038314176245,1.5670263767242432,21.47,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Paula_Candido,2022-12-31T00:00:00,1.6198529411764713,1.5215972661972046,-6.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pecanha,2022-12-31T00:00:00,2.287671232876712,1.7918622493743896,-21.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Azul,2022-12-31T00:00:00,1.5333333333333332,1.155035138130188,-24.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Bonita,2022-12-31T00:00:00,1.8000000000000005,1.425121784210205,-20.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Dourada,2022-12-31T00:00:00,1.2,1.1574406623840332,-3.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_do_Anta,2022-12-31T00:00:00,1.438297872340426,1.2705711126327515,-11.66,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Piedade_de_Caratinga,2022-12-31T00:00:00,1.56,1.6287586688995361,4.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pocrane,2022-12-31T00:00:00,1.440944881889764,1.2608262300491333,-12.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ponto_dos_Volantes,2022-12-31T00:00:00,0.6000000000000001,0.6092945337295532,1.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pote,2022-12-31T00:00:00,1.2,0.6800980567932129,-43.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pratinha,2022-12-31T00:00:00,1.720111731843575,1.83219313621521,6.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Quartel_Geral,2022-12-31T00:00:00,3.0,3.1303815841674805,4.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ritapolis,2022-12-31T00:00:00,1.8000000000000005,1.9397732019424438,7.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sabinopolis,2022-12-31T00:00:00,1.32,1.1700375080108643,-11.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Barbara,2022-12-31T00:00:00,1.083333333333333,1.0941901206970215,1.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Maria_do_Suacui,2022-12-31T00:00:00,1.0,1.6372253894805908,63.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_de_Caldas,2022-12-31T00:00:00,1.8000000000000005,1.8467248678207397,2.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_do_Itueto,2022-12-31T00:00:00,1.6399999999999997,1.6548116207122803,0.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santo_Antonio_do_Retiro,2022-12-31T00:00:00,1.2,1.5692716836929321,30.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Domingos_do_Prata,2022-12-31T00:00:00,1.078125,0.8612551093101501,-20.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Francisco_de_Paula,2022-12-31T00:00:00,1.62,1.6119697093963623,-0.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Geraldo,2022-12-31T00:00:00,1.561038961038961,1.7039988040924072,9.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Goncalo_do_Rio_Preto,2022-12-31T00:00:00,3.0,3.3717451095581055,12.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Manteninha,2022-12-31T00:00:00,1.2,0.875171422958374,-27.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Paraiso,2022-12-31T00:00:00,3.09016393442623,2.9749410152435303,-3.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_da_Barra,2022-12-31T00:00:00,2.3076923076923066,2.1737875938415527,-5.8,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_do_Alegre,2022-12-31T00:00:00,1.682352941176471,1.6294629573822021,-3.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Miguel_do_Anta,2022-12-31T00:00:00,1.5,1.3197245597839355,-12.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_da_Vargem_Alegre,2022-12-31T00:00:00,1.020238095238095,1.113505244255066,9.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Anta,2022-12-31T00:00:00,1.8000000000000005,1.6759518384933472,-6.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Maranhao,2022-12-31T00:00:00,1.141666666666667,0.9247449040412903,-19.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senador_Modestino_Goncalves,2022-12-31T00:00:00,2.7000000000000006,2.8493380546569824,5.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senhora_dos_Remedios,2022-12-31T00:00:00,1.466666666666667,1.7935457229614258,22.29,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Simonesia,2022-12-31T00:00:00,1.8000000000000005,1.5326848030090332,-14.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Taiobeiras,2022-12-31T00:00:00,3.14031971580817,3.332486867904663,6.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tapira,2022-12-31T00:00:00,1.8000000000000005,1.6831789016723633,-6.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Teixeiras,2022-12-31T00:00:00,1.5,1.3995404243469238,-6.7,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tocos_do_Moji,2022-12-31T00:00:00,1.5,1.3631796836853027,-9.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tupaciguara,2022-12-31T00:00:00,2.4,2.186051845550537,-8.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Uberaba,2022-12-31T00:00:00,2.102941176470588,2.4733262062072754,17.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Vargem_Grande_do_Rio_Pardo,2022-12-31T00:00:00,1.8000000000000005,1.9480620622634888,8.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Visconde_do_Rio_Branco,2022-12-31T00:00:00,1.6666666666666672,0.049926143139600754,-97.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Abre_Campo,2023-12-31T00:00:00,1.8000000000000005,1.3226122856140137,-26.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aimores,2023-12-31T00:00:00,1.544827586206897,1.9916068315505981,28.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Alvarenga,2023-12-31T00:00:00,1.75,0.8285993933677673,-52.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Amparo_do_Serra,2023-12-31T00:00:00,1.319444444444444,1.502263069152832,13.86,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Angelandia,2023-12-31T00:00:00,2.095663265306122,1.7563177347183228,-16.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Dias,2023-12-31T00:00:00,1.477272727272727,1.460967779159546,-1.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Prado_de_Minas,2023-12-31T00:00:00,1.4857142857142862,1.2535616159439087,-15.63,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aricanduva,2023-12-31T00:00:00,1.2,1.0973650217056274,-8.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bandeira,2023-12-31T00:00:00,0.95,1.2485102415084839,31.42,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berilo,2023-12-31T00:00:00,1.7988826815642458,1.4583762884140015,-18.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berizal,2023-12-31T00:00:00,3.570967741935484,3.511200428009033,-1.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bocaiuva,2023-12-31T00:00:00,3.0,2.4578843116760254,-18.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bom_Jesus_do_Amparo,2023-12-31T00:00:00,1.25,1.5471665859222412,23.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capela_Nova,2023-12-31T00:00:00,1.8000000000000005,1.9548934698104858,8.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capelinha,2023-12-31T00:00:00,1.541750580945004,1.487741231918335,-3.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caranaiba,2023-12-31T00:00:00,1.45,2.6441843509674072,82.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carmo_da_Mata,2023-12-31T00:00:00,1.2,1.5722440481185913,31.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carrancas,2023-12-31T00:00:00,1.214285714285714,1.645097017288208,35.48,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Casa_Grande,2023-12-31T00:00:00,2.523076923076923,1.7794697284698486,-29.47,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cataguases,2023-12-31T00:00:00,1.333333333333333,1.3543795347213745,1.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catas_Altas_da_Noruega,2023-12-31T00:00:00,1.1,1.6184104681015015,47.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catuji,2023-12-31T00:00:00,1.384615384615385,1.3103153705596924,-5.37,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caxambu,2023-12-31T00:00:00,1.316326530612245,1.4140433073043823,7.42,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Claudio,2023-12-31T00:00:00,2.398692810457516,2.0468969345092773,-14.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coimbra,2023-12-31T00:00:00,1.8000000000000005,1.700872778892517,-5.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Conselheiro_Pena,2023-12-31T00:00:00,1.320048602673147,1.5772051811218262,19.48,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coroaci,2023-12-31T00:00:00,2.4,1.8676247596740723,-22.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Corrego_Novo,2023-12-31T00:00:00,1.8000000000000005,1.4878085851669312,-17.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cruzilia,2023-12-31T00:00:00,1.5,1.4399423599243164,-4.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cuparaque,2023-12-31T00:00:00,1.318367346938776,1.128409743309021,-14.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Diamantina,2023-12-31T00:00:00,2.965183752417796,2.80267596244812,-5.48,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Divisopolis,2023-12-31T00:00:00,1.616883116883117,1.223576307296753,-24.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Dores_do_Turvo,2023-12-31T00:00:00,1.5,1.4772229194641113,-1.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Esmeraldas,2023-12-31T00:00:00,2.222222222222222,2.022914409637451,-8.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Espirito_Santo_do_Dourado,2023-12-31T00:00:00,1.5,1.4348478317260742,-4.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Eugenopolis,2023-12-31T00:00:00,1.68,1.7330338954925537,3.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Felicio_dos_Santos,2023-12-31T00:00:00,3.542087542087541,3.488224506378174,-1.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fervedouro,2023-12-31T00:00:00,1.2,1.3870017528533936,15.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formiga,2023-12-31T00:00:00,2.021543985637344,1.6217182874679565,-19.78,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formoso,2023-12-31T00:00:00,3.3803921568627446,3.1419177055358887,-7.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fortaleza_de_Minas,2023-12-31T00:00:00,1.5,1.5345487594604492,2.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guaraciaba,2023-12-31T00:00:00,1.5,1.901004433631897,26.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guiricema,2023-12-31T00:00:00,1.5,2.326754331588745,55.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Iapu,2023-12-31T00:00:00,1.559210526315789,1.4148125648498535,-9.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Imbe_de_Minas,2023-12-31T00:00:00,1.3799999999999997,1.3503774404525757,-2.15,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Inhapim,2023-12-31T00:00:00,2.7000000000000006,1.6093604564666748,-40.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Irai_de_Minas,2023-12-31T00:00:00,2.18825561312608,2.840576648712158,29.81,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itaipe,2023-12-31T00:00:00,0.9598214285714286,0.9054831266403198,-5.66,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarandiba,2023-12-31T00:00:00,2.4,1.8336433172225952,-23.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarati_de_Minas,2023-12-31T00:00:00,1.2,1.2286083698272705,2.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequeri,2023-12-31T00:00:00,1.5,1.8440361022949219,22.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequitinhonha,2023-12-31T00:00:00,2.7000000000000006,2.255963087081909,-16.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jose_Goncalves_de_Minas,2023-12-31T00:00:00,1.3210526315789468,1.5012845993041992,13.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Lamim,2023-12-31T00:00:00,1.8000000000000005,2.1400418281555176,18.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mantena,2023-12-31T00:00:00,1.2,0.7300575971603394,-39.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mar_de_Espanha,2023-12-31T00:00:00,1.5625,1.8333646059036255,17.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mata_Verde,2023-12-31T00:00:00,1.739700374531835,1.4879107475280762,-14.47,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Matutina,2023-12-31T00:00:00,1.502024291497976,1.7532100677490234,16.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Minas_Novas,2023-12-31T00:00:00,1.26,1.5753579139709473,25.03,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Monte_Santo_de_Minas,2023-12-31T00:00:00,1.620050377833753,1.6729544401168823,3.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Nova_Ponte,2023-12-31T00:00:00,2.6142857142857143,2.475980281829834,-5.29,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novo_Cruzeiro,2023-12-31T00:00:00,1.209891435464415,1.2013190984725952,-0.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novorizonte,2023-12-31T00:00:00,1.333333333333333,1.3398178815841675,0.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ouro_Verde_de_Minas,2023-12-31T00:00:00,2.5,1.6897127628326416,-32.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Passos,2023-12-31T00:00:00,1.829801324503311,1.3616273403167725,-25.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Paula_Candido,2023-12-31T00:00:00,1.6197080291970802,1.5794333219528198,-2.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pecanha,2023-12-31T00:00:00,2.30379746835443,2.0732312202453613,-10.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Azul,2023-12-31T00:00:00,1.166666666666667,1.2860993146896362,10.24,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Bonita,2023-12-31T00:00:00,1.08,1.6104960441589355,49.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Dourada,2023-12-31T00:00:00,1.2,1.2296185493469238,2.47,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_do_Anta,2023-12-31T00:00:00,1.5,1.301323413848877,-13.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Piedade_de_Caratinga,2023-12-31T00:00:00,2.4,1.536179780960083,-35.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pocrane,2023-12-31T00:00:00,1.377952755905512,1.35491144657135,-1.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ponto_dos_Volantes,2023-12-31T00:00:00,0.6000000000000001,0.595989465713501,-0.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pote,2023-12-31T00:00:00,1.125,0.8023564219474792,-28.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pratinha,2023-12-31T00:00:00,2.13,1.7987817525863647,-15.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Quartel_Geral,2023-12-31T00:00:00,3.0,3.1384432315826416,4.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ritapolis,2023-12-31T00:00:00,2.102222222222222,1.8922326564788818,-9.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sabinopolis,2023-12-31T00:00:00,1.2,1.1956121921539307,-0.37,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Barbara,2023-12-31T00:00:00,1.3,1.0769439935684204,-17.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Maria_do_Suacui,2023-12-31T00:00:00,0.6666666666666667,0.8770458102226257,31.56,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_de_Caldas,2023-12-31T00:00:00,1.68,1.7752411365509033,5.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_do_Itueto,2023-12-31T00:00:00,1.2,1.6093990802764893,34.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santo_Antonio_do_Retiro,2023-12-31T00:00:00,1.02,1.145154356956482,12.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Domingos_do_Prata,2023-12-31T00:00:00,1.2,0.977315366268158,-18.56,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Francisco_de_Paula,2023-12-31T00:00:00,1.56,1.5653305053710938,0.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Geraldo,2023-12-31T00:00:00,1.5,1.629638671875,8.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Goncalo_do_Rio_Preto,2023-12-31T00:00:00,2.868421052631579,3.36259126663208,17.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Manteninha,2023-12-31T00:00:00,1.2,0.9683884382247925,-19.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Paraiso,2023-12-31T00:00:00,3.601190476190476,3.1108345985412598,-13.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_da_Barra,2023-12-31T00:00:00,2.2413698630136984,2.274078130722046,1.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_do_Alegre,2023-12-31T00:00:00,1.592307692307692,1.6553802490234375,3.96,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Miguel_do_Anta,2023-12-31T00:00:00,1.5598870056497178,1.4221091270446777,-8.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_da_Vargem_Alegre,2023-12-31T00:00:00,1.5,1.32334566116333,-11.78,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Anta,2023-12-31T00:00:00,1.7998212689901698,1.7199816703796387,-4.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Maranhao,2023-12-31T00:00:00,1.2,0.9890173673629761,-17.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senador_Modestino_Goncalves,2023-12-31T00:00:00,2.7000000000000006,3.00354266166687,11.24,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senhora_dos_Remedios,2023-12-31T00:00:00,1.5,1.6340197324752808,8.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Simonesia,2023-12-31T00:00:00,1.5,1.6159404516220093,7.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Taiobeiras,2023-12-31T00:00:00,3.0,3.3274474143981934,10.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tapira,2023-12-31T00:00:00,1.92,1.5499699115753174,-19.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Teixeiras,2023-12-31T00:00:00,1.5007092198581562,1.4411479234695435,-3.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tocos_do_Moji,2023-12-31T00:00:00,1.5,1.4241023063659668,-5.06,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tupaciguara,2023-12-31T00:00:00,2.622222222222222,2.20584774017334,-15.88,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Uberaba,2023-12-31T00:00:00,2.1200980392156863,2.2945773601531982,8.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Vargem_Grande_do_Rio_Pardo,2023-12-31T00:00:00,2.1,1.7670693397521973,-15.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Visconde_do_Rio_Branco,2023-12-31T00:00:00,1.333333333333333,1.3542184829711914,1.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Abre_Campo,2024-12-31T00:00:00,1.32,1.296344518661499,-1.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aimores,2024-12-31T00:00:00,0.9455882352941176,1.7911617755889893,89.42,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Alvarenga,2024-12-31T00:00:00,1.25,0.9560356140136719,-23.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Amparo_do_Serra,2024-12-31T00:00:00,1.680555555555556,1.370874285697937,-18.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Angelandia,2024-12-31T00:00:00,1.870967741935484,1.8225445747375488,-2.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Dias,2024-12-31T00:00:00,1.5,1.487722396850586,-0.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Prado_de_Minas,2024-12-31T00:00:00,1.933333333333333,1.122413992881775,-41.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aricanduva,2024-12-31T00:00:00,1.320245398773006,1.0418161153793335,-21.09,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bandeira,2024-12-31T00:00:00,1.15,1.106366515159607,-3.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berilo,2024-12-31T00:00:00,1.681818181818182,1.7254149913787842,2.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berizal,2024-12-31T00:00:00,3.5516129032258075,3.5560858249664307,0.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bocaiuva,2024-12-31T00:00:00,2.0989010989010994,2.519850254058838,20.06,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bom_Jesus_do_Amparo,2024-12-31T00:00:00,0.75,1.3835467100143433,84.47,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capela_Nova,2024-12-31T00:00:00,2.101234567901236,1.8699486255645752,-11.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capelinha,2024-12-31T00:00:00,1.62007678089009,1.5092096328735352,-6.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caranaiba,2024-12-31T00:00:00,1.5,2.1249771118164062,41.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carmo_da_Mata,2024-12-31T00:00:00,1.62,1.338331699371338,-17.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carrancas,2024-12-31T00:00:00,1.6428571428571432,1.3331619501113892,-18.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Casa_Grande,2024-12-31T00:00:00,2.522388059701492,1.8508598804473877,-26.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cataguases,2024-12-31T00:00:00,1.0,1.3334717750549316,33.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catas_Altas_da_Noruega,2024-12-31T00:00:00,0.9,1.028847336769104,14.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catuji,2024-12-31T00:00:00,1.138461538461538,1.360391616821289,19.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caxambu,2024-12-31T00:00:00,1.795918367346939,1.3839240074157715,-22.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Claudio,2024-12-31T00:00:00,2.699367088607595,1.666630744934082,-38.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coimbra,2024-12-31T00:00:00,1.5012658227848097,1.6634410619735718,10.8,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Conselheiro_Pena,2024-12-31T00:00:00,1.2,1.4450451135635376,20.42,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coroaci,2024-12-31T00:00:00,2.1,1.9587594270706177,-6.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Corrego_Novo,2024-12-31T00:00:00,1.2,1.4788963794708252,23.24,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cruzilia,2024-12-31T00:00:00,1.502793296089385,1.501464605331421,-0.09,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cuparaque,2024-12-31T00:00:00,1.020408163265306,1.2117584943771362,18.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Diamantina,2024-12-31T00:00:00,2.967637540453075,2.947880983352661,-0.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Divisopolis,2024-12-31T00:00:00,1.4902200488997548,1.233949065208435,-17.2,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Dores_do_Turvo,2024-12-31T00:00:00,1.2,1.373117208480835,14.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Esmeraldas,2024-12-31T00:00:00,3.0,2.0023818016052246,-33.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Espirito_Santo_do_Dourado,2024-12-31T00:00:00,1.4391752577319588,1.325169324874878,-7.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Eugenopolis,2024-12-31T00:00:00,1.68,1.623236894607544,-3.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Felicio_dos_Santos,2024-12-31T00:00:00,2.663299663299664,3.4383153915405273,29.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fervedouro,2024-12-31T00:00:00,1.56,1.1933276653289795,-23.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formiga,2024-12-31T00:00:00,1.9475763016157988,1.6225953102111816,-16.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formoso,2024-12-31T00:00:00,3.6000000000000005,3.235450267791748,-10.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fortaleza_de_Minas,2024-12-31T00:00:00,1.5598425196850392,1.4015424251556396,-10.15,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guaraciaba,2024-12-31T00:00:00,1.8000000000000005,1.8215856552124023,1.2,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guiricema,2024-12-31T00:00:00,1.56,1.844990611076355,18.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Iapu,2024-12-31T00:00:00,1.355263157894737,1.559149980545044,15.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Imbe_de_Minas,2024-12-31T00:00:00,1.26,1.2992302179336548,3.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Inhapim,2024-12-31T00:00:00,1.2,1.6442923545837402,37.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Irai_de_Minas,2024-12-31T00:00:00,2.607944732297064,2.2325830459594727,-14.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itaipe,2024-12-31T00:00:00,0.9,0.8920232057571411,-0.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarandiba,2024-12-31T00:00:00,2.1352380952380954,1.983443021774292,-7.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarati_de_Minas,2024-12-31T00:00:00,1.196969696969697,1.2176451683044434,1.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequeri,2024-12-31T00:00:00,1.8000000000000005,1.5513834953308105,-13.81,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequitinhonha,2024-12-31T00:00:00,2.4,2.2624223232269287,-5.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jose_Goncalves_de_Minas,2024-12-31T00:00:00,1.5,1.4335671663284302,-4.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Lamim,2024-12-31T00:00:00,1.777777777777778,1.7487986087799072,-1.63,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mantena,2024-12-31T00:00:00,1.2,0.884682297706604,-26.28,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mar_de_Espanha,2024-12-31T00:00:00,1.2625,1.5040485858917236,19.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mata_Verde,2024-12-31T00:00:00,1.5,1.5267797708511353,1.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Matutina,2024-12-31T00:00:00,1.380566801619433,1.5955595970153809,15.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Minas_Novas,2024-12-31T00:00:00,1.6204545454545451,1.5288951396942139,-5.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Monte_Santo_de_Minas,2024-12-31T00:00:00,1.5,1.348822832107544,-10.08,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Nova_Ponte,2024-12-31T00:00:00,2.306557377049181,2.597323179244995,12.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novo_Cruzeiro,2024-12-31T00:00:00,1.7997542997542997,1.2049314975738525,-33.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novorizonte,2024-12-31T00:00:00,1.333333333333333,1.34926438331604,1.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ouro_Verde_de_Minas,2024-12-31T00:00:00,2.5,2.1057541370391846,-15.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Passos,2024-12-31T00:00:00,1.95459940652819,1.2160202264785767,-37.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Paula_Candido,2024-12-31T00:00:00,1.31970802919708,1.5654617547988892,18.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pecanha,2024-12-31T00:00:00,1.8855421686746991,2.093804359436035,11.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Azul,2024-12-31T00:00:00,0.9,1.3258253335952759,47.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Bonita,2024-12-31T00:00:00,1.2,1.148118257522583,-4.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Dourada,2024-12-31T00:00:00,1.2800000000000002,1.0547349452972412,-17.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_do_Anta,2024-12-31T00:00:00,1.379166666666667,1.2043951749801636,-12.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Piedade_de_Caratinga,2024-12-31T00:00:00,1.68,1.5370451211929321,-8.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pocrane,2024-12-31T00:00:00,1.7968750000000002,1.3667094707489014,-23.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ponto_dos_Volantes,2024-12-31T00:00:00,0.5,0.5757730007171631,15.15,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pote,2024-12-31T00:00:00,1.125,0.9379499554634094,-16.63,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pratinha,2024-12-31T00:00:00,1.849285714285714,1.6615065336227417,-10.15,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Quartel_Geral,2024-12-31T00:00:00,3.0,2.805128335952759,-6.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ritapolis,2024-12-31T00:00:00,1.8000000000000005,1.7816107273101807,-1.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sabinopolis,2024-12-31T00:00:00,1.2,1.2261602878570557,2.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Barbara,2024-12-31T00:00:00,1.333333333333333,1.08295738697052,-18.78,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Maria_do_Suacui,2024-12-31T00:00:00,1.333333333333333,0.8091528415679932,-39.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_de_Caldas,2024-12-31T00:00:00,1.8000000000000005,1.7143748998641968,-4.76,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_do_Itueto,2024-12-31T00:00:00,1.365238095238095,1.3740463256835938,0.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santo_Antonio_do_Retiro,2024-12-31T00:00:00,1.9666666666666672,1.129915475845337,-42.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Domingos_do_Prata,2024-12-31T00:00:00,1.05,1.0215317010879517,-2.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Francisco_de_Paula,2024-12-31T00:00:00,1.62,1.4877909421920776,-8.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Geraldo,2024-12-31T00:00:00,1.3189189189189188,1.5146448612213135,14.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Goncalo_do_Rio_Preto,2024-12-31T00:00:00,2.5238095238095246,3.0238962173461914,19.81,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Manteninha,2024-12-31T00:00:00,0.9,1.0092822313308716,12.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Paraiso,2024-12-31T00:00:00,3.5396226415094336,3.247126817703247,-8.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_da_Barra,2024-12-31T00:00:00,1.709523809523809,2.003732204437256,17.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_do_Alegre,2024-12-31T00:00:00,1.8000000000000005,1.5740351676940918,-12.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Miguel_do_Anta,2024-12-31T00:00:00,1.55974025974026,1.4065704345703125,-9.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_da_Vargem_Alegre,2024-12-31T00:00:00,1.2800000000000002,1.107978105545044,-13.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Anta,2024-12-31T00:00:00,1.8000000000000005,1.6633374691009521,-7.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Maranhao,2024-12-31T00:00:00,1.380952380952381,1.0887233018875122,-21.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senador_Modestino_Goncalves,2024-12-31T00:00:00,2.706666666666667,1.8762155771255493,-30.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senhora_dos_Remedios,2024-12-31T00:00:00,1.8000000000000005,1.487140417098999,-17.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Simonesia,2024-12-31T00:00:00,1.2,1.3334022760391235,11.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Taiobeiras,2024-12-31T00:00:00,3.1206349206349207,3.1466283798217773,0.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tapira,2024-12-31T00:00:00,1.98,1.2352465391159058,-37.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Teixeiras,2024-12-31T00:00:00,1.5,1.4184290170669556,-5.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tocos_do_Moji,2024-12-31T00:00:00,1.5,1.3964710235595703,-6.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tupaciguara,2024-12-31T00:00:00,2.8219895287958106,2.2883338928222656,-18.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Uberaba,2024-12-31T00:00:00,2.102941176470588,2.069149971008301,-1.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Vargem_Grande_do_Rio_Pardo,2024-12-31T00:00:00,1.8000000000000005,1.843916416168213,2.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Visconde_do_Rio_Branco,2024-12-31T00:00:00,1.333333333333333,1.393154501914978,4.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Abre_Campo,2025-12-31T00:00:00,0.0,1.4565235376358032,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aimores,2025-12-31T00:00:00,0.0,1.6357414722442627,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Alvarenga,2025-12-31T00:00:00,0.0,1.3182556629180908,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Amparo_do_Serra,2025-12-31T00:00:00,0.0,1.63332998752594,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Angelandia,2025-12-31T00:00:00,0.0,1.9898709058761597,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Dias,2025-12-31T00:00:00,0.0,1.4939215183258057,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Antonio_Prado_de_Minas,2025-12-31T00:00:00,0.0,1.5533289909362793,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Aricanduva,2025-12-31T00:00:00,0.0,1.2489434480667114,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bandeira,2025-12-31T00:00:00,0.0,1.3321160078048706,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berilo,2025-12-31T00:00:00,0.0,1.7234094142913818,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Berizal,2025-12-31T00:00:00,0.0,3.5655336380004883,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bocaiuva,2025-12-31T00:00:00,0.0,3.0328879356384277,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Bom_Jesus_do_Amparo,2025-12-31T00:00:00,0.0,1.2622010707855225,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capela_Nova,2025-12-31T00:00:00,0.0,2.0049121379852295,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Capelinha,2025-12-31T00:00:00,0.0,1.583687424659729,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caranaiba,2025-12-31T00:00:00,0.0,2.0128729343414307,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carmo_da_Mata,2025-12-31T00:00:00,0.0,1.459528923034668,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Carrancas,2025-12-31T00:00:00,0.0,1.5248866081237793,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Casa_Grande,2025-12-31T00:00:00,0.0,2.3859176635742188,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cataguases,2025-12-31T00:00:00,0.0,1.2873815298080444,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catas_Altas_da_Noruega,2025-12-31T00:00:00,0.0,1.0044876337051392,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Catuji,2025-12-31T00:00:00,0.0,1.3310120105743408,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Caxambu,2025-12-31T00:00:00,0.0,1.5255171060562134,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Claudio,2025-12-31T00:00:00,0.0,2.025282859802246,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coimbra,2025-12-31T00:00:00,0.0,1.7181650400161743,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Conselheiro_Pena,2025-12-31T00:00:00,0.0,1.361027479171753,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Coroaci,2025-12-31T00:00:00,0.0,2.230515480041504,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Corrego_Novo,2025-12-31T00:00:00,0.0,1.478394627571106,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cruzilia,2025-12-31T00:00:00,0.0,1.5019493103027344,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Cuparaque,2025-12-31T00:00:00,0.0,1.1283565759658813,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Diamantina,2025-12-31T00:00:00,0.0,2.9662747383117676,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Divisopolis,2025-12-31T00:00:00,0.0,1.5142254829406738,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Dores_do_Turvo,2025-12-31T00:00:00,0.0,1.311582326889038,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Esmeraldas,2025-12-31T00:00:00,0.0,2.511005401611328,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Espirito_Santo_do_Dourado,2025-12-31T00:00:00,0.0,1.4174036979675293,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Eugenopolis,2025-12-31T00:00:00,0.0,1.726247787475586,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Felicio_dos_Santos,2025-12-31T00:00:00,0.0,3.204488754272461,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fervedouro,2025-12-31T00:00:00,0.0,1.411131501197815,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formiga,2025-12-31T00:00:00,0.0,1.911609172821045,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Formoso,2025-12-31T00:00:00,0.0,3.4833738803863525,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Fortaleza_de_Minas,2025-12-31T00:00:00,0.0,1.5176165103912354,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guaraciaba,2025-12-31T00:00:00,0.0,1.6977882385253906,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Guiricema,2025-12-31T00:00:00,0.0,1.846624732017517,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Iapu,2025-12-31T00:00:00,0.0,1.5017508268356323,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Imbe_de_Minas,2025-12-31T00:00:00,0.0,1.345171570777893,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Inhapim,2025-12-31T00:00:00,0.0,1.8904517889022827,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Irai_de_Minas,2025-12-31T00:00:00,0.0,2.408802032470703,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itaipe,2025-12-31T00:00:00,0.0,0.9461696147918701,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarandiba,2025-12-31T00:00:00,0.0,2.242680549621582,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Itamarati_de_Minas,2025-12-31T00:00:00,0.0,1.2227946519851685,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequeri,2025-12-31T00:00:00,0.0,1.7193629741668701,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jequitinhonha,2025-12-31T00:00:00,0.0,2.529254674911499,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Jose_Goncalves_de_Minas,2025-12-31T00:00:00,0.0,1.454526424407959,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Lamim,2025-12-31T00:00:00,0.0,2.005608558654785,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mantena,2025-12-31T00:00:00,0.0,1.0422179698944092,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mar_de_Espanha,2025-12-31T00:00:00,0.0,1.655128836631775,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Mata_Verde,2025-12-31T00:00:00,0.0,1.707980751991272,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Matutina,2025-12-31T00:00:00,0.0,1.5710196495056152,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Minas_Novas,2025-12-31T00:00:00,0.0,1.5903096199035645,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Monte_Santo_de_Minas,2025-12-31T00:00:00,0.0,1.5236866474151611,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Nova_Ponte,2025-12-31T00:00:00,0.0,2.550914764404297,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novo_Cruzeiro,2025-12-31T00:00:00,0.0,1.4549410343170166,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Novorizonte,2025-12-31T00:00:00,0.0,1.3957819938659668,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ouro_Verde_de_Minas,2025-12-31T00:00:00,0.0,2.3354332447052,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Passos,2025-12-31T00:00:00,0.0,1.6888834238052368,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Paula_Candido,2025-12-31T00:00:00,0.0,1.5371156930923462,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pecanha,2025-12-31T00:00:00,0.0,2.2007651329040527,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Azul,2025-12-31T00:00:00,0.0,1.3023502826690674,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Bonita,2025-12-31T00:00:00,0.0,1.3725764751434326,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_Dourada,2025-12-31T00:00:00,0.0,1.2300812005996704,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pedra_do_Anta,2025-12-31T00:00:00,0.0,1.448677659034729,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Piedade_de_Caratinga,2025-12-31T00:00:00,0.0,1.8866558074951172,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pocrane,2025-12-31T00:00:00,0.0,1.5473392009735107,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ponto_dos_Volantes,2025-12-31T00:00:00,0.0,0.584234356880188,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pote,2025-12-31T00:00:00,0.0,1.1547003984451294,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Pratinha,2025-12-31T00:00:00,0.0,1.9096629619598389,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Quartel_Geral,2025-12-31T00:00:00,0.0,3.3733084201812744,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Ritapolis,2025-12-31T00:00:00,0.0,1.915785551071167,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sabinopolis,2025-12-31T00:00:00,0.0,1.2482343912124634,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Barbara,2025-12-31T00:00:00,0.0,1.246640920639038,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Maria_do_Suacui,2025-12-31T00:00:00,0.0,1.0279567241668701,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_de_Caldas,2025-12-31T00:00:00,0.0,1.75758957862854,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santa_Rita_do_Itueto,2025-12-31T00:00:00,0.0,1.387058973312378,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Santo_Antonio_do_Retiro,2025-12-31T00:00:00,0.0,1.449393391609192,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Domingos_do_Prata,2025-12-31T00:00:00,0.0,1.115340232849121,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Francisco_de_Paula,2025-12-31T00:00:00,0.0,1.6016401052474976,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Geraldo,2025-12-31T00:00:00,0.0,1.4791884422302246,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Goncalo_do_Rio_Preto,2025-12-31T00:00:00,0.0,2.8204047679901123,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Manteninha,2025-12-31T00:00:00,0.0,1.090868353843689,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Joao_do_Paraiso,2025-12-31T00:00:00,0.0,3.450295925140381,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_da_Barra,2025-12-31T00:00:00,0.0,2.0723462104797363,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Jose_do_Alegre,2025-12-31T00:00:00,0.0,1.6755023002624512,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Miguel_do_Anta,2025-12-31T00:00:00,0.0,1.544454574584961,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_da_Vargem_Alegre,2025-12-31T00:00:00,0.0,1.2848479747772217,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Anta,2025-12-31T00:00:00,0.0,1.7999496459960938,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Sao_Sebastiao_do_Maranhao,2025-12-31T00:00:00,0.0,1.2525620460510254,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senador_Modestino_Goncalves,2025-12-31T00:00:00,0.0,2.702888250350952,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Senhora_dos_Remedios,2025-12-31T00:00:00,0.0,1.5875952243804932,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Simonesia,2025-12-31T00:00:00,0.0,1.514326810836792,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Taiobeiras,2025-12-31T00:00:00,0.0,3.0951457023620605,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tapira,2025-12-31T00:00:00,0.0,1.9099328517913818,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Teixeiras,2025-12-31T00:00:00,0.0,1.5002970695495605,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tocos_do_Moji,2025-12-31T00:00:00,0.0,1.6380796432495117,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Tupaciguara,2025-12-31T00:00:00,0.0,2.636409044265747,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Uberaba,2025-12-31T00:00:00,0.0,2.1094908714294434,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Vargem_Grande_do_Rio_Pardo,2025-12-31T00:00:00,0.0,1.928105115890503,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 1 (2020-225),Visconde_do_Rio_Branco,2025-12-31T00:00:00,0.0,1.4673378467559814,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00012329315471743994
encoder_hidden_size: 256
decoder_layers: 1
decoder_hidden_size: 256
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 100
",2025-09-23T22:57:16
cluster 2 (2020-225),Almenara,2012-12-31T00:00:00,0.45,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Almenara,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Almenara,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Almenara,2015-12-31T00:00:00,1.08,0.7518973350524902,30.38,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Almenara,2016-12-31T00:00:00,0.84,0.9454624056816101,12.56,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Almenara,2017-12-31T00:00:00,1.0,0.905685305595398,9.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Almenara,2018-12-31T00:00:00,1.23,1.0754424333572388,12.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alpinopolis,2012-12-31T00:00:00,2.7,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alpinopolis,2013-12-31T00:00:00,2.71,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alpinopolis,2014-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alpinopolis,2015-12-31T00:00:00,1.82,2.1716384887695312,19.32,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alpinopolis,2016-12-31T00:00:00,2.4,2.0829882621765137,13.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alpinopolis,2017-12-31T00:00:00,2.06,2.070659637451172,0.52,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alpinopolis,2018-12-31T00:00:00,2.23,2.4054203033447266,7.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alto_Jequitiba,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alto_Jequitiba,2013-12-31T00:00:00,1.62,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alto_Jequitiba,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alto_Jequitiba,2015-12-31T00:00:00,1.38,1.3917617797851562,0.85,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alto_Jequitiba,2016-12-31T00:00:00,1.32,1.344586730003357,1.86,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alto_Jequitiba,2017-12-31T00:00:00,1.53,1.2824172973632812,16.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alto_Jequitiba,2018-12-31T00:00:00,1.62,1.4540038108825684,10.25,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Arapua,2012-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Arapua,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Arapua,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Arapua,2015-12-31T00:00:00,1.5,1.726186990737915,15.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Arapua,2016-12-31T00:00:00,1.35,1.676035761833191,24.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Arapua,2017-12-31T00:00:00,1.72,1.5254048109054565,11.31,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Arapua,2018-12-31T00:00:00,1.8,1.582181692123413,12.1,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Araxa,2012-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Araxa,2013-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Araxa,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Araxa,2015-12-31T00:00:00,1.5,1.686822533607483,12.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Araxa,2016-12-31T00:00:00,1.8,1.5833739042282104,12.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Araxa,2017-12-31T00:00:00,1.76,1.6534690856933594,6.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Araxa,2018-12-31T00:00:00,1.94,1.747654676437378,9.91,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Astolfo_Dutra,2012-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Astolfo_Dutra,2013-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Astolfo_Dutra,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Astolfo_Dutra,2015-12-31T00:00:00,1.02,1.0333958864212036,1.31,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Astolfo_Dutra,2016-12-31T00:00:00,1.02,1.0249751806259155,0.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Astolfo_Dutra,2017-12-31T00:00:00,2.67,1.0453181266784668,60.85,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Astolfo_Dutra,2018-12-31T00:00:00,1.2,1.8021128177642822,50.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Bom_Jesus_da_Penha,2012-12-31T00:00:00,2.58,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Bom_Jesus_da_Penha,2013-12-31T00:00:00,2.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Bom_Jesus_da_Penha,2014-12-31T00:00:00,2.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Bom_Jesus_da_Penha,2015-12-31T00:00:00,1.8,2.1663198471069336,20.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Bom_Jesus_da_Penha,2016-12-31T00:00:00,1.92,2.0826034545898438,8.47,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Bom_Jesus_da_Penha,2017-12-31T00:00:00,2.22,1.9101316928863525,13.96,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Bom_Jesus_da_Penha,2018-12-31T00:00:00,1.8,2.1064069271087646,17.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botelhos,2012-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botelhos,2013-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botelhos,2014-12-31T00:00:00,1.74,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botelhos,2015-12-31T00:00:00,1.44,1.4849427938461304,3.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botelhos,2016-12-31T00:00:00,1.2,1.5698540210723877,30.82,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botelhos,2017-12-31T00:00:00,1.88,1.513013482093811,19.52,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botelhos,2018-12-31T00:00:00,2.07,1.6128218173980713,22.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botumirim,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botumirim,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botumirim,2014-12-31T00:00:00,0.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botumirim,2015-12-31T00:00:00,0.48,0.8077327609062195,68.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botumirim,2016-12-31T00:00:00,0.72,0.5560148358345032,22.78,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botumirim,2017-12-31T00:00:00,0.86,0.6131565570831299,28.7,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botumirim,2018-12-31T00:00:00,0.9,0.7695107460021973,14.5,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cabo_Verde,2012-12-31T00:00:00,2.07,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cabo_Verde,2013-12-31T00:00:00,2.04,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cabo_Verde,2014-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cabo_Verde,2015-12-31T00:00:00,1.98,1.8338865041732788,7.38,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cabo_Verde,2016-12-31T00:00:00,1.65,1.8984133005142212,15.06,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cabo_Verde,2017-12-31T00:00:00,1.81,1.7887494564056396,1.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cabo_Verde,2018-12-31T00:00:00,1.61,1.8823935985565186,16.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Caldas,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Caldas,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Caldas,2014-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Caldas,2015-12-31T00:00:00,1.5,1.395910620689392,6.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Caldas,2016-12-31T00:00:00,1.32,1.534272313117981,16.23,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Caldas,2017-12-31T00:00:00,1.86,1.4435993432998657,22.39,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Caldas,2018-12-31T00:00:00,1.62,1.6224215030670166,0.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Conceicao_da_Aparecida,2012-12-31T00:00:00,1.95,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Conceicao_da_Aparecida,2013-12-31T00:00:00,1.7,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Conceicao_da_Aparecida,2014-12-31T00:00:00,1.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Conceicao_da_Aparecida,2015-12-31T00:00:00,2.52,1.7884635925292969,29.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Conceicao_da_Aparecida,2016-12-31T00:00:00,1.99,2.016416311264038,1.33,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Conceicao_da_Aparecida,2017-12-31T00:00:00,1.85,2.1913657188415527,18.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Conceicao_da_Aparecida,2018-12-31T00:00:00,2.03,2.308274507522583,13.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Congonhal,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Congonhal,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Congonhal,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Congonhal,2015-12-31T00:00:00,1.5,1.2380506992340088,17.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Congonhal,2016-12-31T00:00:00,1.5,1.2833305597305298,14.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Congonhal,2017-12-31T00:00:00,2.1,1.4592636823654175,30.51,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Congonhal,2018-12-31T00:00:00,1.8,1.8383480310440063,2.13,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Coromandel,2012-12-31T00:00:00,1.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Coromandel,2013-12-31T00:00:00,2.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Coromandel,2014-12-31T00:00:00,1.91,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Coromandel,2015-12-31T00:00:00,1.91,1.944379210472107,1.8,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Coromandel,2016-12-31T00:00:00,2.09,1.9351991415023804,7.41,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Coromandel,2017-12-31T00:00:00,1.81,1.9703096151351929,8.86,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Coromandel,2018-12-31T00:00:00,2.65,1.975998878479004,25.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Corrego_Fundo,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Corrego_Fundo,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Corrego_Fundo,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Corrego_Fundo,2015-12-31T00:00:00,1.26,1.3349565267562866,5.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Corrego_Fundo,2016-12-31T00:00:00,1.5,1.292163372039795,13.86,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Corrego_Fundo,2017-12-31T00:00:00,1.47,1.4362653493881226,2.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Corrego_Fundo,2018-12-31T00:00:00,3.0,1.5141329765319824,49.53,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cruzeiro_da_Fortaleza,2012-12-31T00:00:00,2.28,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cruzeiro_da_Fortaleza,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cruzeiro_da_Fortaleza,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cruzeiro_da_Fortaleza,2015-12-31T00:00:00,1.8,1.8501116037368774,2.78,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cruzeiro_da_Fortaleza,2016-12-31T00:00:00,2.1,1.914594292640686,8.83,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cruzeiro_da_Fortaleza,2017-12-31T00:00:00,2.33,1.9002519845962524,18.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cruzeiro_da_Fortaleza,2018-12-31T00:00:00,2.1,2.1329309940338135,1.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Dom_Cavati,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Dom_Cavati,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Dom_Cavati,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Dom_Cavati,2015-12-31T00:00:00,0.9,1.0376769304275513,15.3,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Dom_Cavati,2016-12-31T00:00:00,0.72,0.8494462966918945,17.98,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Dom_Cavati,2017-12-31T00:00:00,1.27,0.8583876490592957,32.41,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Dom_Cavati,2018-12-31T00:00:00,1.54,1.0340704917907715,32.85,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaranesia,2012-12-31T00:00:00,1.98,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaranesia,2013-12-31T00:00:00,1.52,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaranesia,2014-12-31T00:00:00,1.86,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaranesia,2015-12-31T00:00:00,1.65,1.7335107326507568,5.06,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaranesia,2016-12-31T00:00:00,2.63,1.7003121376037598,35.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaranesia,2017-12-31T00:00:00,1.36,2.0647456645965576,51.82,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaranesia,2018-12-31T00:00:00,1.7,2.2411789894104004,31.83,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarani,2012-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarani,2013-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarani,2014-12-31T00:00:00,2.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarani,2015-12-31T00:00:00,2.5,1.2212741374969482,51.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarani,2016-12-31T00:00:00,1.19,1.6704535484313965,40.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarani,2017-12-31T00:00:00,2.98,2.15333890914917,27.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarani,2018-12-31T00:00:00,1.8,2.6341257095336914,46.34,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarda-Mor,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarda-Mor,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarda-Mor,2014-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarda-Mor,2015-12-31T00:00:00,1.8,1.87509286403656,4.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarda-Mor,2016-12-31T00:00:00,1.8,1.9528454542160034,8.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarda-Mor,2017-12-31T00:00:00,1.83,1.9412167072296143,6.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarda-Mor,2018-12-31T00:00:00,2.4,1.8246479034423828,23.97,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaxupe,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaxupe,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaxupe,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaxupe,2015-12-31T00:00:00,1.5,1.479814887046814,1.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaxupe,2016-12-31T00:00:00,1.62,1.5391970872879028,4.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaxupe,2017-12-31T00:00:00,1.51,1.6827445030212402,11.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaxupe,2018-12-31T00:00:00,1.8,1.5659407377243042,13.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guimarania,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guimarania,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guimarania,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guimarania,2015-12-31T00:00:00,2.4,1.5246047973632812,36.47,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guimarania,2016-12-31T00:00:00,1.8,1.8046319484710693,0.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guimarania,2017-12-31T00:00:00,1.45,1.8990418910980225,30.97,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guimarania,2018-12-31T00:00:00,1.8,1.9430941343307495,7.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ibitiura_de_Minas,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ibitiura_de_Minas,2013-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ibitiura_de_Minas,2014-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ibitiura_de_Minas,2015-12-31T00:00:00,1.92,1.920753002166748,0.04,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ibitiura_de_Minas,2016-12-31T00:00:00,1.76,2.041055679321289,15.97,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ibitiura_de_Minas,2017-12-31T00:00:00,1.3,1.9841736555099487,52.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ibitiura_de_Minas,2018-12-31T00:00:00,1.68,1.731164813041687,3.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Indaiabira,2012-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Indaiabira,2013-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Indaiabira,2014-12-31T00:00:00,2.7,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Indaiabira,2015-12-31T00:00:00,1.8,2.321148633956909,28.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Indaiabira,2016-12-31T00:00:00,1.86,2.144031524658203,15.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Indaiabira,2017-12-31T00:00:00,2.94,2.077385425567627,29.34,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Indaiabira,2018-12-31T00:00:00,3.0,2.3636326789855957,21.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itabirinha,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itabirinha,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itabirinha,2014-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itabirinha,2015-12-31T00:00:00,0.9,0.9325342774391174,3.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itabirinha,2016-12-31T00:00:00,0.9,0.7519705295562744,16.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itabirinha,2017-12-31T00:00:00,0.57,0.8263580203056335,44.98,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itabirinha,2018-12-31T00:00:00,0.9,0.8787167072296143,2.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itambacuri,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itambacuri,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itambacuri,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itambacuri,2015-12-31T00:00:00,1.2,1.050687551498413,12.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itambacuri,2016-12-31T00:00:00,1.08,0.9362123012542725,13.31,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itambacuri,2017-12-31T00:00:00,0.84,1.0442343950271606,24.31,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itambacuri,2018-12-31T00:00:00,1.2,1.1138628721237183,7.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Jacui,2012-12-31T00:00:00,1.65,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Jacui,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Jacui,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Jacui,2015-12-31T00:00:00,1.5,1.3943681716918945,7.04,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Jacui,2016-12-31T00:00:00,1.7,1.375565767288208,19.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Jacui,2017-12-31T00:00:00,1.73,1.5609668493270874,9.77,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Jacui,2018-12-31T00:00:00,1.8,1.7178444862365723,4.56,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juiz_de_Fora,2012-12-31T00:00:00,0.73,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juiz_de_Fora,2013-12-31T00:00:00,0.73,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juiz_de_Fora,2014-12-31T00:00:00,0.73,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juiz_de_Fora,2015-12-31T00:00:00,5.5,0.3214225172996521,94.16,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juiz_de_Fora,2016-12-31T00:00:00,0.73,0.4543943405151367,37.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juiz_de_Fora,2017-12-31T00:00:00,1.0,1.5904916524887085,59.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juiz_de_Fora,2018-12-31T00:00:00,1.0,2.675532817840576,167.55,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juruaia,2012-12-31T00:00:00,1.92,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juruaia,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juruaia,2014-12-31T00:00:00,1.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juruaia,2015-12-31T00:00:00,1.5,1.4787077903747559,1.42,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juruaia,2016-12-31T00:00:00,1.8,1.530951738357544,14.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juruaia,2017-12-31T00:00:00,1.69,1.7323508262634277,2.51,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juruaia,2018-12-31T00:00:00,1.8,1.742262601852417,3.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ladainha,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ladainha,2013-12-31T00:00:00,1.11,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ladainha,2014-12-31T00:00:00,1.15,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ladainha,2015-12-31T00:00:00,1.02,1.024988055229187,0.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ladainha,2016-12-31T00:00:00,0.9,1.0561957359313965,17.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ladainha,2017-12-31T00:00:00,0.56,1.0112519264221191,80.58,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ladainha,2018-12-31T00:00:00,1.08,0.9236157536506653,14.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Lagoa_Dourada,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Lagoa_Dourada,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Lagoa_Dourada,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Lagoa_Dourada,2015-12-31T00:00:00,2.25,0.7976273894309998,64.55,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Lagoa_Dourada,2016-12-31T00:00:00,2.1,1.5785454511642456,24.83,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Lagoa_Dourada,2017-12-31T00:00:00,1.75,2.044032573699951,16.8,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Lagoa_Dourada,2018-12-31T00:00:00,2.7,2.154952049255371,20.19,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Martins_Soares,2012-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Martins_Soares,2013-12-31T00:00:00,1.98,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Martins_Soares,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Martins_Soares,2015-12-31T00:00:00,1.8,1.75961434841156,2.24,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Martins_Soares,2016-12-31T00:00:00,1.8,1.5511647462844849,13.82,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Martins_Soares,2017-12-31T00:00:00,1.45,1.673377275466919,15.41,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Martins_Soares,2018-12-31T00:00:00,1.62,1.7626116275787354,8.8,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Miradouro,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Miradouro,2013-12-31T00:00:00,2.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Miradouro,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Miradouro,2015-12-31T00:00:00,1.59,1.259960412979126,20.76,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Miradouro,2016-12-31T00:00:00,1.2,1.511164665222168,25.93,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Miradouro,2017-12-31T00:00:00,1.16,1.2813843488693237,10.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Miradouro,2018-12-31T00:00:00,1.5,1.4476643800735474,3.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Monte_Belo,2012-12-31T00:00:00,1.63,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Monte_Belo,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Monte_Belo,2014-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Monte_Belo,2015-12-31T00:00:00,1.26,1.3119040727615356,4.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Monte_Belo,2016-12-31T00:00:00,1.62,1.281730055809021,20.88,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Monte_Belo,2017-12-31T00:00:00,2.43,1.4296875,41.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Monte_Belo,2018-12-31T00:00:00,1.8,1.989560842514038,10.53,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Nova_Resende,2012-12-31T00:00:00,2.88,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Nova_Resende,2013-12-31T00:00:00,2.18,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Nova_Resende,2014-12-31T00:00:00,1.7,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Nova_Resende,2015-12-31T00:00:00,2.2,1.9727580547332764,10.33,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Nova_Resende,2016-12-31T00:00:00,2.08,2.1082427501678467,1.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Nova_Resende,2017-12-31T00:00:00,1.76,2.046687364578247,16.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Nova_Resende,2018-12-31T00:00:00,1.62,2.115283727645874,30.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Patos_de_Minas,2012-12-31T00:00:00,1.92,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Patos_de_Minas,2013-12-31T00:00:00,2.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Patos_de_Minas,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Patos_de_Minas,2015-12-31T00:00:00,2.4,1.8643275499343872,22.32,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Patos_de_Minas,2016-12-31T00:00:00,1.5,2.137369155883789,42.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Patos_de_Minas,2017-12-31T00:00:00,1.7,1.8337081670761108,7.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Patos_de_Minas,2018-12-31T00:00:00,1.8,2.011425733566284,11.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Piracema,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Piracema,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Piracema,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Piracema,2015-12-31T00:00:00,1.32,1.3968110084533691,5.82,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Piracema,2016-12-31T00:00:00,1.2,1.3920464515686035,16.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Piracema,2017-12-31T00:00:00,1.33,1.3554463386535645,1.91,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Piracema,2018-12-31T00:00:00,1.5,1.3162040710449219,12.25,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Pratapolis,2012-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Pratapolis,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Pratapolis,2014-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Pratapolis,2015-12-31T00:00:00,2.4,1.6384494304656982,31.73,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Pratapolis,2016-12-31T00:00:00,1.8,1.7505590915679932,2.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Pratapolis,2017-12-31T00:00:00,1.62,2.1113815307617188,30.33,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Pratapolis,2018-12-31T00:00:00,1.8,2.225127696990967,23.62,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Presidente_Kubitschek,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Presidente_Kubitschek,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Presidente_Kubitschek,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Presidente_Kubitschek,2015-12-31T00:00:00,1.33,1.4515061378479004,9.14,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Presidente_Kubitschek,2016-12-31T00:00:00,1.25,1.2289268970489502,1.69,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Presidente_Kubitschek,2017-12-31T00:00:00,2.13,1.257613182067871,40.96,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Presidente_Kubitschek,2018-12-31T00:00:00,0.81,1.694915533065796,109.25,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Rio_Vermelho,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Rio_Vermelho,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Rio_Vermelho,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Rio_Vermelho,2015-12-31T00:00:00,1.2,1.2631924152374268,5.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Rio_Vermelho,2016-12-31T00:00:00,1.08,1.1321128606796265,4.83,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Rio_Vermelho,2017-12-31T00:00:00,0.9,1.2197386026382446,35.53,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Rio_Vermelho,2018-12-31T00:00:00,1.2,1.117842674255371,6.85,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Domingos_das_Dores,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Domingos_das_Dores,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Domingos_das_Dores,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Domingos_das_Dores,2015-12-31T00:00:00,1.5,1.4694801568984985,2.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Domingos_das_Dores,2016-12-31T00:00:00,1.8,1.4888160228729248,17.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Domingos_das_Dores,2017-12-31T00:00:00,1.68,1.5640788078308105,6.9,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Domingos_das_Dores,2018-12-31T00:00:00,1.74,1.7162220478057861,1.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Goncalo_do_Abaete,2012-12-31T00:00:00,2.52,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Goncalo_do_Abaete,2013-12-31T00:00:00,3.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Goncalo_do_Abaete,2014-12-31T00:00:00,3.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Goncalo_do_Abaete,2015-12-31T00:00:00,2.4,2.701124906539917,12.55,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Goncalo_do_Abaete,2016-12-31T00:00:00,2.4,2.728376626968384,13.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Goncalo_do_Abaete,2017-12-31T00:00:00,2.0,2.544513702392578,27.23,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Goncalo_do_Abaete,2018-12-31T00:00:00,1.92,2.401392698287964,25.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_Batista_do_Gloria,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_Batista_do_Gloria,2013-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_Batista_do_Gloria,2014-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_Batista_do_Gloria,2015-12-31T00:00:00,1.08,1.707198977470398,58.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_Batista_do_Gloria,2016-12-31T00:00:00,1.32,1.4628667831420898,10.82,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_Batista_do_Gloria,2017-12-31T00:00:00,1.67,1.308329463005066,21.66,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_Batista_do_Gloria,2018-12-31T00:00:00,1.8,1.7353755235671997,3.59,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_da_Mata,2012-12-31T00:00:00,1.62,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_da_Mata,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_da_Mata,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_da_Mata,2015-12-31T00:00:00,1.08,1.2156355381011963,12.56,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_da_Mata,2016-12-31T00:00:00,1.38,1.1467525959014893,16.9,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_da_Mata,2017-12-31T00:00:00,1.5,1.248988151550293,16.73,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_da_Mata,2018-12-31T00:00:00,1.38,1.3982884883880615,1.33,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Pedro_da_Uniao,2012-12-31T00:00:00,2.28,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Pedro_da_Uniao,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Pedro_da_Uniao,2014-12-31T00:00:00,1.62,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Pedro_da_Uniao,2015-12-31T00:00:00,1.5,1.5102109909057617,0.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Pedro_da_Uniao,2016-12-31T00:00:00,1.92,1.4792745113372803,22.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Pedro_da_Uniao,2017-12-31T00:00:00,1.69,1.7582839727401733,4.04,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Pedro_da_Uniao,2018-12-31T00:00:00,1.8,1.7999420166015625,0.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Tapirai,2012-12-31T00:00:00,1.62,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Tapirai,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Tapirai,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Tapirai,2015-12-31T00:00:00,1.5,1.5896332263946533,5.98,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Tapirai,2016-12-31T00:00:00,1.62,1.5456814765930176,4.59,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Tapirai,2017-12-31T00:00:00,1.76,1.6627700328826904,5.52,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Tapirai,2018-12-31T00:00:00,1.8,1.6783734560012817,6.76,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Unai,2012-12-31T00:00:00,2.94,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Unai,2013-12-31T00:00:00,3.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Unai,2014-12-31T00:00:00,2.94,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Unai,2015-12-31T00:00:00,2.64,2.9432713985443115,11.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Unai,2016-12-31T00:00:00,2.7,2.8479154109954834,5.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Unai,2017-12-31T00:00:00,2.6,2.7181291580200195,4.54,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Unai,2018-12-31T00:00:00,2.52,2.675899028778076,6.19,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varjao_de_Minas,2012-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varjao_de_Minas,2013-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varjao_de_Minas,2014-12-31T00:00:00,3.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varjao_de_Minas,2015-12-31T00:00:00,2.4,2.4970903396606445,4.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varjao_de_Minas,2016-12-31T00:00:00,2.4,2.4756922721862793,3.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varjao_de_Minas,2017-12-31T00:00:00,2.4,2.5590286254882812,6.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varjao_de_Minas,2018-12-31T00:00:00,2.4,4.36019229888916,81.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varzea_da_Palma,2012-12-31T00:00:00,4.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varzea_da_Palma,2013-12-31T00:00:00,3.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varzea_da_Palma,2014-12-31T00:00:00,3.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varzea_da_Palma,2015-12-31T00:00:00,2.46,3.357973098754883,36.5,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varzea_da_Palma,2016-12-31T00:00:00,2.4,2.7775402069091797,15.73,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varzea_da_Palma,2017-12-31T00:00:00,3.2,2.568408250808716,19.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varzea_da_Palma,2018-12-31T00:00:00,3.6,2.843975305557251,21.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Almenara,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alpinopolis,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alto_Jequitiba,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Arapua,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Araxa,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Astolfo_Dutra,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Bom_Jesus_da_Penha,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botelhos,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botumirim,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cabo_Verde,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Caldas,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Conceicao_da_Aparecida,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Congonhal,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Coromandel,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Corrego_Fundo,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cruzeiro_da_Fortaleza,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Dom_Cavati,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaranesia,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarani,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarda-Mor,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaxupe,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guimarania,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ibitiura_de_Minas,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Indaiabira,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itabirinha,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itambacuri,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Jacui,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juiz_de_Fora,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juruaia,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ladainha,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Lagoa_Dourada,2019-12-31T00:00:00,3.0,2,-33.33,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Martins_Soares,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Miradouro,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Monte_Belo,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Nova_Resende,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Patos_de_Minas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Piracema,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Pratapolis,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Presidente_Kubitschek,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Rio_Vermelho,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Domingos_das_Dores,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Goncalo_do_Abaete,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_Batista_do_Gloria,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_da_Mata,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Pedro_da_Uniao,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Tapirai,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Unai,2019-12-31T00:00:00,3.0,3,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varjao_de_Minas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varzea_da_Palma,2019-12-31T00:00:00,4.0,3,-25.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Almenara,2020-12-31T00:00:00,0.78,0.9833159446716309,26.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alpinopolis,2020-12-31T00:00:00,2.765998457979954,2.1175708770751953,-23.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alto_Jequitiba,2020-12-31T00:00:00,1.8000000000000005,1.549996256828308,-13.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Arapua,2020-12-31T00:00:00,1.5,1.5257205963134766,1.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Araxa,2020-12-31T00:00:00,1.786008230452675,1.6083500385284424,-9.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Astolfo_Dutra,2020-12-31T00:00:00,1.75,1.506742238998413,-13.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Bom_Jesus_da_Penha,2020-12-31T00:00:00,1.986087924318308,2.046079158782959,3.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botelhos,2020-12-31T00:00:00,1.716062736614386,1.9509522914886475,13.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botumirim,2020-12-31T00:00:00,0.9200000000000002,0.8558419346809387,-6.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cabo_Verde,2020-12-31T00:00:00,2.249052581714827,1.7920702695846558,-20.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Caldas,2020-12-31T00:00:00,1.68,1.6266136169433594,-3.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Conceicao_da_Aparecida,2020-12-31T00:00:00,2.243948871362524,1.9323418140411377,-13.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Congonhal,2020-12-31T00:00:00,1.619354838709677,1.7874698638916016,10.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Coromandel,2020-12-31T00:00:00,1.818934240362812,1.9407658576965332,6.7,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Corrego_Fundo,2020-12-31T00:00:00,1.5,1.8412762880325317,22.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cruzeiro_da_Fortaleza,2020-12-31T00:00:00,2.1,1.9162486791610718,-8.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Dom_Cavati,2020-12-31T00:00:00,0.7894736842105263,1.0923885107040405,38.37,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaranesia,2020-12-31T00:00:00,1.564686285397002,1.3902015686035156,-11.15,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarani,2020-12-31T00:00:00,1.127272727272727,2.1277713775634766,88.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarda-Mor,2020-12-31T00:00:00,1.7994350282485878,1.8918589353561401,5.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaxupe,2020-12-31T00:00:00,1.7400000000000002,1.4905251264572144,-14.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guimarania,2020-12-31T00:00:00,2.149930843706777,1.5821460485458374,-26.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ibitiura_de_Minas,2020-12-31T00:00:00,1.68,1.4220539331436157,-15.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Indaiabira,2020-12-31T00:00:00,3.0,2.8165154457092285,-6.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itabirinha,2020-12-31T00:00:00,0.7992424242424243,0.8469907641410828,5.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itambacuri,2020-12-31T00:00:00,1.091304347826087,0.9901677966117859,-9.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Jacui,2020-12-31T00:00:00,1.68,1.7305958271026611,3.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juiz_de_Fora,2020-12-31T00:00:00,1.222222222222222,0.9810856580734253,-19.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juruaia,2020-12-31T00:00:00,1.92,1.7135694026947021,-10.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ladainha,2020-12-31T00:00:00,1.242268041237113,0.9288707971572876,-25.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Lagoa_Dourada,2020-12-31T00:00:00,2.1,2.126150608062744,1.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Martins_Soares,2020-12-31T00:00:00,1.8000000000000005,1.5626718997955322,-13.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Miradouro,2020-12-31T00:00:00,1.5,1.3160908222198486,-12.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Monte_Belo,2020-12-31T00:00:00,1.8000000000000005,1.87919020652771,4.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Nova_Resende,2020-12-31T00:00:00,2.353040067245727,1.7826743125915527,-24.24,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Patos_de_Minas,2020-12-31T00:00:00,1.941759465478842,1.695046305656433,-12.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Piracema,2020-12-31T00:00:00,1.333333333333333,1.291872262954712,-3.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Pratapolis,2020-12-31T00:00:00,2.402298850574712,1.7795238494873047,-25.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Presidente_Kubitschek,2020-12-31T00:00:00,1.789473684210526,1.4006056785583496,-21.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Rio_Vermelho,2020-12-31T00:00:00,0.9333333333333332,1.0802295207977295,15.74,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Domingos_das_Dores,2020-12-31T00:00:00,1.56,1.546181082725525,-0.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Goncalo_do_Abaete,2020-12-31T00:00:00,2.44,1.939623475074768,-20.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_Batista_do_Gloria,2020-12-31T00:00:00,1.5,1.7189552783966064,14.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_da_Mata,2020-12-31T00:00:00,1.6399999999999997,1.4132390022277832,-13.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Pedro_da_Uniao,2020-12-31T00:00:00,2.1,1.6854548454284668,-19.74,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Tapirai,2020-12-31T00:00:00,1.799716914366596,1.7331039905548096,-3.7,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Unai,2020-12-31T00:00:00,2.3398907103825146,2.6706864833831787,14.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varjao_de_Minas,2020-12-31T00:00:00,2.519786096256685,2.4173805713653564,-4.06,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varzea_da_Palma,2020-12-31T00:00:00,3.0,3.440274715423584,14.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Almenara,2021-12-31T00:00:00,0.78,0.9363704323768616,20.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alpinopolis,2021-12-31T00:00:00,1.67993145468393,2.3577656745910645,40.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alto_Jequitiba,2021-12-31T00:00:00,0.9,1.6318845748901367,81.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Arapua,2021-12-31T00:00:00,1.32,1.4860866069793701,12.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Araxa,2021-12-31T00:00:00,1.250440917107584,1.6664854288101196,33.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Astolfo_Dutra,2021-12-31T00:00:00,1.75,1.3446159362792969,-23.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Bom_Jesus_da_Penha,2021-12-31T00:00:00,1.519900497512438,1.9803799390792847,30.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botelhos,2021-12-31T00:00:00,1.26,1.8983991146087646,50.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botumirim,2021-12-31T00:00:00,0.7241379310344828,0.8407806158065796,16.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cabo_Verde,2021-12-31T00:00:00,1.32,1.9470806121826172,47.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Caldas,2021-12-31T00:00:00,1.2,1.5845264196395874,32.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Conceicao_da_Aparecida,2021-12-31T00:00:00,1.62,2.079167604446411,28.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Congonhal,2021-12-31T00:00:00,1.6215384615384625,1.645151138305664,1.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Coromandel,2021-12-31T00:00:00,1.249939246658566,1.999185562133789,59.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Corrego_Fundo,2021-12-31T00:00:00,1.1984126984126982,2.0164670944213867,68.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cruzeiro_da_Fortaleza,2021-12-31T00:00:00,1.800184162062615,1.8795595169067383,4.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Dom_Cavati,2021-12-31T00:00:00,0.6000000000000001,1.0221699476242065,70.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaranesia,2021-12-31T00:00:00,1.031014249790444,1.4848239421844482,44.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarani,2021-12-31T00:00:00,0.953846153846154,1.4990483522415161,57.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarda-Mor,2021-12-31T00:00:00,1.7994350282485878,1.96263587474823,9.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaxupe,2021-12-31T00:00:00,1.02,1.6101653575897217,57.86,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guimarania,2021-12-31T00:00:00,1.541380188439012,1.8223443031311035,18.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ibitiura_de_Minas,2021-12-31T00:00:00,1.9198795180722887,1.5638489723205566,-18.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Indaiabira,2021-12-31T00:00:00,1.984444444444444,2.8321170806884766,42.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itabirinha,2021-12-31T00:00:00,0.9659090909090908,0.924831748008728,-4.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itambacuri,2021-12-31T00:00:00,0.8987341772151899,1.0604301691055298,17.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Jacui,2021-12-31T00:00:00,0.96,1.710076093673706,78.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juiz_de_Fora,2021-12-31T00:00:00,1.142857142857143,1.0226173400878906,-10.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juruaia,2021-12-31T00:00:00,1.5,1.794990062713623,19.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ladainha,2021-12-31T00:00:00,0.7199999999999999,1.1540451049804688,60.28,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Lagoa_Dourada,2021-12-31T00:00:00,1.2,2.433434009552002,102.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Martins_Soares,2021-12-31T00:00:00,1.2,1.6771303415298462,39.76,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Miradouro,2021-12-31T00:00:00,1.2,1.435734748840332,19.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Monte_Belo,2021-12-31T00:00:00,1.56,1.7425737380981445,11.7,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Nova_Resende,2021-12-31T00:00:00,1.3800031392246113,2.0063793659210205,45.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Patos_de_Minas,2021-12-31T00:00:00,1.7224109589041097,1.7818880081176758,3.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Piracema,2021-12-31T00:00:00,1.2,1.3260698318481445,10.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Pratapolis,2021-12-31T00:00:00,0.9602272727272728,1.9306151866912842,101.06,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Presidente_Kubitschek,2021-12-31T00:00:00,0.7368421052631579,1.293152928352356,75.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Rio_Vermelho,2021-12-31T00:00:00,0.7777777777777777,1.1135791540145874,43.17,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Domingos_das_Dores,2021-12-31T00:00:00,1.2,1.5281004905700684,27.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Goncalo_do_Abaete,2021-12-31T00:00:00,1.67887323943662,2.073613166809082,23.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_Batista_do_Gloria,2021-12-31T00:00:00,1.110749185667752,1.6600911617279053,49.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_da_Mata,2021-12-31T00:00:00,0.8995327102803738,1.463550329208374,62.7,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Pedro_da_Uniao,2021-12-31T00:00:00,1.479945054945055,1.821309208869934,23.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Tapirai,2021-12-31T00:00:00,1.396563119629874,1.7617383003234863,26.15,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Unai,2021-12-31T00:00:00,2.639892904953146,2.5682075023651123,-2.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varjao_de_Minas,2021-12-31T00:00:00,2.390254805543138,2.470228672027588,3.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varzea_da_Palma,2021-12-31T00:00:00,1.68,3.3926875591278076,101.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Almenara,2022-12-31T00:00:00,0.78,0.7814896702766418,0.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alpinopolis,2022-12-31T00:00:00,1.4420494699646638,1.9587280750274658,35.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alto_Jequitiba,2022-12-31T00:00:00,0.9,1.3188912868499756,46.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Arapua,2022-12-31T00:00:00,1.14,1.313887119293213,15.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Araxa,2022-12-31T00:00:00,1.077872465471643,1.4239046573638916,32.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Astolfo_Dutra,2022-12-31T00:00:00,0.625,1.5119171142578125,141.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Bom_Jesus_da_Penha,2022-12-31T00:00:00,1.6298200514138823,1.6540093421936035,1.48,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botelhos,2022-12-31T00:00:00,0.9,1.5550014972686768,72.78,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botumirim,2022-12-31T00:00:00,0.7241379310344828,0.801638126373291,10.7,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cabo_Verde,2022-12-31T00:00:00,1.35,1.6967129707336426,25.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Caldas,2022-12-31T00:00:00,1.2,1.410395622253418,17.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Conceicao_da_Aparecida,2022-12-31T00:00:00,1.53,1.8932948112487793,23.74,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Congonhal,2022-12-31T00:00:00,1.080597014925373,1.5721503496170044,45.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Coromandel,2022-12-31T00:00:00,1.260025542784164,1.4572499990463257,15.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Corrego_Fundo,2022-12-31T00:00:00,1.079051383399209,1.4451056718826294,33.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cruzeiro_da_Fortaleza,2022-12-31T00:00:00,1.5598526703499078,1.7175766229629517,10.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Dom_Cavati,2022-12-31T00:00:00,0.6000000000000001,0.7103283405303955,18.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaranesia,2022-12-31T00:00:00,1.086968085106383,1.1544302701950073,6.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarani,2022-12-31T00:00:00,1.2,1.1816909313201904,-1.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarda-Mor,2022-12-31T00:00:00,1.858757062146893,1.7993476390838623,-3.2,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaxupe,2022-12-31T00:00:00,1.14,1.2450953722000122,9.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guimarania,2022-12-31T00:00:00,1.2220588235294123,1.635636806488037,33.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ibitiura_de_Minas,2022-12-31T00:00:00,1.3799999999999997,1.5928850173950195,15.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Indaiabira,2022-12-31T00:00:00,2.1723404255319148,2.4418039321899414,12.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itabirinha,2022-12-31T00:00:00,0.8674242424242423,0.9355524778366089,7.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itambacuri,2022-12-31T00:00:00,0.9620253164556964,0.9763664603233337,1.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Jacui,2022-12-31T00:00:00,1.08,1.291463851928711,19.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juiz_de_Fora,2022-12-31T00:00:00,1.5714285714285707,1.059368371963501,-32.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juruaia,2022-12-31T00:00:00,1.5,1.6254751682281494,8.37,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ladainha,2022-12-31T00:00:00,0.6000000000000001,0.9047709107398987,50.8,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Lagoa_Dourada,2022-12-31T00:00:00,1.5,1.8833246231079102,25.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Martins_Soares,2022-12-31T00:00:00,1.3799999999999997,1.498138427734375,8.56,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Miradouro,2022-12-31T00:00:00,0.96,1.3050559759140015,35.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Monte_Belo,2022-12-31T00:00:00,0.96,1.642592430114746,71.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Nova_Resende,2022-12-31T00:00:00,1.5,1.7765955924987793,18.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Patos_de_Minas,2022-12-31T00:00:00,1.91614730878187,1.7444331645965576,-8.96,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Piracema,2022-12-31T00:00:00,1.0,1.2129414081573486,21.29,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Pratapolis,2022-12-31T00:00:00,1.2,1.0422122478485107,-13.15,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Presidente_Kubitschek,2022-12-31T00:00:00,0.896551724137931,1.2257436513900757,36.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Rio_Vermelho,2022-12-31T00:00:00,0.9090909090909092,0.9620172381401062,5.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Domingos_das_Dores,2022-12-31T00:00:00,1.2,1.3210968971252441,10.09,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Goncalo_do_Abaete,2022-12-31T00:00:00,1.5,1.9552183151245117,30.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_Batista_do_Gloria,2022-12-31T00:00:00,1.232209737827715,1.3157693147659302,6.78,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_da_Mata,2022-12-31T00:00:00,0.959748427672956,1.239460825920105,29.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Pedro_da_Uniao,2022-12-31T00:00:00,1.4700000000000002,1.588232398033142,8.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Tapirai,2022-12-31T00:00:00,1.324520819563781,1.5024938583374023,13.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Unai,2022-12-31T00:00:00,2.52010582010582,2.5463545322418213,1.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varjao_de_Minas,2022-12-31T00:00:00,2.383928571428572,2.4582810401916504,3.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varzea_da_Palma,2022-12-31T00:00:00,2.5200000000000005,2.8127329349517822,11.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Almenara,2023-12-31T00:00:00,0.78,0.6963400840759277,-10.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alpinopolis,2023-12-31T00:00:00,1.8000000000000005,1.969742774963379,9.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alto_Jequitiba,2023-12-31T00:00:00,1.08,1.1196752786636353,3.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Arapua,2023-12-31T00:00:00,0.9,1.3316794633865356,47.96,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Araxa,2023-12-31T00:00:00,1.638401296246287,1.49452805519104,-8.78,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Astolfo_Dutra,2023-12-31T00:00:00,0.625,1.5207293033599854,143.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Bom_Jesus_da_Penha,2023-12-31T00:00:00,1.2,1.7380576133728027,44.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botelhos,2023-12-31T00:00:00,1.080052666227781,1.2927173376083374,19.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botumirim,2023-12-31T00:00:00,0.59375,0.7585144639015198,27.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cabo_Verde,2023-12-31T00:00:00,1.319976635514019,1.6334563493728638,23.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Caldas,2023-12-31T00:00:00,1.56,1.3338525295257568,-14.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Conceicao_da_Aparecida,2023-12-31T00:00:00,1.679948420373952,1.8187825679779053,8.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Congonhal,2023-12-31T00:00:00,1.2,1.447479486465454,20.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Coromandel,2023-12-31T00:00:00,2.0926575541308825,1.4593145847320557,-30.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Corrego_Fundo,2023-12-31T00:00:00,1.320158102766798,1.2919018268585205,-2.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cruzeiro_da_Fortaleza,2023-12-31T00:00:00,1.5,1.8329572677612305,22.2,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Dom_Cavati,2023-12-31T00:00:00,0.6000000000000001,0.6567723751068115,9.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaranesia,2023-12-31T00:00:00,1.1961439588688951,1.269586205482483,6.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarani,2023-12-31T00:00:00,0.9655172413793104,1.1090588569641113,14.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarda-Mor,2023-12-31T00:00:00,1.7994350282485878,1.827648401260376,1.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaxupe,2023-12-31T00:00:00,1.2,1.3192800283432007,9.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guimarania,2023-12-31T00:00:00,1.7704600484261497,1.6411964893341064,-7.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ibitiura_de_Minas,2023-12-31T00:00:00,1.150344827586207,1.647496223449707,43.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Indaiabira,2023-12-31T00:00:00,1.9320754716981128,2.17579984664917,12.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itabirinha,2023-12-31T00:00:00,1.027027027027027,0.8810473680496216,-14.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itambacuri,2023-12-31T00:00:00,0.8987341772151899,0.9701905846595764,7.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Jacui,2023-12-31T00:00:00,1.32,1.2807526588439941,-2.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juiz_de_Fora,2023-12-31T00:00:00,0.7142857142857143,1.6054283380508423,124.76,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juruaia,2023-12-31T00:00:00,1.5,1.66291081905365,10.86,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ladainha,2023-12-31T00:00:00,1.081818181818182,0.7754219770431519,-28.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Lagoa_Dourada,2023-12-31T00:00:00,1.8000000000000005,2.2396678924560547,24.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Martins_Soares,2023-12-31T00:00:00,1.2,1.405429482460022,17.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Miradouro,2023-12-31T00:00:00,1.140084388185654,1.2585264444351196,10.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Monte_Belo,2023-12-31T00:00:00,1.3799999999999997,1.4490277767181396,5.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Nova_Resende,2023-12-31T00:00:00,1.56,1.7667450904846191,13.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Patos_de_Minas,2023-12-31T00:00:00,1.782452316076294,1.8809906244277954,5.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Piracema,2023-12-31T00:00:00,1.2,1.1910347938537598,-0.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Pratapolis,2023-12-31T00:00:00,1.8011363636363642,1.5216675996780396,-15.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Presidente_Kubitschek,2023-12-31T00:00:00,0.8947368421052632,1.1005202531814575,23.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Rio_Vermelho,2023-12-31T00:00:00,0.9,0.8391260504722595,-6.76,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Domingos_das_Dores,2023-12-31T00:00:00,1.296078431372549,1.292525291442871,-0.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Goncalo_do_Abaete,2023-12-31T00:00:00,1.619047619047619,1.9409971237182617,19.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_Batista_do_Gloria,2023-12-31T00:00:00,1.5232974910394272,1.300846815109253,-14.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_da_Mata,2023-12-31T00:00:00,1.08,1.1968778371810913,10.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Pedro_da_Uniao,2023-12-31T00:00:00,1.5,1.7077240943908691,13.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Tapirai,2023-12-31T00:00:00,1.5579665220086798,1.50071120262146,-3.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Unai,2023-12-31T00:00:00,2.520095187731359,2.46653151512146,-2.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varjao_de_Minas,2023-12-31T00:00:00,2.3892857142857142,2.43648624420166,1.98,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varzea_da_Palma,2023-12-31T00:00:00,3.3,2.482365131378174,-24.78,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Almenara,2024-12-31T00:00:00,0.78,0.5721628665924072,-26.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alpinopolis,2024-12-31T00:00:00,2.08,1.7064040899276733,-17.96,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alto_Jequitiba,2024-12-31T00:00:00,1.02,0.9684171676635742,-5.06,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Arapua,2024-12-31T00:00:00,1.0805194805194802,1.100975751876831,1.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Araxa,2024-12-31T00:00:00,1.412283279459301,1.3194923400878906,-6.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Astolfo_Dutra,2024-12-31T00:00:00,0.8749999999999999,1.0060372352600098,14.98,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Bom_Jesus_da_Penha,2024-12-31T00:00:00,1.56,1.5605037212371826,0.03,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botelhos,2024-12-31T00:00:00,1.08,1.387978434562683,28.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botumirim,2024-12-31T00:00:00,0.7250000000000001,0.6401609778404236,-11.7,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cabo_Verde,2024-12-31T00:00:00,1.32,1.3350830078125,1.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Caldas,2024-12-31T00:00:00,1.68,1.3535411357879639,-19.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Conceicao_da_Aparecida,2024-12-31T00:00:00,1.5,1.628330945968628,8.56,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Congonhal,2024-12-31T00:00:00,1.8000000000000005,1.3162634372711182,-26.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Coromandel,2024-12-31T00:00:00,1.6144686299615878,1.548271656036377,-4.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Corrego_Fundo,2024-12-31T00:00:00,1.501960784313725,1.2374253273010254,-17.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cruzeiro_da_Fortaleza,2024-12-31T00:00:00,2.22,1.6038403511047363,-27.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Dom_Cavati,2024-12-31T00:00:00,0.6000000000000001,0.7802995443344116,30.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaranesia,2024-12-31T00:00:00,1.205714285714286,1.1212371587753296,-7.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarani,2024-12-31T00:00:00,0.9655172413793104,1.062463402748108,10.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarda-Mor,2024-12-31T00:00:00,1.740112994350282,1.826176404953003,4.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaxupe,2024-12-31T00:00:00,1.32,1.1385400295257568,-13.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guimarania,2024-12-31T00:00:00,1.8219512195121947,1.4736692905426025,-19.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ibitiura_de_Minas,2024-12-31T00:00:00,1.360264900662252,1.581781029701233,16.28,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Indaiabira,2024-12-31T00:00:00,2.511111111111111,1.9779043197631836,-21.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itabirinha,2024-12-31T00:00:00,0.8918918918918919,0.9617440700531006,7.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itambacuri,2024-12-31T00:00:00,1.084388185654009,0.9137753248214722,-15.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Jacui,2024-12-31T00:00:00,1.3799999999999997,1.197453498840332,-13.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juiz_de_Fora,2024-12-31T00:00:00,1.166666666666667,1.206222414970398,3.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juruaia,2024-12-31T00:00:00,1.68,3.152611017227173,87.66,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ladainha,2024-12-31T00:00:00,1.12,0.7740495800971985,-30.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Lagoa_Dourada,2024-12-31T00:00:00,1.5,1.6999998092651367,13.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Martins_Soares,2024-12-31T00:00:00,1.2,1.2699823379516602,5.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Miradouro,2024-12-31T00:00:00,1.13996383363472,1.1077295541763306,-2.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Monte_Belo,2024-12-31T00:00:00,1.5,1.6333355903625488,8.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Nova_Resende,2024-12-31T00:00:00,1.32,1.5171279907226562,14.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Patos_de_Minas,2024-12-31T00:00:00,1.6624523160762943,1.7949551343917847,7.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Piracema,2024-12-31T00:00:00,0.8,1.1887627840042114,48.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Pratapolis,2024-12-31T00:00:00,1.1988636363636362,1.381994366645813,15.28,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Presidente_Kubitschek,2024-12-31T00:00:00,0.7368421052631579,0.7971233129501343,8.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Rio_Vermelho,2024-12-31T00:00:00,0.9230769230769232,0.8351401090621948,-9.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Domingos_das_Dores,2024-12-31T00:00:00,1.41,1.236568808555603,-12.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Goncalo_do_Abaete,2024-12-31T00:00:00,1.5,1.6013209819793701,6.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_Batista_do_Gloria,2024-12-31T00:00:00,1.548387096774194,1.331183910369873,-14.03,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_da_Mata,2024-12-31T00:00:00,1.32,1.004290223121643,-23.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Pedro_da_Uniao,2024-12-31T00:00:00,2.04,1.4926424026489258,-26.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Tapirai,2024-12-31T00:00:00,1.9163050216986977,1.426127314567566,-25.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Unai,2024-12-31T00:00:00,2.2801061007957566,2.546196460723877,11.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varjao_de_Minas,2024-12-31T00:00:00,2.6876337184424486,2.387718439102173,-11.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varzea_da_Palma,2024-12-31T00:00:00,3.0,2.0806851387023926,-30.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Almenara,2025-12-31T00:00:00,0.0,0.9548200964927673,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alpinopolis,2025-12-31T00:00:00,0.0,1.797171711921692,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Alto_Jequitiba,2025-12-31T00:00:00,0.0,1.025197148323059,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Arapua,2025-12-31T00:00:00,0.0,1.058391809463501,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Araxa,2025-12-31T00:00:00,0.0,1.3924278020858765,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Astolfo_Dutra,2025-12-31T00:00:00,0.0,0.7273032069206238,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Bom_Jesus_da_Penha,2025-12-31T00:00:00,0.0,1.4718384742736816,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botelhos,2025-12-31T00:00:00,0.0,1.0394855737686157,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Botumirim,2025-12-31T00:00:00,0.0,0.6926424503326416,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cabo_Verde,2025-12-31T00:00:00,0.0,1.3327136039733887,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Caldas,2025-12-31T00:00:00,0.0,1.521661639213562,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Conceicao_da_Aparecida,2025-12-31T00:00:00,0.0,1.5802539587020874,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Congonhal,2025-12-31T00:00:00,0.0,1.4230847358703613,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Coromandel,2025-12-31T00:00:00,0.0,1.7005791664123535,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Corrego_Fundo,2025-12-31T00:00:00,0.0,1.3390597105026245,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Cruzeiro_da_Fortaleza,2025-12-31T00:00:00,0.0,1.8157626390457153,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Dom_Cavati,2025-12-31T00:00:00,0.0,1.6206426620483398,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaranesia,2025-12-31T00:00:00,0.0,1.1683070659637451,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarani,2025-12-31T00:00:00,0.0,1.0650447607040405,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guarda-Mor,2025-12-31T00:00:00,0.0,1.8083608150482178,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guaxupe,2025-12-31T00:00:00,0.0,1.2268192768096924,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Guimarania,2025-12-31T00:00:00,0.0,1.653617262840271,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ibitiura_de_Minas,2025-12-31T00:00:00,0.0,1.3219208717346191,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Indaiabira,2025-12-31T00:00:00,0.0,2.257848024368286,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itabirinha,2025-12-31T00:00:00,0.0,0.9597426652908325,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Itambacuri,2025-12-31T00:00:00,0.0,1.0788503885269165,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Jacui,2025-12-31T00:00:00,0.0,1.2736198902130127,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juiz_de_Fora,2025-12-31T00:00:00,0.0,1.1816470623016357,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Juruaia,2025-12-31T00:00:00,0.0,1.5705807209014893,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Ladainha,2025-12-31T00:00:00,0.0,1.0634093284606934,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Lagoa_Dourada,2025-12-31T00:00:00,0.0,1.6286824941635132,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Martins_Soares,2025-12-31T00:00:00,0.0,1.2879191637039185,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Miradouro,2025-12-31T00:00:00,0.0,1.0973670482635498,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Monte_Belo,2025-12-31T00:00:00,0.0,1.3098870515823364,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Nova_Resende,2025-12-31T00:00:00,0.0,1.4682101011276245,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Patos_de_Minas,2025-12-31T00:00:00,0.0,1.791952133178711,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Piracema,2025-12-31T00:00:00,0.0,1.032012939453125,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Pratapolis,2025-12-31T00:00:00,0.0,1.42364501953125,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Presidente_Kubitschek,2025-12-31T00:00:00,0.0,0.8589496612548828,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Rio_Vermelho,2025-12-31T00:00:00,0.0,0.9158118367195129,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Domingos_das_Dores,2025-12-31T00:00:00,0.0,1.3282158374786377,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Goncalo_do_Abaete,2025-12-31T00:00:00,0.0,1.5530669689178467,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_Batista_do_Gloria,2025-12-31T00:00:00,0.0,1.4492251873016357,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Joao_da_Mata,2025-12-31T00:00:00,0.0,1.1427781581878662,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Sao_Pedro_da_Uniao,2025-12-31T00:00:00,0.0,1.6902731657028198,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Tapirai,2025-12-31T00:00:00,0.0,1.6284925937652588,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Unai,2025-12-31T00:00:00,0.0,2.47377872467041,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varjao_de_Minas,2025-12-31T00:00:00,0.0,2.5124189853668213,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 2 (2020-225),Varzea_da_Palma,2025-12-31T00:00:00,0.0,2.9900732040405273,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.00011752302748122787
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 128
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 400
",2025-09-23T22:58:25
cluster 3 (2020-225),Aguanil,2012-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aguanil,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aguanil,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aguanil,2015-12-31T00:00:00,0.9,1.0248165130615234,13.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aguanil,2016-12-31T00:00:00,1.8,1.7984861135482788,0.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aguanil,2017-12-31T00:00:00,1.85,1.8936604261398315,2.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aguanil,2018-12-31T00:00:00,2.1,1.9964144229888916,4.93,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aiuruoca,2012-12-31T00:00:00,1.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aiuruoca,2013-12-31T00:00:00,1.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aiuruoca,2014-12-31T00:00:00,1.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aiuruoca,2015-12-31T00:00:00,1.09,1.1299710273742676,3.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aiuruoca,2016-12-31T00:00:00,1.19,1.170816421508789,1.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aiuruoca,2017-12-31T00:00:00,1.5,1.1761928796768188,21.59,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aiuruoca,2018-12-31T00:00:00,1.77,1.5854040384292603,10.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Albertina,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Albertina,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Albertina,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Albertina,2015-12-31T00:00:00,1.08,1.103759765625,2.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Albertina,2016-12-31T00:00:00,1.2,1.1603015661239624,3.31,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Albertina,2017-12-31T00:00:00,1.83,1.1504395008087158,37.13,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Albertina,2018-12-31T00:00:00,1.56,2.289083480834961,46.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alfenas,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alfenas,2013-12-31T00:00:00,1.98,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alfenas,2014-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alfenas,2015-12-31T00:00:00,1.38,1.5027081966400146,8.89,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alfenas,2016-12-31T00:00:00,2.25,2.2977664470672607,2.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alfenas,2017-12-31T00:00:00,1.68,1.7828478813171387,6.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alfenas,2018-12-31T00:00:00,2.34,2.0818424224853516,11.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alterosa,2012-12-31T00:00:00,1.59,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alterosa,2013-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alterosa,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alterosa,2015-12-31T00:00:00,1.08,1.2707931995391846,17.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alterosa,2016-12-31T00:00:00,1.92,1.924396276473999,0.23,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alterosa,2017-12-31T00:00:00,1.8,2.0980935096740723,16.56,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alterosa,2018-12-31T00:00:00,1.96,2.013228416442871,2.72,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andradas,2012-12-31T00:00:00,1.98,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andradas,2013-12-31T00:00:00,1.59,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andradas,2014-12-31T00:00:00,1.21,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andradas,2015-12-31T00:00:00,1.89,1.7015326023101807,9.97,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andradas,2016-12-31T00:00:00,1.8,1.8676778078079224,3.76,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andradas,2017-12-31T00:00:00,1.34,1.6327049732208252,21.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andradas,2018-12-31T00:00:00,1.73,1.8809854984283447,8.73,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andrelandia,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andrelandia,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andrelandia,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andrelandia,2015-12-31T00:00:00,1.2,0.9010297060012817,24.91,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andrelandia,2016-12-31T00:00:00,1.5,1.7487881183624268,16.59,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andrelandia,2017-12-31T00:00:00,1.4,1.422594428062439,1.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andrelandia,2018-12-31T00:00:00,1.5,1.4947292804718018,0.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Arceburgo,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Arceburgo,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Arceburgo,2014-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Arceburgo,2015-12-31T00:00:00,1.32,1.3622169494628906,3.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Arceburgo,2016-12-31T00:00:00,1.8,1.7981805801391602,0.1,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Arceburgo,2017-12-31T00:00:00,1.02,1.604624629020691,57.32,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Arceburgo,2018-12-31T00:00:00,1.72,1.785456895828247,3.81,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Baependi,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Baependi,2013-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Baependi,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Baependi,2015-12-31T00:00:00,0.84,1.1767834424972534,40.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Baependi,2016-12-31T00:00:00,1.8,1.8086906671524048,0.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Baependi,2017-12-31T00:00:00,1.56,2.101011276245117,34.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Baependi,2018-12-31T00:00:00,1.56,1.8631404638290405,19.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bandeira_do_Sul,2012-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bandeira_do_Sul,2013-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bandeira_do_Sul,2014-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bandeira_do_Sul,2015-12-31T00:00:00,1.26,1.3806439638137817,9.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bandeira_do_Sul,2016-12-31T00:00:00,1.5,1.4005681276321411,6.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bandeira_do_Sul,2017-12-31T00:00:00,1.76,1.4947733879089355,15.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bandeira_do_Sul,2018-12-31T00:00:00,1.8,1.632692813873291,9.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Boa_Esperanca,2012-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Boa_Esperanca,2013-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Boa_Esperanca,2014-12-31T00:00:00,1.15,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Boa_Esperanca,2015-12-31T00:00:00,1.26,1.2712591886520386,0.89,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Boa_Esperanca,2016-12-31T00:00:00,1.92,1.9711493253707886,2.66,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Boa_Esperanca,2017-12-31T00:00:00,2.53,1.9020756483078003,24.82,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Boa_Esperanca,2018-12-31T00:00:00,1.8,2.4989676475524902,38.83,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bom_Sucesso,2012-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bom_Sucesso,2013-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bom_Sucesso,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bom_Sucesso,2015-12-31T00:00:00,1.14,1.2484519481658936,9.51,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bom_Sucesso,2016-12-31T00:00:00,1.92,1.4435667991638184,24.81,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bom_Sucesso,2017-12-31T00:00:00,1.48,1.8966102600097656,28.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bom_Sucesso,2018-12-31T00:00:00,1.7,1.7275149822235107,1.62,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Borda_da_Mata,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Borda_da_Mata,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Borda_da_Mata,2014-12-31T00:00:00,0.78,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Borda_da_Mata,2015-12-31T00:00:00,1.2,1.0150240659713745,15.41,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Borda_da_Mata,2016-12-31T00:00:00,1.8,1.5184303522109985,15.64,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Borda_da_Mata,2017-12-31T00:00:00,1.67,1.4383958578109741,13.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Borda_da_Mata,2018-12-31T00:00:00,1.8,1.7900044918060303,0.56,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Brazopolis,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Brazopolis,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Brazopolis,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Brazopolis,2015-12-31T00:00:00,1.08,1.2111151218414307,12.14,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Brazopolis,2016-12-31T00:00:00,1.44,1.4500699043273926,0.7,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Brazopolis,2017-12-31T00:00:00,0.63,1.3050588369369507,107.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Brazopolis,2018-12-31T00:00:00,1.56,1.187174916267395,23.9,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bueno_Brandao,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bueno_Brandao,2013-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bueno_Brandao,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bueno_Brandao,2015-12-31T00:00:00,1.26,1.2044984102249146,4.4,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bueno_Brandao,2016-12-31T00:00:00,2.1,1.8012434244155884,14.23,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bueno_Brandao,2017-12-31T00:00:00,1.2,1.3872175216674805,15.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bueno_Brandao,2018-12-31T00:00:00,1.8,1.8730313777923584,4.06,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cachoeira_de_Minas,2012-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cachoeira_de_Minas,2013-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cachoeira_de_Minas,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cachoeira_de_Minas,2015-12-31T00:00:00,1.02,1.2567452192306519,23.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cachoeira_de_Minas,2016-12-31T00:00:00,1.56,1.536125898361206,1.53,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cachoeira_de_Minas,2017-12-31T00:00:00,2.28,1.6882174015045166,25.96,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cachoeira_de_Minas,2018-12-31T00:00:00,2.2,1.9608852863311768,10.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Caete,2012-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Caete,2013-12-31T00:00:00,0.93,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Caete,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Caete,2015-12-31T00:00:00,1.08,1.0625364780426025,1.62,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Caete,2016-12-31T00:00:00,1.2,1.2569408416748047,4.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Caete,2017-12-31T00:00:00,1.53,1.7780689001083374,16.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Caete,2018-12-31T00:00:00,1.5,1.3399627208709717,10.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Camacho,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Camacho,2013-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Camacho,2014-12-31T00:00:00,0.67,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Camacho,2015-12-31T00:00:00,1.02,0.9830990433692932,3.62,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Camacho,2016-12-31T00:00:00,1.5,2.0415329933166504,36.1,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Camacho,2017-12-31T00:00:00,1.73,1.434812068939209,17.06,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Camacho,2018-12-31T00:00:00,1.8,1.7563493251800537,2.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cambuquira,2012-12-31T00:00:00,1.15,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cambuquira,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cambuquira,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cambuquira,2015-12-31T00:00:00,1.08,1.1054729223251343,2.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cambuquira,2016-12-31T00:00:00,1.68,1.4785417318344116,11.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cambuquira,2017-12-31T00:00:00,1.82,1.8971047401428223,4.24,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cambuquira,2018-12-31T00:00:00,1.74,1.8435271978378296,5.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campanha,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campanha,2013-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campanha,2014-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campanha,2015-12-31T00:00:00,1.02,1.0931651592254639,7.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campanha,2016-12-31T00:00:00,1.5,1.524788498878479,1.65,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campanha,2017-12-31T00:00:00,1.88,1.64697265625,12.4,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campanha,2018-12-31T00:00:00,1.8,1.8669278621673584,3.72,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campestre,2012-12-31T00:00:00,1.35,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campestre,2013-12-31T00:00:00,1.31,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campestre,2014-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campestre,2015-12-31T00:00:00,1.91,1.350178837776184,29.31,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campestre,2016-12-31T00:00:00,1.85,2.1110739707946777,14.11,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campestre,2017-12-31T00:00:00,2.11,1.7967662811279297,14.85,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campestre,2018-12-31T00:00:00,1.54,2.03171968460083,31.93,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_Belo,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_Belo,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_Belo,2014-12-31T00:00:00,0.63,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_Belo,2015-12-31T00:00:00,0.84,0.7536476850509644,10.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_Belo,2016-12-31T00:00:00,1.62,1.3608825206756592,15.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_Belo,2017-12-31T00:00:00,1.32,1.7656545639038086,33.76,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_Belo,2018-12-31T00:00:00,1.26,1.5588488578796387,23.72,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_do_Meio,2012-12-31T00:00:00,1.17,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_do_Meio,2013-12-31T00:00:00,1.17,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_do_Meio,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_do_Meio,2015-12-31T00:00:00,0.9,0.9143021702766418,1.59,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_do_Meio,2016-12-31T00:00:00,1.8,1.9850695133209229,10.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_do_Meio,2017-12-31T00:00:00,2.08,2.155482769012451,3.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_do_Meio,2018-12-31T00:00:00,1.82,2.1956593990325928,20.64,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campos_Gerais,2012-12-31T00:00:00,1.65,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campos_Gerais,2013-12-31T00:00:00,1.22,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campos_Gerais,2014-12-31T00:00:00,1.15,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campos_Gerais,2015-12-31T00:00:00,1.32,1.2125606536865234,8.14,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campos_Gerais,2016-12-31T00:00:00,2.5,1.7040181159973145,31.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campos_Gerais,2017-12-31T00:00:00,1.81,2.49629545211792,37.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campos_Gerais,2018-12-31T00:00:00,2.21,2.342569351196289,6.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Candeias,2012-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Candeias,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Candeias,2014-12-31T00:00:00,0.99,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Candeias,2015-12-31T00:00:00,1.2,1.1870323419570923,1.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Candeias,2016-12-31T00:00:00,2.28,2.177840232849121,4.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Candeias,2017-12-31T00:00:00,1.38,2.1676084995269775,57.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Candeias,2018-12-31T00:00:00,2.04,1.8545737266540527,9.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capetinga,2012-12-31T00:00:00,1.81,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capetinga,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capetinga,2014-12-31T00:00:00,1.59,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capetinga,2015-12-31T00:00:00,1.2,1.516413927078247,26.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capetinga,2016-12-31T00:00:00,1.93,1.891004204750061,2.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capetinga,2017-12-31T00:00:00,2.09,1.8120770454406738,13.3,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capetinga,2018-12-31T00:00:00,2.7,2.5164613723754883,6.8,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capitolio,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capitolio,2013-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capitolio,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capitolio,2015-12-31T00:00:00,1.2,1.2563353776931763,4.69,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capitolio,2016-12-31T00:00:00,1.32,1.375899314880371,4.23,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capitolio,2017-12-31T00:00:00,1.47,1.351416826248169,8.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capitolio,2018-12-31T00:00:00,1.8,1.5562788248062134,13.54,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_da_Cachoeira,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_da_Cachoeira,2013-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_da_Cachoeira,2014-12-31T00:00:00,1.15,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_da_Cachoeira,2015-12-31T00:00:00,1.26,1.2128760814666748,3.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_da_Cachoeira,2016-12-31T00:00:00,1.44,1.7263251543045044,19.88,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_da_Cachoeira,2017-12-31T00:00:00,1.73,1.5641200542449951,9.59,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_da_Cachoeira,2018-12-31T00:00:00,1.98,1.7980375289916992,9.19,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_de_Minas,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_de_Minas,2013-12-31T00:00:00,1.74,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_de_Minas,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_de_Minas,2015-12-31T00:00:00,1.32,1.5128939151763916,14.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_de_Minas,2016-12-31T00:00:00,1.5,1.7879537343978882,19.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_de_Minas,2017-12-31T00:00:00,1.38,1.6223779916763306,17.56,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_de_Minas,2018-12-31T00:00:00,1.56,1.5065388679504395,3.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_do_Rio_Claro,2012-12-31T00:00:00,1.66,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_do_Rio_Claro,2013-12-31T00:00:00,1.94,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_do_Rio_Claro,2014-12-31T00:00:00,1.65,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_do_Rio_Claro,2015-12-31T00:00:00,1.53,1.5162231922149658,0.9,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_do_Rio_Claro,2016-12-31T00:00:00,2.37,2.2452197074890137,5.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_do_Rio_Claro,2017-12-31T00:00:00,1.82,2.160217761993408,18.69,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_do_Rio_Claro,2018-12-31T00:00:00,2.22,2.3535842895507812,6.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carvalhopolis,2012-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carvalhopolis,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carvalhopolis,2014-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carvalhopolis,2015-12-31T00:00:00,1.44,1.826041579246521,26.81,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carvalhopolis,2016-12-31T00:00:00,1.8,1.8564770221710205,3.14,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carvalhopolis,2017-12-31T00:00:00,1.44,1.5909289121627808,10.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carvalhopolis,2018-12-31T00:00:00,1.56,1.8176157474517822,16.51,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cassia,2012-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cassia,2013-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cassia,2014-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cassia,2015-12-31T00:00:00,1.14,1.3995070457458496,22.76,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cassia,2016-12-31T00:00:00,1.56,1.5607470273971558,0.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cassia,2017-12-31T00:00:00,1.6,1.6667444705963135,4.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cassia,2018-12-31T00:00:00,2.1,2.3891661167144775,13.77,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_da_Barra_de_Minas,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_da_Barra_de_Minas,2013-12-31T00:00:00,1.46,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_da_Barra_de_Minas,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_da_Barra_de_Minas,2015-12-31T00:00:00,1.2,1.5262491703033447,27.19,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_da_Barra_de_Minas,2016-12-31T00:00:00,2.1,1.7043633460998535,18.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_da_Barra_de_Minas,2017-12-31T00:00:00,2.63,2.3326194286346436,11.31,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_da_Barra_de_Minas,2018-12-31T00:00:00,2.08,2.548588514328003,22.53,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_das_Pedras,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_das_Pedras,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_das_Pedras,2014-12-31T00:00:00,1.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_das_Pedras,2015-12-31T00:00:00,1.26,1.2040233612060547,4.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_das_Pedras,2016-12-31T00:00:00,1.44,1.577470302581787,9.55,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_das_Pedras,2017-12-31T00:00:00,1.47,1.4223942756652832,3.24,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_das_Pedras,2018-12-31T00:00:00,1.56,1.4858851432800293,4.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_do_Rio_Verde,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_do_Rio_Verde,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_do_Rio_Verde,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_do_Rio_Verde,2015-12-31T00:00:00,1.62,1.1600284576416016,28.39,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_do_Rio_Verde,2016-12-31T00:00:00,1.62,2.359201431274414,45.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_do_Rio_Verde,2017-12-31T00:00:00,2.04,1.7369904518127441,14.85,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_do_Rio_Verde,2018-12-31T00:00:00,1.98,2.0265982151031494,2.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_dos_Ouros,2012-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_dos_Ouros,2013-12-31T00:00:00,1.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_dos_Ouros,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_dos_Ouros,2015-12-31T00:00:00,1.32,1.3183811902999878,0.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_dos_Ouros,2016-12-31T00:00:00,1.5,1.6434836387634277,9.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_dos_Ouros,2017-12-31T00:00:00,1.52,1.4319648742675781,5.79,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_dos_Ouros,2018-12-31T00:00:00,1.5,1.4800026416778564,1.33,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Coqueiral,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Coqueiral,2013-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Coqueiral,2014-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Coqueiral,2015-12-31T00:00:00,1.26,1.1384387016296387,9.65,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Coqueiral,2016-12-31T00:00:00,1.68,1.5456162691116333,8.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Coqueiral,2017-12-31T00:00:00,1.75,1.703594446182251,2.65,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Coqueiral,2018-12-31T00:00:00,1.8,1.8109837770462036,0.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cordislandia,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cordislandia,2013-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cordislandia,2014-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cordislandia,2015-12-31T00:00:00,1.26,1.0299468040466309,18.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cordislandia,2016-12-31T00:00:00,1.62,1.7008718252182007,4.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cordislandia,2017-12-31T00:00:00,2.1,1.71817946434021,18.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cordislandia,2018-12-31T00:00:00,1.86,1.9836393594741821,6.65,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Corrego_Danta,2012-12-31T00:00:00,1.25,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Corrego_Danta,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Corrego_Danta,2014-12-31T00:00:00,1.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Corrego_Danta,2015-12-31T00:00:00,1.32,1.1977293491363525,9.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Corrego_Danta,2016-12-31T00:00:00,1.5,1.4806926250457764,1.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Corrego_Danta,2017-12-31T00:00:00,1.44,1.8776192665100098,30.39,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Corrego_Danta,2018-12-31T00:00:00,1.8,1.7123757600784302,4.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristais,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristais,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristais,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristais,2015-12-31T00:00:00,1.2,1.2501637935638428,4.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristais,2016-12-31T00:00:00,1.92,1.8908870220184326,1.52,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristais,2017-12-31T00:00:00,2.4,2.058312177658081,14.24,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristais,2018-12-31T00:00:00,1.92,2.443671464920044,27.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristina,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristina,2013-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristina,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristina,2015-12-31T00:00:00,1.08,1.1430892944335938,5.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristina,2016-12-31T00:00:00,1.44,1.4239422082901,1.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristina,2017-12-31T00:00:00,1.62,1.4694693088531494,9.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristina,2018-12-31T00:00:00,1.56,1.577315330505371,1.11,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Delfinopolis,2012-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Delfinopolis,2013-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Delfinopolis,2014-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Delfinopolis,2015-12-31T00:00:00,1.32,1.352125883102417,2.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Delfinopolis,2016-12-31T00:00:00,1.5,1.5538743734359741,3.59,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Delfinopolis,2017-12-31T00:00:00,1.24,1.596548318862915,28.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Delfinopolis,2018-12-31T00:00:00,1.8,1.7376420497894287,3.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Desterro_de_Entre_Rios,2012-12-31T00:00:00,0.57,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Desterro_de_Entre_Rios,2013-12-31T00:00:00,0.57,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Desterro_de_Entre_Rios,2014-12-31T00:00:00,3.28,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Desterro_de_Entre_Rios,2015-12-31T00:00:00,1.51,1.2237181663513184,18.96,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Desterro_de_Entre_Rios,2016-12-31T00:00:00,2.28,2.0337772369384766,10.8,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Desterro_de_Entre_Rios,2017-12-31T00:00:00,2.08,4.2637104988098145,104.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Desterro_de_Entre_Rios,2018-12-31T00:00:00,2.1,2.1433072090148926,2.06,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divinesia,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divinesia,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divinesia,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divinesia,2015-12-31T00:00:00,1.5,1.8635821342468262,24.24,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divinesia,2016-12-31T00:00:00,1.8,1.831261157989502,1.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divinesia,2017-12-31T00:00:00,2.04,1.997543454170227,2.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divinesia,2018-12-31T00:00:00,4.0,2.0428829193115234,48.93,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divisa_Nova,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divisa_Nova,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divisa_Nova,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divisa_Nova,2015-12-31T00:00:00,1.08,1.423264980316162,31.78,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divisa_Nova,2016-12-31T00:00:00,1.68,1.576099157333374,6.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divisa_Nova,2017-12-31T00:00:00,2.28,1.9204237461090088,15.77,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divisa_Nova,2018-12-31T00:00:00,1.8,2.1244945526123047,18.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Dom_Vicoso,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Dom_Vicoso,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Dom_Vicoso,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Dom_Vicoso,2015-12-31T00:00:00,1.32,1.2012568712234497,9.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Dom_Vicoso,2016-12-31T00:00:00,1.5,1.3750014305114746,8.33,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Dom_Vicoso,2017-12-31T00:00:00,1.16,1.518228530883789,30.88,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Dom_Vicoso,2018-12-31T00:00:00,1.5,1.485435962677002,0.97,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Doresopolis,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Doresopolis,2013-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Doresopolis,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Doresopolis,2015-12-31T00:00:00,1.2,1.1536064147949219,3.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Doresopolis,2016-12-31T00:00:00,1.26,1.2436373233795166,1.3,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Doresopolis,2017-12-31T00:00:00,1.43,1.3191118240356445,7.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Doresopolis,2018-12-31T00:00:00,1.8,1.696507453918457,5.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Eloi_Mendes,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Eloi_Mendes,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Eloi_Mendes,2014-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Eloi_Mendes,2015-12-31T00:00:00,1.26,1.103472352027893,12.42,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Eloi_Mendes,2016-12-31T00:00:00,1.74,1.7513750791549683,0.65,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Eloi_Mendes,2017-12-31T00:00:00,1.83,1.8523242473602295,1.22,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Eloi_Mendes,2018-12-31T00:00:00,1.72,1.8955752849578857,10.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Entre_Rios_de_Minas,2012-12-31T00:00:00,1.58,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Entre_Rios_de_Minas,2013-12-31T00:00:00,1.58,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Entre_Rios_de_Minas,2014-12-31T00:00:00,1.58,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Entre_Rios_de_Minas,2015-12-31T00:00:00,1.63,1.5806529521942139,3.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Entre_Rios_de_Minas,2016-12-31T00:00:00,2.4,1.668630838394165,30.47,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Entre_Rios_de_Minas,2017-12-31T00:00:00,2.28,2.370673894882202,3.98,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Entre_Rios_de_Minas,2018-12-31T00:00:00,2.1,2.198162078857422,4.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Fama,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Fama,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Fama,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Fama,2015-12-31T00:00:00,1.08,1.1358706951141357,5.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Fama,2016-12-31T00:00:00,1.68,1.394695520401001,16.98,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Fama,2017-12-31T00:00:00,1.32,2.0074000358581543,52.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Fama,2018-12-31T00:00:00,1.51,1.6962854862213135,12.34,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Grao_Mogol,2012-12-31T00:00:00,0.3,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Grao_Mogol,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Grao_Mogol,2014-12-31T00:00:00,0.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Grao_Mogol,2015-12-31T00:00:00,0.9,0.7705363035202026,14.38,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Grao_Mogol,2016-12-31T00:00:00,0.6,0.7810238003730774,30.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Grao_Mogol,2017-12-31T00:00:00,3.5,1.4415172338485718,58.81,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Grao_Mogol,2018-12-31T00:00:00,1.0,1.6591765880584717,65.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Guape,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Guape,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Guape,2014-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Guape,2015-12-31T00:00:00,1.8,1.3273431062698364,26.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Guape,2016-12-31T00:00:00,1.92,2.0896685123443604,8.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Guape,2017-12-31T00:00:00,1.92,2.002685308456421,4.31,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Guape,2018-12-31T00:00:00,2.1,1.9660274982452393,6.38,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Heliodora,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Heliodora,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Heliodora,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Heliodora,2015-12-31T00:00:00,1.2,1.0883727073669434,9.3,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Heliodora,2016-12-31T00:00:00,1.5,1.5556755065917969,3.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Heliodora,2017-12-31T00:00:00,1.57,1.366499900817871,12.96,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Heliodora,2018-12-31T00:00:00,1.8,1.5792288780212402,12.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ibituruna,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ibituruna,2013-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ibituruna,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ibituruna,2015-12-31T00:00:00,1.2,1.2041476964950562,0.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ibituruna,2016-12-31T00:00:00,1.74,1.4627765417099,15.93,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ibituruna,2017-12-31T00:00:00,1.62,1.7140586376190186,5.81,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ibituruna,2018-12-31T00:00:00,1.66,1.717380166053772,3.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ijaci,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ijaci,2013-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ijaci,2014-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ijaci,2015-12-31T00:00:00,0.84,1.089347004890442,29.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ijaci,2016-12-31T00:00:00,1.8,1.8503327369689941,2.8,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ijaci,2017-12-31T00:00:00,1.25,1.7040703296661377,36.33,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ijaci,2018-12-31T00:00:00,2.0,1.8002638816833496,9.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ilicinea,2012-12-31T00:00:00,1.69,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ilicinea,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ilicinea,2014-12-31T00:00:00,1.35,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ilicinea,2015-12-31T00:00:00,1.5,1.4548144340515137,3.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ilicinea,2016-12-31T00:00:00,2.46,1.7355425357818604,29.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ilicinea,2017-12-31T00:00:00,1.92,2.213843584060669,15.3,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ilicinea,2018-12-31T00:00:00,2.46,2.261579990386963,8.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Inconfidentes,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Inconfidentes,2013-12-31T00:00:00,1.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Inconfidentes,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Inconfidentes,2015-12-31T00:00:00,1.2,1.0895105600357056,9.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Inconfidentes,2016-12-31T00:00:00,2.1,2.114046096801758,0.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Inconfidentes,2017-12-31T00:00:00,1.3,1.68611741065979,29.7,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Inconfidentes,2018-12-31T00:00:00,2.88,2.1227359771728516,26.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ingai,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ingai,2013-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ingai,2014-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ingai,2015-12-31T00:00:00,1.2,1.2897322177886963,7.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ingai,2016-12-31T00:00:00,1.74,1.9826195240020752,13.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ingai,2017-12-31T00:00:00,2.22,1.6294374465942383,26.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ingai,2018-12-31T00:00:00,1.8,2.1860268115997314,21.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itajuba,2012-12-31T00:00:00,0.91,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itajuba,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itajuba,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itajuba,2015-12-31T00:00:00,1.5,1.480924129486084,1.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itajuba,2016-12-31T00:00:00,1.8,1.9475282430648804,8.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itajuba,2017-12-31T00:00:00,2.33,2.0228590965270996,13.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itajuba,2018-12-31T00:00:00,1.82,2.109816312789917,15.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itamogi,2012-12-31T00:00:00,1.95,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itamogi,2013-12-31T00:00:00,1.17,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itamogi,2014-12-31T00:00:00,1.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itamogi,2015-12-31T00:00:00,1.58,1.304532766342163,17.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itamogi,2016-12-31T00:00:00,1.95,2.0410685539245605,4.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itamogi,2017-12-31T00:00:00,1.42,1.664020299911499,17.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itamogi,2018-12-31T00:00:00,2.1,2.1897311210632324,4.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itapecerica,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itapecerica,2013-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itapecerica,2014-12-31T00:00:00,0.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itapecerica,2015-12-31T00:00:00,1.2,0.9006156921386719,24.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itapecerica,2016-12-31T00:00:00,1.5,1.6103599071502686,7.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itapecerica,2017-12-31T00:00:00,1.56,1.506556749343872,3.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itapecerica,2018-12-31T00:00:00,1.8,1.6913769245147705,6.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itumirim,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itumirim,2013-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itumirim,2014-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itumirim,2015-12-31T00:00:00,1.08,1.0533874034881592,2.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itumirim,2016-12-31T00:00:00,1.62,1.625127911567688,0.32,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itumirim,2017-12-31T00:00:00,1.32,1.498643398284912,13.53,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itumirim,2018-12-31T00:00:00,1.8,1.5204954147338867,15.53,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itutinga,2012-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itutinga,2013-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itutinga,2014-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itutinga,2015-12-31T00:00:00,1.26,1.3314975500106812,5.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itutinga,2016-12-31T00:00:00,1.68,1.417698621749878,15.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itutinga,2017-12-31T00:00:00,1.49,1.6676064729690552,11.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itutinga,2018-12-31T00:00:00,1.52,1.591923475265503,4.73,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jacutinga,2012-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jacutinga,2013-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jacutinga,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jacutinga,2015-12-31T00:00:00,1.32,1.2865818738937378,2.53,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jacutinga,2016-12-31T00:00:00,1.5,1.5020029544830322,0.13,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jacutinga,2017-12-31T00:00:00,1.37,1.290718913078308,5.79,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jacutinga,2018-12-31T00:00:00,1.74,1.6524064540863037,5.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jesuania,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jesuania,2013-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jesuania,2014-12-31T00:00:00,1.05,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jesuania,2015-12-31T00:00:00,0.96,1.092040777206421,13.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jesuania,2016-12-31T00:00:00,1.56,1.4055622816085815,9.9,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jesuania,2017-12-31T00:00:00,1.52,1.653979778289795,8.81,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jesuania,2018-12-31T00:00:00,1.68,1.6960649490356445,0.96,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lambari,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lambari,2013-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lambari,2014-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lambari,2015-12-31T00:00:00,1.08,1.0052164793014526,6.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lambari,2016-12-31T00:00:00,1.5,1.4912502765655518,0.58,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lambari,2017-12-31T00:00:00,1.44,1.6648958921432495,15.62,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lambari,2018-12-31T00:00:00,1.56,1.5248754024505615,2.25,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lavras,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lavras,2013-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lavras,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lavras,2015-12-31T00:00:00,0.84,1.0092244148254395,20.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lavras,2016-12-31T00:00:00,1.68,2.188931703567505,30.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lavras,2017-12-31T00:00:00,2.25,1.6170170307159424,28.13,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lavras,2018-12-31T00:00:00,1.92,2.581397294998169,34.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Luminarias,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Luminarias,2013-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Luminarias,2014-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Luminarias,2015-12-31T00:00:00,0.84,1.024124026298523,21.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Luminarias,2016-12-31T00:00:00,1.86,1.4659990072250366,21.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Luminarias,2017-12-31T00:00:00,2.33,1.6411700248718262,29.56,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Luminarias,2018-12-31T00:00:00,2.0,2.382007122039795,19.1,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Machado,2012-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Machado,2013-12-31T00:00:00,1.62,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Machado,2014-12-31T00:00:00,1.15,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Machado,2015-12-31T00:00:00,1.32,1.4033387899398804,6.31,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Machado,2016-12-31T00:00:00,1.86,1.7967215776443481,3.4,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Machado,2017-12-31T00:00:00,1.51,1.4854377508163452,1.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Machado,2018-12-31T00:00:00,1.98,2.1983323097229004,11.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Maria_da_Fe,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Maria_da_Fe,2013-12-31T00:00:00,0.89,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Maria_da_Fe,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Maria_da_Fe,2015-12-31T00:00:00,1.51,1.1860339641571045,21.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Maria_da_Fe,2016-12-31T00:00:00,1.91,2.373025417327881,24.24,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Maria_da_Fe,2017-12-31T00:00:00,1.51,1.9059501886367798,26.22,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Maria_da_Fe,2018-12-31T00:00:00,1.5,1.7709565162658691,18.06,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Moeda,2012-12-31T00:00:00,0.77,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Moeda,2013-12-31T00:00:00,0.77,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Moeda,2014-12-31T00:00:00,0.69,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Moeda,2015-12-31T00:00:00,0.94,0.936511218547821,0.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Moeda,2016-12-31T00:00:00,0.94,1.069701075553894,13.8,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Moeda,2017-12-31T00:00:00,2.0,1.7002508640289307,14.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Moeda,2018-12-31T00:00:00,1.2,1.3595563173294067,13.3,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monsenhor_Paulo,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monsenhor_Paulo,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monsenhor_Paulo,2014-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monsenhor_Paulo,2015-12-31T00:00:00,1.26,1.087620496749878,13.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monsenhor_Paulo,2016-12-31T00:00:00,1.62,1.5623493194580078,3.56,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monsenhor_Paulo,2017-12-31T00:00:00,1.72,1.7606511116027832,2.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monsenhor_Paulo,2018-12-31T00:00:00,1.8,1.7553659677505493,2.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Alegre_de_Minas,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Alegre_de_Minas,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Alegre_de_Minas,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Alegre_de_Minas,2015-12-31T00:00:00,1.2,4.3763651847839355,264.7,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Alegre_de_Minas,2016-12-31T00:00:00,1.5,0.6424295902252197,57.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Alegre_de_Minas,2017-12-31T00:00:00,3.27,1.724205732345581,47.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Alegre_de_Minas,2018-12-31T00:00:00,1.8,2.5273795127868652,40.41,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Formoso,2012-12-31T00:00:00,0.45,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Formoso,2013-12-31T00:00:00,0.45,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Formoso,2014-12-31T00:00:00,0.42,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Formoso,2015-12-31T00:00:00,0.42,0.42526647448539734,1.25,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Formoso,2016-12-31T00:00:00,0.42,0.423946738243103,0.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Formoso,2017-12-31T00:00:00,1.33,172.7125701904297,12885.91,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Formoso,2018-12-31T00:00:00,1.2,0.7281539440155029,39.32,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Siao,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Siao,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Siao,2014-12-31T00:00:00,0.66,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Siao,2015-12-31T00:00:00,1.2,0.7923682928085327,33.97,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Siao,2016-12-31T00:00:00,1.62,1.4700608253479004,9.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Siao,2017-12-31T00:00:00,1.2,1.349480390548706,12.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Siao,2018-12-31T00:00:00,1.8,1.8178871870040894,0.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Muzambinho,2012-12-31T00:00:00,1.74,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Muzambinho,2013-12-31T00:00:00,1.35,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Muzambinho,2014-12-31T00:00:00,1.35,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Muzambinho,2015-12-31T00:00:00,1.48,1.5143747329711914,2.32,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Muzambinho,2016-12-31T00:00:00,1.98,1.752366065979004,11.5,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Muzambinho,2017-12-31T00:00:00,1.62,1.949652075767517,20.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Muzambinho,2018-12-31T00:00:00,1.3,1.8505550622940063,42.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Natercia,2012-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Natercia,2013-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Natercia,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Natercia,2015-12-31T00:00:00,1.38,1.1788091659545898,14.58,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Natercia,2016-12-31T00:00:00,1.56,1.6472723484039307,5.59,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Natercia,2017-12-31T00:00:00,1.6,1.4954261779785156,6.54,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Natercia,2018-12-31T00:00:00,1.8,1.6041874885559082,10.88,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nazareno,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nazareno,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nazareno,2014-12-31T00:00:00,1.3,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nazareno,2015-12-31T00:00:00,1.8,1.7701390981674194,1.66,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nazareno,2016-12-31T00:00:00,1.92,2.435631275177002,26.86,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nazareno,2017-12-31T00:00:00,1.59,2.0523996353149414,29.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nazareno,2018-12-31T00:00:00,1.8,1.8705165386199951,3.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nepomuceno,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nepomuceno,2013-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nepomuceno,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nepomuceno,2015-12-31T00:00:00,1.26,1.1656676530838013,7.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nepomuceno,2016-12-31T00:00:00,1.74,1.9312227964401245,10.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nepomuceno,2017-12-31T00:00:00,1.69,1.6020686626434326,5.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nepomuceno,2018-12-31T00:00:00,1.74,1.8448169231414795,6.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Olimpio_Noronha,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Olimpio_Noronha,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Olimpio_Noronha,2014-12-31T00:00:00,0.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Olimpio_Noronha,2015-12-31T00:00:00,1.2,0.7662678360939026,36.14,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Olimpio_Noronha,2016-12-31T00:00:00,1.5,1.4981330633163452,0.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Olimpio_Noronha,2017-12-31T00:00:00,1.23,1.592112421989441,29.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Olimpio_Noronha,2018-12-31T00:00:00,1.5,1.45936119556427,2.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Oliveira,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Oliveira,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Oliveira,2014-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Oliveira,2015-12-31T00:00:00,1.32,1.3554186820983887,2.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Oliveira,2016-12-31T00:00:00,1.74,1.7647833824157715,1.42,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Oliveira,2017-12-31T00:00:00,1.4,1.8715354204177856,33.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Oliveira,2018-12-31T00:00:00,2.1,2.084582805633545,0.73,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ouro_Fino,2012-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ouro_Fino,2013-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ouro_Fino,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ouro_Fino,2015-12-31T00:00:00,1.29,1.3224438428878784,2.52,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ouro_Fino,2016-12-31T00:00:00,1.44,1.611153483390808,11.89,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ouro_Fino,2017-12-31T00:00:00,1.42,1.3247047662734985,6.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ouro_Fino,2018-12-31T00:00:00,1.69,1.468963623046875,13.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraguacu,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraguacu,2013-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraguacu,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraguacu,2015-12-31T00:00:00,1.2,1.1483839750289917,4.3,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraguacu,2016-12-31T00:00:00,1.98,1.4388370513916016,27.33,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraguacu,2017-12-31T00:00:00,1.56,1.6970535516738892,8.79,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraguacu,2018-12-31T00:00:00,1.98,1.9269481897354126,2.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraisopolis,2012-12-31T00:00:00,1.06,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraisopolis,2013-12-31T00:00:00,1.06,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraisopolis,2014-12-31T00:00:00,1.11,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraisopolis,2015-12-31T00:00:00,1.2,1.1749650239944458,2.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraisopolis,2016-12-31T00:00:00,1.5,1.3699769973754883,8.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraisopolis,2017-12-31T00:00:00,2.1,1.426566481590271,32.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraisopolis,2018-12-31T00:00:00,1.8,1.8348380327224731,1.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pedralva,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pedralva,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pedralva,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pedralva,2015-12-31T00:00:00,0.96,1.4719645977020264,53.33,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pedralva,2016-12-31T00:00:00,2.28,1.7753520011901855,22.13,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pedralva,2017-12-31T00:00:00,1.43,2.514240026473999,75.82,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pedralva,2018-12-31T00:00:00,1.8,1.9373879432678223,7.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Perdoes,2012-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Perdoes,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Perdoes,2014-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Perdoes,2015-12-31T00:00:00,1.14,1.1855909824371338,4.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Perdoes,2016-12-31T00:00:00,1.68,1.6997098922729492,1.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Perdoes,2017-12-31T00:00:00,1.84,1.6353442668914795,11.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Perdoes,2018-12-31T00:00:00,1.8,1.8996449708938599,5.54,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranga,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranga,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranga,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranga,2015-12-31T00:00:00,1.32,3.1101362705230713,135.62,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranga,2016-12-31T00:00:00,1.5,1.5438838005065918,2.93,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranga,2017-12-31T00:00:00,2.5,1.899911880493164,24.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranga,2018-12-31T00:00:00,1.8,2.2384793758392334,24.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pirangucu,2012-12-31T00:00:00,1.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pirangucu,2013-12-31T00:00:00,1.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pirangucu,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pirangucu,2015-12-31T00:00:00,1.32,1.2916425466537476,2.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pirangucu,2016-12-31T00:00:00,1.5,1.4139316082000732,5.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pirangucu,2017-12-31T00:00:00,1.2,1.4169530868530273,18.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pirangucu,2018-12-31T00:00:00,1.17,1.2426098585128784,6.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranguinho,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranguinho,2013-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranguinho,2014-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranguinho,2015-12-31T00:00:00,1.26,1.32023024559021,4.78,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranguinho,2016-12-31T00:00:00,1.5,1.5761044025421143,5.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranguinho,2017-12-31T00:00:00,1.41,1.4912984371185303,5.77,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranguinho,2018-12-31T00:00:00,1.5,1.4633562564849854,2.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Poco_Fundo,2012-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Poco_Fundo,2013-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Poco_Fundo,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Poco_Fundo,2015-12-31T00:00:00,1.2,1.0726183652877808,10.62,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Poco_Fundo,2016-12-31T00:00:00,1.41,1.4032886028289795,0.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Poco_Fundo,2017-12-31T00:00:00,1.2,1.1911230087280273,0.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Poco_Fundo,2018-12-31T00:00:00,1.8,1.4250566959381104,20.83,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pocos_de_Caldas,2012-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pocos_de_Caldas,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pocos_de_Caldas,2014-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pocos_de_Caldas,2015-12-31T00:00:00,1.5,1.483123540878296,1.13,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pocos_de_Caldas,2016-12-31T00:00:00,1.56,1.5442194938659668,1.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pocos_de_Caldas,2017-12-31T00:00:00,1.77,1.6311545372009277,7.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pocos_de_Caldas,2018-12-31T00:00:00,1.8,1.6945703029632568,5.86,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alegre,2012-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alegre,2013-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alegre,2014-12-31T00:00:00,0.78,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alegre,2015-12-31T00:00:00,0.72,0.9798622131347656,36.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alegre,2016-12-31T00:00:00,1.49,1.6743550300598145,12.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alegre,2017-12-31T00:00:00,1.41,1.3350961208343506,5.31,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alegre,2018-12-31T00:00:00,1.51,1.5509932041168213,2.71,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alto,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alto,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alto,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alto,2015-12-31T00:00:00,1.32,3.2721621990203857,147.89,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alto,2016-12-31T00:00:00,1.5,1.474236011505127,1.72,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alto,2017-12-31T00:00:00,1.5,1.5226389169692993,1.51,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alto,2018-12-31T00:00:00,1.65,1.5978045463562012,3.16,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Presidente_Bernardes,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Presidente_Bernardes,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Presidente_Bernardes,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Presidente_Bernardes,2015-12-31T00:00:00,1.26,1.2268365621566772,2.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Presidente_Bernardes,2016-12-31T00:00:00,1.5,1.4868278503417969,0.88,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Presidente_Bernardes,2017-12-31T00:00:00,1.2,1.6891661882400513,40.76,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Presidente_Bernardes,2018-12-31T00:00:00,1.5,1.5968289375305176,6.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ribeirao_Vermelho,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ribeirao_Vermelho,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ribeirao_Vermelho,2014-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ribeirao_Vermelho,2015-12-31T00:00:00,1.14,0.9859321117401123,13.51,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ribeirao_Vermelho,2016-12-31T00:00:00,1.62,1.9529004096984863,20.55,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ribeirao_Vermelho,2017-12-31T00:00:00,1.51,1.5897654294967651,5.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ribeirao_Vermelho,2018-12-31T00:00:00,1.8,1.6567440032958984,7.96,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Barbara_do_Monte_Verde,2012-12-31T00:00:00,0.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Barbara_do_Monte_Verde,2013-12-31T00:00:00,0.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Barbara_do_Monte_Verde,2014-12-31T00:00:00,0.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Barbara_do_Monte_Verde,2015-12-31T00:00:00,1.33,1.6365530490875244,23.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Barbara_do_Monte_Verde,2016-12-31T00:00:00,3.0,2.815430164337158,6.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Barbara_do_Monte_Verde,2017-12-31T00:00:00,0.67,1.9397069215774536,189.51,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Barbara_do_Monte_Verde,2018-12-31T00:00:00,1.0,1.6642096042633057,66.42,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Rita_do_Sapucai,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Rita_do_Sapucai,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Rita_do_Sapucai,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Rita_do_Sapucai,2015-12-31T00:00:00,1.2,1.1122710704803467,7.31,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Rita_do_Sapucai,2016-12-31T00:00:00,1.68,1.6630606651306152,1.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Rita_do_Sapucai,2017-12-31T00:00:00,1.79,1.5322532653808594,14.4,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Rita_do_Sapucai,2018-12-31T00:00:00,1.8,1.784391164779663,0.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_da_Vargem,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_da_Vargem,2013-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_da_Vargem,2014-12-31T00:00:00,1.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_da_Vargem,2015-12-31T00:00:00,1.14,1.2217997312545776,7.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_da_Vargem,2016-12-31T00:00:00,1.74,1.6632733345031738,4.41,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_da_Vargem,2017-12-31T00:00:00,2.09,1.7008953094482422,18.62,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_da_Vargem,2018-12-31T00:00:00,1.8,2.1146128177642822,17.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_do_Jacare,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_do_Jacare,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_do_Jacare,2014-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_do_Jacare,2015-12-31T00:00:00,1.68,0.995468258857727,40.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_do_Jacare,2016-12-31T00:00:00,1.56,2.1041698455810547,34.88,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_do_Jacare,2017-12-31T00:00:00,1.2,1.758864402770996,46.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_do_Jacare,2018-12-31T00:00:00,1.8,1.7123262882232666,4.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Amparo,2012-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Amparo,2013-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Amparo,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Amparo,2015-12-31T00:00:00,1.2,1.4329192638397217,19.41,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Amparo,2016-12-31T00:00:00,1.8,1.7833720445632935,0.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Amparo,2017-12-31T00:00:00,1.34,1.8351894617080688,36.95,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Amparo,2018-12-31T00:00:00,2.16,1.6803897619247437,22.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Grama,2012-12-31T00:00:00,0.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Grama,2013-12-31T00:00:00,0.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Grama,2014-12-31T00:00:00,0.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Grama,2015-12-31T00:00:00,0.9,0.9348475933074951,3.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Grama,2016-12-31T00:00:00,0.9,0.9201855659484863,2.24,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Grama,2017-12-31T00:00:00,3.33,1.6588102579116821,50.19,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Grama,2018-12-31T00:00:00,1.8,1.6103891134262085,10.53,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Bento_Abade,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Bento_Abade,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Bento_Abade,2014-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Bento_Abade,2015-12-31T00:00:00,1.08,1.2100574970245361,12.04,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Bento_Abade,2016-12-31T00:00:00,1.62,1.6532869338989258,2.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Bento_Abade,2017-12-31T00:00:00,2.41,1.6488943099975586,31.58,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Bento_Abade,2018-12-31T00:00:00,2.4,2.4300172328948975,1.25,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Goncalo_do_Sapucai,2012-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Goncalo_do_Sapucai,2013-12-31T00:00:00,1.26,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Goncalo_do_Sapucai,2014-12-31T00:00:00,0.96,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Goncalo_do_Sapucai,2015-12-31T00:00:00,1.26,1.1243873834609985,10.76,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Goncalo_do_Sapucai,2016-12-31T00:00:00,1.5,1.490613579750061,0.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Goncalo_do_Sapucai,2017-12-31T00:00:00,2.04,1.4060745239257812,31.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Goncalo_do_Sapucai,2018-12-31T00:00:00,2.22,1.9081774950027466,14.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Joao_del_Rei,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Joao_del_Rei,2013-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Joao_del_Rei,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Joao_del_Rei,2015-12-31T00:00:00,1.62,1.2232129573822021,24.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Joao_del_Rei,2016-12-31T00:00:00,1.5,1.6694340705871582,11.3,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Joao_del_Rei,2017-12-31T00:00:00,1.75,1.6511049270629883,5.65,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Joao_del_Rei,2018-12-31T00:00:00,1.8,1.8230499029159546,1.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Lourenco,2012-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Lourenco,2013-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Lourenco,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Lourenco,2015-12-31T00:00:00,1.32,1.312077522277832,0.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Lourenco,2016-12-31T00:00:00,1.6,1.5490620136260986,3.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Lourenco,2017-12-31T00:00:00,1.77,1.6443135738372803,7.1,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Lourenco,2018-12-31T00:00:00,1.6,1.9096713066101074,19.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Roque_de_Minas,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Roque_de_Minas,2013-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Roque_de_Minas,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Roque_de_Minas,2015-12-31T00:00:00,1.08,1.0867326259613037,0.62,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Roque_de_Minas,2016-12-31T00:00:00,1.32,1.3147082328796387,0.4,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Roque_de_Minas,2017-12-31T00:00:00,1.46,1.5990314483642578,9.52,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Roque_de_Minas,2018-12-31T00:00:00,1.62,1.8678090572357178,15.3,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_da_Bela_Vista,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_da_Bela_Vista,2013-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_da_Bela_Vista,2014-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_da_Bela_Vista,2015-12-31T00:00:00,1.02,1.0665243864059448,4.56,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_da_Bela_Vista,2016-12-31T00:00:00,1.56,1.5561976432800293,0.24,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_da_Bela_Vista,2017-12-31T00:00:00,1.58,1.4903621673583984,5.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_da_Bela_Vista,2018-12-31T00:00:00,1.32,1.6107081174850464,22.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_do_Paraiso,2012-12-31T00:00:00,1.62,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_do_Paraiso,2013-12-31T00:00:00,1.71,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_do_Paraiso,2014-12-31T00:00:00,1.42,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_do_Paraiso,2015-12-31T00:00:00,1.38,1.4568512439727783,5.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_do_Paraiso,2016-12-31T00:00:00,1.7,1.6371612548828125,3.7,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_do_Paraiso,2017-12-31T00:00:00,1.54,1.5226629972457886,1.13,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_do_Paraiso,2018-12-31T00:00:00,1.89,1.966109275817871,4.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tiago,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tiago,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tiago,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tiago,2015-12-31T00:00:00,1.62,1.5013306140899658,7.33,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tiago,2016-12-31T00:00:00,1.62,1.661827802658081,2.58,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tiago,2017-12-31T00:00:00,2.17,1.7041339874267578,21.47,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tiago,2018-12-31T00:00:00,2.7,2.057589292526245,23.79,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tomas_de_Aquino,2012-12-31T00:00:00,1.75,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tomas_de_Aquino,2013-12-31T00:00:00,1.05,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tomas_de_Aquino,2014-12-31T00:00:00,1.35,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tomas_de_Aquino,2015-12-31T00:00:00,0.96,1.075282096862793,12.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tomas_de_Aquino,2016-12-31T00:00:00,1.8,1.930983066558838,7.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tomas_de_Aquino,2017-12-31T00:00:00,1.5,1.5181957483291626,1.21,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tomas_de_Aquino,2018-12-31T00:00:00,2.1,2.0193190574645996,3.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tome_das_Letras,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tome_das_Letras,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tome_das_Letras,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tome_das_Letras,2015-12-31T00:00:00,1.02,1.0617755651474,4.1,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tome_das_Letras,2016-12-31T00:00:00,1.62,1.6041715145111084,0.98,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tome_das_Letras,2017-12-31T00:00:00,1.05,1.4295152425765991,36.14,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tome_das_Letras,2018-12-31T00:00:00,1.5,1.4874625205993652,0.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senador_Jose_Bento,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senador_Jose_Bento,2013-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senador_Jose_Bento,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senador_Jose_Bento,2015-12-31T00:00:00,1.5,1.2740366458892822,15.06,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senador_Jose_Bento,2016-12-31T00:00:00,1.38,1.5944006443023682,15.54,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senador_Jose_Bento,2017-12-31T00:00:00,1.15,1.3712962865829468,19.24,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senador_Jose_Bento,2018-12-31T00:00:00,1.32,1.4543650150299072,10.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senhora_de_Oliveira,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senhora_de_Oliveira,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senhora_de_Oliveira,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senhora_de_Oliveira,2015-12-31T00:00:00,1.26,1.4443405866622925,14.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senhora_de_Oliveira,2016-12-31T00:00:00,1.2,1.2702876329421997,5.86,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senhora_de_Oliveira,2017-12-31T00:00:00,2.0,1.3308639526367188,33.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senhora_de_Oliveira,2018-12-31T00:00:00,1.8,1.781449556350708,1.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Serrania,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Serrania,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Serrania,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Serrania,2015-12-31T00:00:00,0.9,1.3152849674224854,46.14,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Serrania,2016-12-31T00:00:00,1.56,1.5202627182006836,2.55,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Serrania,2017-12-31T00:00:00,1.5,1.495526909828186,0.3,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Serrania,2018-12-31T00:00:00,1.8,1.5690546035766602,12.83,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Silvianopolis,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Silvianopolis,2013-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Silvianopolis,2014-12-31T00:00:00,0.78,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Silvianopolis,2015-12-31T00:00:00,0.9,0.9304393529891968,3.38,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Silvianopolis,2016-12-31T00:00:00,1.5,1.4435371160507202,3.76,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Silvianopolis,2017-12-31T00:00:00,1.07,1.3832584619522095,29.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Silvianopolis,2018-12-31T00:00:00,1.2,1.414219856262207,17.85,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Soledade_de_Minas,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Soledade_de_Minas,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Soledade_de_Minas,2014-12-31T00:00:00,1.06,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Soledade_de_Minas,2015-12-31T00:00:00,1.71,1.3319768905639648,22.11,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Soledade_de_Minas,2016-12-31T00:00:00,1.6,1.9218635559082031,20.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Soledade_de_Minas,2017-12-31T00:00:00,1.6,1.8107472658157349,13.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Soledade_de_Minas,2018-12-31T00:00:00,1.8,1.768530011177063,1.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Coracoes,2012-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Coracoes,2013-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Coracoes,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Coracoes,2015-12-31T00:00:00,0.96,1.07209312915802,11.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Coracoes,2016-12-31T00:00:00,1.68,1.9645732641220093,16.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Coracoes,2017-12-31T00:00:00,2.67,1.7136685848236084,35.82,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Coracoes,2018-12-31T00:00:00,2.1,2.6113932132720947,24.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Pontas,2012-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Pontas,2013-12-31T00:00:00,1.53,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Pontas,2014-12-31T00:00:00,1.02,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Pontas,2015-12-31T00:00:00,1.14,1.2569531202316284,10.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Pontas,2016-12-31T00:00:00,1.86,1.9891389608383179,6.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Pontas,2017-12-31T00:00:00,1.88,1.8155155181884766,3.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Pontas,2018-12-31T00:00:00,2.22,1.9655057191848755,11.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Turvolandia,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Turvolandia,2013-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Turvolandia,2014-12-31T00:00:00,0.84,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Turvolandia,2015-12-31T00:00:00,1.08,1.0457971096038818,3.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Turvolandia,2016-12-31T00:00:00,1.5,1.689098596572876,12.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Turvolandia,2017-12-31T00:00:00,1.8,1.5314953327178955,14.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Turvolandia,2018-12-31T00:00:00,2.29,1.90030038356781,17.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Uberlandia,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Uberlandia,2013-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Uberlandia,2014-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Uberlandia,2015-12-31T00:00:00,1.32,1.493908405303955,13.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Uberlandia,2016-12-31T00:00:00,1.62,1.6702128648757935,3.1,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Uberlandia,2017-12-31T00:00:00,3.15,1.9522956609725952,38.02,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Uberlandia,2018-12-31T00:00:00,2.64,4.421339511871338,67.47,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Vargem_Bonita,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Vargem_Bonita,2013-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Vargem_Bonita,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Vargem_Bonita,2015-12-31T00:00:00,1.08,1.1618657112121582,7.58,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Vargem_Bonita,2016-12-31T00:00:00,1.32,1.2940988540649414,1.96,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Vargem_Bonita,2017-12-31T00:00:00,1.67,1.5989041328430176,4.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Vargem_Bonita,2018-12-31T00:00:00,2.4,2.100925922393799,12.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Varginha,2012-12-31T00:00:00,1.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Varginha,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Varginha,2014-12-31T00:00:00,1.03,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Varginha,2015-12-31T00:00:00,1.2,1.2509454488754272,4.25,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Varginha,2016-12-31T00:00:00,1.74,1.860472321510315,6.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Varginha,2017-12-31T00:00:00,2.12,2.0173513889312744,4.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Varginha,2018-12-31T00:00:00,2.1,2.172278881072998,3.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Virginia,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Virginia,2013-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Virginia,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Virginia,2015-12-31T00:00:00,1.5,5.346618175506592,256.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Virginia,2016-12-31T00:00:00,1.6,1.5496132373809814,3.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Virginia,2017-12-31T00:00:00,1.4,1.6015949249267578,14.4,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Virginia,2018-12-31T00:00:00,1.8,1.5683259963989258,12.87,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aguanil,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aiuruoca,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Albertina,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alfenas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alterosa,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andradas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andrelandia,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Arceburgo,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Baependi,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bandeira_do_Sul,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Boa_Esperanca,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bom_Sucesso,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Borda_da_Mata,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Brazopolis,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bueno_Brandao,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cachoeira_de_Minas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Caete,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Camacho,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cambuquira,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campanha,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campestre,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_Belo,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_do_Meio,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campos_Gerais,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Candeias,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capetinga,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capitolio,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_da_Cachoeira,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_de_Minas,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_do_Rio_Claro,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carvalhopolis,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cassia,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_da_Barra_de_Minas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_das_Pedras,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_do_Rio_Verde,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_dos_Ouros,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Coqueiral,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cordislandia,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Corrego_Danta,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristais,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristina,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Delfinopolis,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Desterro_de_Entre_Rios,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divinesia,2019-12-31T00:00:00,2.0,3,50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divisa_Nova,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Dom_Vicoso,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Doresopolis,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Eloi_Mendes,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Entre_Rios_de_Minas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Fama,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Grao_Mogol,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Guape,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Heliodora,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ibituruna,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ijaci,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ilicinea,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Inconfidentes,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ingai,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itajuba,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itamogi,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itapecerica,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itumirim,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itutinga,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jacutinga,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jesuania,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lambari,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lavras,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Luminarias,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Machado,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Maria_da_Fe,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Moeda,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monsenhor_Paulo,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Alegre_de_Minas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Formoso,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Siao,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Muzambinho,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Natercia,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nazareno,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nepomuceno,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Olimpio_Noronha,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Oliveira,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ouro_Fino,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraguacu,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraisopolis,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pedralva,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Perdoes,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranga,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pirangucu,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranguinho,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Poco_Fundo,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pocos_de_Caldas,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alegre,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alto,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Presidente_Bernardes,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ribeirao_Vermelho,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Barbara_do_Monte_Verde,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Rita_do_Sapucai,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_da_Vargem,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_do_Jacare,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Amparo,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Grama,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Bento_Abade,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Goncalo_do_Sapucai,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Joao_del_Rei,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Lourenco,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Roque_de_Minas,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_da_Bela_Vista,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_do_Paraiso,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tiago,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tomas_de_Aquino,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tome_das_Letras,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senador_Jose_Bento,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senhora_de_Oliveira,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Serrania,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Silvianopolis,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Soledade_de_Minas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Coracoes,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Pontas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Turvolandia,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Uberlandia,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Vargem_Bonita,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Varginha,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Virginia,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aguanil,2020-12-31T00:00:00,2.1644859813084114,2.0106723308563232,-7.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aiuruoca,2020-12-31T00:00:00,1.8000000000000005,1.5328491926193237,-14.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Albertina,2020-12-31T00:00:00,1.68,1.6470930576324463,-1.96,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alfenas,2020-12-31T00:00:00,2.364130434782609,1.9883497953414917,-15.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alterosa,2020-12-31T00:00:00,1.980023501762632,1.8616691827774048,-5.98,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andradas,2020-12-31T00:00:00,2.568,1.6444509029388428,-35.96,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andrelandia,2020-12-31T00:00:00,1.8000000000000005,1.471191644668579,-18.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Arceburgo,2020-12-31T00:00:00,1.8387096774193543,1.5835902690887451,-13.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Baependi,2020-12-31T00:00:00,1.67948717948718,1.5063321590423584,-10.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bandeira_do_Sul,2020-12-31T00:00:00,1.5,1.8676373958587646,24.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Boa_Esperanca,2020-12-31T00:00:00,2.063248407643312,2.4900765419006348,20.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bom_Sucesso,2020-12-31T00:00:00,2.1,1.7012248039245605,-18.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Borda_da_Mata,2020-12-31T00:00:00,1.62,1.6632578372955322,2.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Brazopolis,2020-12-31T00:00:00,1.680327868852459,1.4351227283477783,-14.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bueno_Brandao,2020-12-31T00:00:00,1.8000000000000005,1.4435524940490723,-19.8,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cachoeira_de_Minas,2020-12-31T00:00:00,1.8596638655462183,2.1560630798339844,15.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Caete,2020-12-31T00:00:00,1.2,1.4470405578613281,20.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Camacho,2020-12-31T00:00:00,2.1,1.7964248657226562,-14.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cambuquira,2020-12-31T00:00:00,1.800145348837209,1.748801589012146,-2.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campanha,2020-12-31T00:00:00,1.6833930704898452,1.7847440242767334,6.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campestre,2020-12-31T00:00:00,2.138000770416025,1.8913850784301758,-11.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_Belo,2020-12-31T00:00:00,1.8000000000000005,1.309015154838562,-27.28,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_do_Meio,2020-12-31T00:00:00,2.4,2.081045389175415,-13.29,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campos_Gerais,2020-12-31T00:00:00,2.690649114843395,2.179189682006836,-19.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Candeias,2020-12-31T00:00:00,1.859937013094646,1.903700351715088,2.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capetinga,2020-12-31T00:00:00,2.213808463251671,2.9072647094726562,31.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capitolio,2020-12-31T00:00:00,2.1,1.9278558492660522,-8.2,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_da_Cachoeira,2020-12-31T00:00:00,1.8000000000000005,1.8995254039764404,5.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_de_Minas,2020-12-31T00:00:00,1.7400000000000002,1.4697372913360596,-15.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_do_Rio_Claro,2020-12-31T00:00:00,2.7847803881511743,2.1853044033050537,-21.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carvalhopolis,2020-12-31T00:00:00,1.679765395894428,1.6256535053253174,-3.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cassia,2020-12-31T00:00:00,2.1679245283018864,2.5698654651641846,18.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_da_Barra_de_Minas,2020-12-31T00:00:00,1.911042944785276,2.47019100189209,29.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_das_Pedras,2020-12-31T00:00:00,1.739921976592978,1.5456364154815674,-11.17,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_do_Rio_Verde,2020-12-31T00:00:00,1.8000000000000005,1.9556266069412231,8.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_dos_Ouros,2020-12-31T00:00:00,1.5028571428571431,1.5117533206939697,0.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Coqueiral,2020-12-31T00:00:00,2.160028248587571,1.8402951955795288,-14.8,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cordislandia,2020-12-31T00:00:00,1.62,1.8663444519042969,15.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Corrego_Danta,2020-12-31T00:00:00,1.8000000000000005,1.6464369297027588,-8.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristais,2020-12-31T00:00:00,1.92,2.315948009490967,20.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristina,2020-12-31T00:00:00,2.100263852242744,1.5415611267089844,-26.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Delfinopolis,2020-12-31T00:00:00,1.8000000000000005,2.4161508083343506,34.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Desterro_de_Entre_Rios,2020-12-31T00:00:00,2.4,2.0897488594055176,-12.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divinesia,2020-12-31T00:00:00,2.1,6.2058000564575195,195.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divisa_Nova,2020-12-31T00:00:00,1.8000000000000005,2.1158125400543213,17.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Dom_Vicoso,2020-12-31T00:00:00,1.4285714285714288,1.3207988739013672,-7.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Doresopolis,2020-12-31T00:00:00,1.8000000000000005,1.9797916412353516,9.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Eloi_Mendes,2020-12-31T00:00:00,2.109128416709644,1.7410402297973633,-17.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Entre_Rios_de_Minas,2020-12-31T00:00:00,1.8000000000000005,2.0843443870544434,15.8,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Fama,2020-12-31T00:00:00,1.8000000000000005,1.6304261684417725,-9.42,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Grao_Mogol,2020-12-31T00:00:00,0.7749999999999999,3.8701727390289307,399.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Guape,2020-12-31T00:00:00,1.847986942328618,2.1413450241088867,15.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Heliodora,2020-12-31T00:00:00,1.8000000000000005,1.6314626932144165,-9.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ibituruna,2020-12-31T00:00:00,1.8000000000000005,1.6671133041381836,-7.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ijaci,2020-12-31T00:00:00,2.458064516129032,1.8859741687774658,-23.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ilicinea,2020-12-31T00:00:00,2.09995817649519,2.4610509872436523,17.2,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Inconfidentes,2020-12-31T00:00:00,2.46,1.8735047578811646,-23.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ingai,2020-12-31T00:00:00,1.6791666666666667,2.054701805114746,22.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itajuba,2020-12-31T00:00:00,1.5882352941176472,1.9464356899261475,22.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itamogi,2020-12-31T00:00:00,2.400049176297025,1.5443332195281982,-35.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itapecerica,2020-12-31T00:00:00,1.8000000000000005,1.6540794372558594,-8.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itumirim,2020-12-31T00:00:00,2.050724637681159,1.809286117553711,-11.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itutinga,2020-12-31T00:00:00,1.8000000000000005,1.542004108428955,-14.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jacutinga,2020-12-31T00:00:00,1.8000000000000005,1.5634429454803467,-13.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jesuania,2020-12-31T00:00:00,1.5,1.5422133207321167,2.81,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lambari,2020-12-31T00:00:00,1.8000970402717127,1.5055605173110962,-16.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lavras,2020-12-31T00:00:00,1.8000000000000005,2.176401138305664,20.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Luminarias,2020-12-31T00:00:00,1.860103626943005,2.081143856048584,11.88,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Machado,2020-12-31T00:00:00,2.211839166046165,1.9543851613998413,-11.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Maria_da_Fe,2020-12-31T00:00:00,1.7333333333333332,1.5018420219421387,-13.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Moeda,2020-12-31T00:00:00,1.2,2.0683786869049072,72.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monsenhor_Paulo,2020-12-31T00:00:00,2.1,1.7145026922225952,-18.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Alegre_de_Minas,2020-12-31T00:00:00,3.0,1.786327600479126,-40.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Formoso,2020-12-31T00:00:00,0.6000000000000001,1.194969892501831,99.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Siao,2020-12-31T00:00:00,1.8000000000000005,1.339212417602539,-25.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Muzambinho,2020-12-31T00:00:00,1.764006791171477,1.754845142364502,-0.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Natercia,2020-12-31T00:00:00,1.4399193548387097,1.5312881469726562,6.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nazareno,2020-12-31T00:00:00,2.1,1.8235907554626465,-13.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nepomuceno,2020-12-31T00:00:00,1.920040743570155,1.7261879444122314,-10.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Olimpio_Noronha,2020-12-31T00:00:00,1.5595854922279788,1.3715791702270508,-12.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Oliveira,2020-12-31T00:00:00,1.947151898734177,2.2480673789978027,15.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ouro_Fino,2020-12-31T00:00:00,2.099920063948841,1.5073728561401367,-28.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraguacu,2020-12-31T00:00:00,2.3413259668508286,1.740100383758545,-25.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraisopolis,2020-12-31T00:00:00,2.4,1.804344654083252,-24.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pedralva,2020-12-31T00:00:00,1.68,1.428813099861145,-14.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Perdoes,2020-12-31T00:00:00,1.92,1.831849217414856,-4.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranga,2020-12-31T00:00:00,2.4,2.0636684894561768,-14.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pirangucu,2020-12-31T00:00:00,1.333333333333333,1.1793103218078613,-11.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranguinho,2020-12-31T00:00:00,1.458181818181818,1.479084849357605,1.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Poco_Fundo,2020-12-31T00:00:00,1.740033329060377,2.0624094009399414,18.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pocos_de_Caldas,2020-12-31T00:00:00,1.8000000000000005,1.9514641761779785,8.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alegre,2020-12-31T00:00:00,1.6375,1.4755489826202393,-9.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alto,2020-12-31T00:00:00,1.45,1.59614098072052,10.08,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Presidente_Bernardes,2020-12-31T00:00:00,1.5,1.6332467794418335,8.88,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ribeirao_Vermelho,2020-12-31T00:00:00,1.92,1.7495160102844238,-8.88,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Barbara_do_Monte_Verde,2020-12-31T00:00:00,1.6000000000000003,1.1822971105575562,-26.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Rita_do_Sapucai,2020-12-31T00:00:00,1.68,1.7288551330566406,2.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_da_Vargem,2020-12-31T00:00:00,2.22,2.002723217010498,-9.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_do_Jacare,2020-12-31T00:00:00,1.8000000000000005,1.592921495437622,-11.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Amparo,2020-12-31T00:00:00,2.1,2.125459909439087,1.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Grama,2020-12-31T00:00:00,1.695652173913043,1.9469046592712402,14.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Bento_Abade,2020-12-31T00:00:00,1.8000000000000005,2.4383203983306885,35.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Goncalo_do_Sapucai,2020-12-31T00:00:00,2.4,1.9711649417877197,-17.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Joao_del_Rei,2020-12-31T00:00:00,1.9709821428571432,1.9657793045043945,-0.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Lourenco,2020-12-31T00:00:00,3.3928571428571423,1.614876627922058,-52.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Roque_de_Minas,2020-12-31T00:00:00,1.8000000000000005,1.5830399990081787,-12.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_da_Bela_Vista,2020-12-31T00:00:00,1.5,1.5523912906646729,3.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_do_Paraiso,2020-12-31T00:00:00,1.929071661237785,1.5819494724273682,-17.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tiago,2020-12-31T00:00:00,2.69971671388102,2.4821739196777344,-8.06,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tomas_de_Aquino,2020-12-31T00:00:00,2.099966151415999,1.9205727577209473,-8.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tome_das_Letras,2020-12-31T00:00:00,2.133763094278808,1.3593425750732422,-36.29,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senador_Jose_Bento,2020-12-31T00:00:00,1.620618556701031,1.257548213005066,-22.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senhora_de_Oliveira,2020-12-31T00:00:00,1.8000000000000005,1.8996660709381104,5.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Serrania,2020-12-31T00:00:00,1.8000000000000005,1.7315349578857422,-3.8,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Silvianopolis,2020-12-31T00:00:00,1.50032154340836,1.2447121143341064,-17.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Soledade_de_Minas,2020-12-31T00:00:00,3.2000000000000006,1.721281886100769,-46.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Coracoes,2020-12-31T00:00:00,2.1,2.051683187484741,-2.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Pontas,2020-12-31T00:00:00,2.26,2.2145354747772217,-2.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Turvolandia,2020-12-31T00:00:00,1.6202531645569618,2.270873546600342,40.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Uberlandia,2020-12-31T00:00:00,1.920754716981132,2.9995150566101074,56.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Vargem_Bonita,2020-12-31T00:00:00,1.8031222896790975,2.580533266067505,43.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Varginha,2020-12-31T00:00:00,1.899977968715576,2.1227293014526367,11.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Virginia,2020-12-31T00:00:00,1.5333333333333332,1.5706562995910645,2.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aguanil,2021-12-31T00:00:00,1.7382352941176469,2.065581798553467,18.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aiuruoca,2021-12-31T00:00:00,1.2,2.167367935180664,80.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Albertina,2021-12-31T00:00:00,1.56,1.6210143566131592,3.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alfenas,2021-12-31T00:00:00,1.6209774981853378,1.8848888874053955,16.28,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alterosa,2021-12-31T00:00:00,1.08,1.7747108936309814,64.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andradas,2021-12-31T00:00:00,1.26,2.4019508361816406,90.63,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andrelandia,2021-12-31T00:00:00,1.4,1.901984691619873,35.86,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Arceburgo,2021-12-31T00:00:00,1.354903268845897,1.7556123733520508,29.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Baependi,2021-12-31T00:00:00,1.079646017699115,1.9600661993026733,81.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bandeira_do_Sul,2021-12-31T00:00:00,1.8011695906432754,1.6448657512664795,-8.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Boa_Esperanca,2021-12-31T00:00:00,1.570339108544351,2.0591278076171875,31.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bom_Sucesso,2021-12-31T00:00:00,1.37989417989418,1.675354242324829,21.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Borda_da_Mata,2021-12-31T00:00:00,1.2,1.6868271827697754,40.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Brazopolis,2021-12-31T00:00:00,1.200934579439252,1.62140953540802,35.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bueno_Brandao,2021-12-31T00:00:00,1.2,1.7891467809677124,49.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cachoeira_de_Minas,2021-12-31T00:00:00,1.5602040816326532,1.9183259010314941,22.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Caete,2021-12-31T00:00:00,1.222222222222222,1.359647274017334,11.24,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Camacho,2021-12-31T00:00:00,1.5,1.7055692672729492,13.7,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cambuquira,2021-12-31T00:00:00,1.2,1.811835765838623,50.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campanha,2021-12-31T00:00:00,1.325757575757576,1.6761057376861572,26.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campestre,2021-12-31T00:00:00,1.2899602385685882,1.9982292652130127,54.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_Belo,2021-12-31T00:00:00,1.5,1.2724571228027344,-15.17,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_do_Meio,2021-12-31T00:00:00,1.255813953488372,1.8870552778244019,50.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campos_Gerais,2021-12-31T00:00:00,1.589070422535211,1.980525255203247,24.63,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Candeias,2021-12-31T00:00:00,1.199963309484498,1.5417604446411133,28.48,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capetinga,2021-12-31T00:00:00,1.6399999999999997,2.2047653198242188,34.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capitolio,2021-12-31T00:00:00,1.2,1.6974151134490967,41.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_da_Cachoeira,2021-12-31T00:00:00,1.439980158730159,1.7222474813461304,19.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_de_Minas,2021-12-31T00:00:00,1.3799999999999997,1.848787546157837,33.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_do_Rio_Claro,2021-12-31T00:00:00,1.643702081051479,1.947216272354126,18.47,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carvalhopolis,2021-12-31T00:00:00,1.14,1.5842018127441406,38.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cassia,2021-12-31T00:00:00,1.314977578475336,1.8409714698791504,40.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_da_Barra_de_Minas,2021-12-31T00:00:00,1.462068965517241,1.8575732707977295,27.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_das_Pedras,2021-12-31T00:00:00,1.5,1.752727746963501,16.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_do_Rio_Verde,2021-12-31T00:00:00,1.500115340253749,1.8741073608398438,24.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_dos_Ouros,2021-12-31T00:00:00,1.26,2.7816827297210693,120.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Coqueiral,2021-12-31T00:00:00,1.5,1.8153527975082397,21.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cordislandia,2021-12-31T00:00:00,1.319791666666667,1.6001653671264648,21.24,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Corrego_Danta,2021-12-31T00:00:00,1.08029197080292,1.6467949151992798,52.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristais,2021-12-31T00:00:00,1.318776371308017,1.7674078941345215,34.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristina,2021-12-31T00:00:00,1.68,1.808915376663208,7.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Delfinopolis,2021-12-31T00:00:00,0.9,1.6751973628997803,86.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Desterro_de_Entre_Rios,2021-12-31T00:00:00,1.5,2.0333125591278076,35.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divinesia,2021-12-31T00:00:00,2.1,2.3792977333068848,13.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divisa_Nova,2021-12-31T00:00:00,1.2601319509896318,1.799949049949646,42.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Dom_Vicoso,2021-12-31T00:00:00,1.2,1.7821619510650635,48.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Doresopolis,2021-12-31T00:00:00,1.2,1.769157886505127,47.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Eloi_Mendes,2021-12-31T00:00:00,1.037885462555066,2.008521556854248,93.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Entre_Rios_de_Minas,2021-12-31T00:00:00,1.8000000000000005,1.8012651205062866,0.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Fama,2021-12-31T00:00:00,1.5,1.7640304565429688,17.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Grao_Mogol,2021-12-31T00:00:00,1.155555555555555,0.8109672665596008,-29.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Guape,2021-12-31T00:00:00,1.376842105263158,1.669555425643921,21.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Heliodora,2021-12-31T00:00:00,1.2,1.5439453125,28.66,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ibituruna,2021-12-31T00:00:00,1.6000000000000003,1.7114405632019043,6.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ijaci,2021-12-31T00:00:00,1.2375,2.371281147003174,91.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ilicinea,2021-12-31T00:00:00,1.560035211267606,1.73832106590271,11.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Inconfidentes,2021-12-31T00:00:00,1.5,2.587655544281006,72.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ingai,2021-12-31T00:00:00,1.320075757575758,1.6589741706848145,25.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itajuba,2021-12-31T00:00:00,1.181818181818182,1.6752898693084717,41.76,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itamogi,2021-12-31T00:00:00,1.260064724919094,2.084897041320801,65.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itapecerica,2021-12-31T00:00:00,1.222784810126582,1.4049909114837646,14.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itumirim,2021-12-31T00:00:00,1.2595155709342565,1.886427879333496,49.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itutinga,2021-12-31T00:00:00,1.2,1.6886661052703857,40.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jacutinga,2021-12-31T00:00:00,1.32014652014652,1.8620924949645996,41.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jesuania,2021-12-31T00:00:00,1.2,1.5839649438858032,32.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lambari,2021-12-31T00:00:00,1.199954863461973,1.902866244316101,58.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lavras,2021-12-31T00:00:00,1.259900454447089,1.8239368200302124,44.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Luminarias,2021-12-31T00:00:00,1.380229885057471,1.974675178527832,43.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Machado,2021-12-31T00:00:00,1.444286871961102,1.7806416749954224,23.29,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Maria_da_Fe,2021-12-31T00:00:00,1.444444444444444,1.8189787864685059,25.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Moeda,2021-12-31T00:00:00,1.8000000000000005,1.1451752185821533,-36.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monsenhor_Paulo,2021-12-31T00:00:00,1.41,1.972421407699585,39.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Alegre_de_Minas,2021-12-31T00:00:00,2.765957446808511,1.1115859746932983,-59.81,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Formoso,2021-12-31T00:00:00,0.55,0.9704223871231079,76.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Siao,2021-12-31T00:00:00,1.5,1.7845661640167236,18.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Muzambinho,2021-12-31T00:00:00,1.199937762564182,1.7108737230300903,42.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Natercia,2021-12-31T00:00:00,1.4399193548387097,1.4108967781066895,-2.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nazareno,2021-12-31T00:00:00,1.5,1.8843421936035156,25.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nepomuceno,2021-12-31T00:00:00,1.379962721342032,1.8608795404434204,34.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Olimpio_Noronha,2021-12-31T00:00:00,1.20026525198939,1.597048282623291,33.06,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Oliveira,2021-12-31T00:00:00,1.537805840568272,1.8281035423278809,18.88,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ouro_Fino,2021-12-31T00:00:00,1.2,2.171642780303955,80.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraguacu,2021-12-31T00:00:00,0.9712280701754386,1.9163901805877686,97.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraisopolis,2021-12-31T00:00:00,1.2,2.005373954772949,67.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pedralva,2021-12-31T00:00:00,1.5,1.5481383800506592,3.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Perdoes,2021-12-31T00:00:00,1.44017094017094,1.8315871953964233,27.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranga,2021-12-31T00:00:00,2.1,2.0381295680999756,-2.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pirangucu,2021-12-31T00:00:00,1.333333333333333,1.3025293350219727,-2.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranguinho,2021-12-31T00:00:00,1.380549682875264,1.4798258543014526,7.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Poco_Fundo,2021-12-31T00:00:00,0.9601760412194076,1.519062876701355,58.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pocos_de_Caldas,2021-12-31T00:00:00,1.4199999999999997,1.4950220584869385,5.28,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alegre,2021-12-31T00:00:00,1.2,1.6079212427139282,33.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alto,2021-12-31T00:00:00,1.5047619047619047,1.6827129125595093,11.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Presidente_Bernardes,2021-12-31T00:00:00,1.56,1.4370698928833008,-7.88,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ribeirao_Vermelho,2021-12-31T00:00:00,1.258899676375405,1.9542090892791748,55.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Barbara_do_Monte_Verde,2021-12-31T00:00:00,1.4,1.469290018081665,4.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Rita_do_Sapucai,2021-12-31T00:00:00,1.5,1.6187344789505005,7.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_da_Vargem,2021-12-31T00:00:00,1.44,2.086214542388916,44.88,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_do_Jacare,2021-12-31T00:00:00,1.2,1.5166943073272705,26.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Amparo,2021-12-31T00:00:00,1.3799999999999997,1.8144904375076294,31.48,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Grama,2021-12-31T00:00:00,1.9130434782608696,1.6928012371063232,-11.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Bento_Abade,2021-12-31T00:00:00,1.32,1.7940497398376465,35.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Goncalo_do_Sapucai,2021-12-31T00:00:00,1.62,2.1144752502441406,30.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Joao_del_Rei,2021-12-31T00:00:00,1.296875,1.893868088722229,46.03,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Lourenco,2021-12-31T00:00:00,1.5087719298245608,4.4565629959106445,195.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Roque_de_Minas,2021-12-31T00:00:00,1.2,1.6137646436691284,34.48,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_da_Bela_Vista,2021-12-31T00:00:00,1.2,1.5042195320129395,25.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_do_Paraiso,2021-12-31T00:00:00,1.14450771643146,1.7660048007965088,54.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tiago,2021-12-31T00:00:00,1.5,2.274521827697754,51.63,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tomas_de_Aquino,2021-12-31T00:00:00,1.2,1.8056156635284424,50.47,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tome_das_Letras,2021-12-31T00:00:00,1.2,2.3699841499328613,97.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senador_Jose_Bento,2021-12-31T00:00:00,1.32,1.519789218902588,15.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senhora_de_Oliveira,2021-12-31T00:00:00,1.62037037037037,1.755039930343628,8.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Serrania,2021-12-31T00:00:00,1.08,1.7354035377502441,60.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Silvianopolis,2021-12-31T00:00:00,0.9601265822784812,1.4766631126403809,53.8,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Soledade_de_Minas,2021-12-31T00:00:00,1.7204819277108432,4.6110076904296875,168.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Coracoes,2021-12-31T00:00:00,1.5,2.2493605613708496,49.96,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Pontas,2021-12-31T00:00:00,1.3500296384113808,1.995776891708374,47.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Turvolandia,2021-12-31T00:00:00,1.3797385620915028,1.7973811626434326,30.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Uberlandia,2021-12-31T00:00:00,1.77962962962963,2.1594297885894775,21.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Vargem_Bonita,2021-12-31T00:00:00,1.519565217391304,2.022216796875,33.08,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Varginha,2021-12-31T00:00:00,1.2,1.8261103630065918,52.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Virginia,2021-12-31T00:00:00,1.433333333333333,1.6889276504516602,17.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aguanil,2022-12-31T00:00:00,1.56,1.7846893072128296,14.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aiuruoca,2022-12-31T00:00:00,1.5,1.3511940240859985,-9.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Albertina,2022-12-31T00:00:00,1.44,1.589609146118164,10.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alfenas,2022-12-31T00:00:00,0.9243951612903224,1.6418250799179077,77.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alterosa,2022-12-31T00:00:00,1.080104712041885,0.4904058277606964,-54.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andradas,2022-12-31T00:00:00,1.26,0.4383935332298279,-65.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andrelandia,2022-12-31T00:00:00,1.8000000000000005,1.4363136291503906,-20.2,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Arceburgo,2022-12-31T00:00:00,0.9244897959183672,1.4823129177093506,60.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Baependi,2022-12-31T00:00:00,1.079558011049724,1.2642728090286255,17.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bandeira_do_Sul,2022-12-31T00:00:00,1.110344827586207,1.5073728561401367,35.76,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Boa_Esperanca,2022-12-31T00:00:00,0.9726256983240223,1.6861958503723145,73.37,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bom_Sucesso,2022-12-31T00:00:00,1.440136054421769,1.4257116317749023,-1.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Borda_da_Mata,2022-12-31T00:00:00,1.2,1.356534481048584,13.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Brazopolis,2022-12-31T00:00:00,1.200934579439252,1.317058801651001,9.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bueno_Brandao,2022-12-31T00:00:00,1.440196078431373,1.3372784852981567,-7.15,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cachoeira_de_Minas,2022-12-31T00:00:00,1.5,1.6662390232086182,11.08,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Caete,2022-12-31T00:00:00,1.5333333333333332,1.2192643880844116,-20.48,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Camacho,2022-12-31T00:00:00,1.380176211453745,1.5400038957595825,11.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cambuquira,2022-12-31T00:00:00,1.32,1.360072374343872,3.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campanha,2022-12-31T00:00:00,1.266469038208169,1.4000060558319092,10.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campestre,2022-12-31T00:00:00,1.020018115942029,1.4050874710083008,37.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_Belo,2022-12-31T00:00:00,1.2,1.372018575668335,14.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_do_Meio,2022-12-31T00:00:00,1.580968858131488,1.3640750646591187,-13.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campos_Gerais,2022-12-31T00:00:00,1.535771358328211,1.5515861511230469,1.03,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Candeias,2022-12-31T00:00:00,1.020017406440383,1.3430219888687134,31.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capetinga,2022-12-31T00:00:00,1.407056229327453,1.7630038261413574,25.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capitolio,2022-12-31T00:00:00,1.08,1.3370591402053833,23.8,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_da_Cachoeira,2022-12-31T00:00:00,1.2,1.4939236640930176,24.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_de_Minas,2022-12-31T00:00:00,1.14,1.2179280519485474,6.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_do_Rio_Claro,2022-12-31T00:00:00,1.619555143651529,1.7823071479797363,10.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carvalhopolis,2022-12-31T00:00:00,1.44,1.2928876876831055,-10.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cassia,2022-12-31T00:00:00,1.413626373626374,1.3804231882095337,-2.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_da_Barra_de_Minas,2022-12-31T00:00:00,1.370833333333333,1.5725979804992676,14.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_das_Pedras,2022-12-31T00:00:00,1.375,1.5838139057159424,15.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_do_Rio_Verde,2022-12-31T00:00:00,1.5,1.6353554725646973,9.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_dos_Ouros,2022-12-31T00:00:00,1.261168384879725,1.3169310092926025,4.42,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Coqueiral,2022-12-31T00:00:00,1.2,1.474721074104309,22.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cordislandia,2022-12-31T00:00:00,1.260122699386503,1.391932725906372,10.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Corrego_Danta,2022-12-31T00:00:00,1.5,1.2842528820037842,-14.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristais,2022-12-31T00:00:00,1.059063136456212,1.499030351638794,41.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristina,2022-12-31T00:00:00,1.320183486238532,1.6189141273498535,22.63,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Delfinopolis,2022-12-31T00:00:00,1.3799999999999997,1.0329523086547852,-25.15,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Desterro_de_Entre_Rios,2022-12-31T00:00:00,1.8000000000000005,1.6234335899353027,-9.81,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divinesia,2022-12-31T00:00:00,2.2230769230769227,2.043001174926758,-8.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divisa_Nova,2022-12-31T00:00:00,1.260204081632653,1.105879545211792,-12.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Dom_Vicoso,2022-12-31T00:00:00,1.320588235294118,1.2982935905456543,-1.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Doresopolis,2022-12-31T00:00:00,1.32,1.442903757095337,9.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Eloi_Mendes,2022-12-31T00:00:00,1.090201870999508,1.1686989068984985,7.2,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Entre_Rios_de_Minas,2022-12-31T00:00:00,1.8000000000000005,1.7989284992218018,-0.06,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Fama,2022-12-31T00:00:00,1.379787234042553,1.5350451469421387,11.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Grao_Mogol,2022-12-31T00:00:00,1.177777777777778,0.7303963899612427,-37.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Guape,2022-12-31T00:00:00,1.262758620689655,1.4888641834259033,17.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Heliodora,2022-12-31T00:00:00,1.080140597539543,1.3631328344345093,26.2,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ibituruna,2022-12-31T00:00:00,1.2,1.6177771091461182,34.81,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ijaci,2022-12-31T00:00:00,1.173913043478261,1.245813012123108,6.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ilicinea,2022-12-31T00:00:00,1.080069324090121,1.6660678386688232,54.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Inconfidentes,2022-12-31T00:00:00,1.739917695473251,1.7132713794708252,-1.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ingai,2022-12-31T00:00:00,1.380228136882129,1.348367691040039,-2.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itajuba,2022-12-31T00:00:00,1.068965517241379,1.370559573173523,28.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itamogi,2022-12-31T00:00:00,1.32,1.5341827869415283,16.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itapecerica,2022-12-31T00:00:00,1.040506329113924,1.246994972229004,19.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itumirim,2022-12-31T00:00:00,1.249307479224377,1.3380048274993896,7.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itutinga,2022-12-31T00:00:00,1.8000000000000005,1.317490577697754,-26.81,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jacutinga,2022-12-31T00:00:00,1.289920424403183,1.391826868057251,7.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jesuania,2022-12-31T00:00:00,1.2,1.3124983310699463,9.37,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lambari,2022-12-31T00:00:00,1.2,1.3472087383270264,12.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lavras,2022-12-31T00:00:00,1.260078277886497,1.296554684638977,2.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Luminarias,2022-12-31T00:00:00,1.380229885057471,1.430439829826355,3.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Machado,2022-12-31T00:00:00,0.964047619047619,1.4359583854675293,48.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Maria_da_Fe,2022-12-31T00:00:00,1.2,1.472549557685852,22.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Moeda,2022-12-31T00:00:00,1.25,1.168020248413086,-6.56,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monsenhor_Paulo,2022-12-31T00:00:00,1.23,1.5146193504333496,23.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Alegre_de_Minas,2022-12-31T00:00:00,2.3296703296703294,2.159230947494507,-7.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Formoso,2022-12-31T00:00:00,0.6000000000000001,0.5489338040351868,-8.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Siao,2022-12-31T00:00:00,1.2,1.5379066467285156,28.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Muzambinho,2022-12-31T00:00:00,1.43993993993994,1.4229297637939453,-1.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Natercia,2022-12-31T00:00:00,1.4399193548387097,0.6022238731384277,-58.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nazareno,2022-12-31T00:00:00,1.319811320754717,1.5706332921981812,19.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nepomuceno,2022-12-31T00:00:00,0.8699884125144843,1.4028470516204834,61.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Olimpio_Noronha,2022-12-31T00:00:00,1.2,1.3225157260894775,10.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Oliveira,2022-12-31T00:00:00,1.535560504825538,1.491426944732666,-2.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ouro_Fino,2022-12-31T00:00:00,1.2,1.2695337533950806,5.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraguacu,2022-12-31T00:00:00,1.043954802259887,1.1588976383209229,11.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraisopolis,2022-12-31T00:00:00,1.322222222222222,1.1699941158294678,-11.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pedralva,2022-12-31T00:00:00,1.320091324200913,1.517044186592102,14.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Perdoes,2022-12-31T00:00:00,1.259859154929577,1.5207985639572144,20.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranga,2022-12-31T00:00:00,1.8000000000000005,2.103846549987793,16.88,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pirangucu,2022-12-31T00:00:00,1.333333333333333,1.2307640314102173,-7.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranguinho,2022-12-31T00:00:00,1.380549682875264,1.459838628768921,5.74,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Poco_Fundo,2022-12-31T00:00:00,1.079978925184405,1.0776665210723877,-0.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pocos_de_Caldas,2022-12-31T00:00:00,1.169867549668874,1.3247737884521484,13.24,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alegre,2022-12-31T00:00:00,1.5,1.3168163299560547,-12.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alto,2022-12-31T00:00:00,1.5,1.4410827159881592,-3.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Presidente_Bernardes,2022-12-31T00:00:00,1.501449275362319,1.3992167711257935,-6.81,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ribeirao_Vermelho,2022-12-31T00:00:00,1.32,1.3024418354034424,-1.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Barbara_do_Monte_Verde,2022-12-31T00:00:00,1.3,1.0407633781433105,-19.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Rita_do_Sapucai,2022-12-31T00:00:00,1.14,1.5111783742904663,32.56,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_da_Vargem,2022-12-31T00:00:00,0.9900921658986176,1.5616872310638428,57.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_do_Jacare,2022-12-31T00:00:00,1.5,1.234349012374878,-17.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Amparo,2022-12-31T00:00:00,1.26,1.5081062316894531,19.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Grama,2022-12-31T00:00:00,1.565217391304348,1.4517240524291992,-7.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Bento_Abade,2022-12-31T00:00:00,1.2,1.3322792053222656,11.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Goncalo_do_Sapucai,2022-12-31T00:00:00,1.08,1.5694785118103027,45.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Joao_del_Rei,2022-12-31T00:00:00,1.836244541484716,1.5220715999603271,-17.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Lourenco,2022-12-31T00:00:00,1.8000000000000005,1.4822416305541992,-17.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Roque_de_Minas,2022-12-31T00:00:00,1.32,1.3803104162216187,4.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_da_Bela_Vista,2022-12-31T00:00:00,1.5,1.2968195676803589,-13.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_do_Paraiso,2022-12-31T00:00:00,1.3172147001934242,1.291125774383545,-1.98,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tiago,2022-12-31T00:00:00,1.3210526315789468,1.6619274616241455,25.8,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tomas_de_Aquino,2022-12-31T00:00:00,1.2,1.3384711742401123,11.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tome_das_Letras,2022-12-31T00:00:00,1.434375,1.2540936470031738,-12.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senador_Jose_Bento,2022-12-31T00:00:00,1.32,1.3110013008117676,-0.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senhora_de_Oliveira,2022-12-31T00:00:00,1.5,1.650039553642273,10.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Serrania,2022-12-31T00:00:00,1.2,1.2490365505218506,4.09,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Silvianopolis,2022-12-31T00:00:00,0.9,0.784358561038971,-12.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Soledade_de_Minas,2022-12-31T00:00:00,1.5,1.6084163188934326,7.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Coracoes,2022-12-31T00:00:00,1.08,1.5437949895858765,42.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Pontas,2022-12-31T00:00:00,0.975023651844844,1.4789819717407227,51.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Turvolandia,2022-12-31T00:00:00,1.4398692810457523,1.3915603160858154,-3.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Uberlandia,2022-12-31T00:00:00,1.681818181818182,1.845735788345337,9.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Vargem_Bonita,2022-12-31T00:00:00,1.235217391304348,1.674967885017395,35.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Varginha,2022-12-31T00:00:00,1.020039292730845,1.2902930974960327,26.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Virginia,2022-12-31T00:00:00,1.62,1.3867673873901367,-14.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aguanil,2023-12-31T00:00:00,1.717774762550882,1.765472650527954,2.78,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aiuruoca,2023-12-31T00:00:00,1.6000000000000003,1.5938465595245361,-0.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Albertina,2023-12-31T00:00:00,1.5,1.634873867034912,8.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alfenas,2023-12-31T00:00:00,1.698430922311519,1.646017074584961,-3.09,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alterosa,2023-12-31T00:00:00,1.2603960396039595,1.356403112411499,7.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andradas,2023-12-31T00:00:00,1.22,1.9376505613327026,58.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andrelandia,2023-12-31T00:00:00,1.8000000000000005,1.7394323348999023,-3.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Arceburgo,2023-12-31T00:00:00,1.8482558139534884,1.336034893989563,-27.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Baependi,2023-12-31T00:00:00,1.439779005524862,1.4986857175827026,4.09,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bandeira_do_Sul,2023-12-31T00:00:00,1.5,1.4770753383636475,-1.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Boa_Esperanca,2023-12-31T00:00:00,1.08,1.4766967296600342,36.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bom_Sucesso,2023-12-31T00:00:00,1.5,1.6512184143066406,10.08,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Borda_da_Mata,2023-12-31T00:00:00,1.290476190476191,1.443069338798523,11.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Brazopolis,2023-12-31T00:00:00,1.440389294403893,1.421618938446045,-1.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bueno_Brandao,2023-12-31T00:00:00,1.379901960784314,2.360888957977295,71.09,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cachoeira_de_Minas,2023-12-31T00:00:00,1.6204081632653062,1.6938745975494385,4.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Caete,2023-12-31T00:00:00,1.166666666666667,1.7118600606918335,46.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Camacho,2023-12-31T00:00:00,1.680155642023346,1.573425531387329,-6.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cambuquira,2023-12-31T00:00:00,1.56,1.5232070684432983,-2.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campanha,2023-12-31T00:00:00,1.327075098814229,1.4127124547958374,6.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campestre,2023-12-31T00:00:00,1.319964428634949,1.4739716053009033,11.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_Belo,2023-12-31T00:00:00,1.5,1.416778326034546,-5.55,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_do_Meio,2023-12-31T00:00:00,0.8771626297577856,1.67824125289917,91.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campos_Gerais,2023-12-31T00:00:00,1.560242401107029,1.8806533813476562,20.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Candeias,2023-12-31T00:00:00,1.080032206119163,1.2378450632095337,14.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capetinga,2023-12-31T00:00:00,1.4484046164290565,1.6341526508331299,12.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capitolio,2023-12-31T00:00:00,1.32,1.314254641532898,-0.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_da_Cachoeira,2023-12-31T00:00:00,1.32,1.456505298614502,10.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_de_Minas,2023-12-31T00:00:00,1.5,1.604259967803955,6.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_do_Rio_Claro,2023-12-31T00:00:00,1.816856256463288,1.858900785446167,2.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carvalhopolis,2023-12-31T00:00:00,1.560106382978723,1.4314355850219727,-8.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cassia,2023-12-31T00:00:00,1.6377649325626198,1.4743521213531494,-9.98,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_da_Barra_de_Minas,2023-12-31T00:00:00,1.680769230769231,1.4953290224075317,-11.03,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_das_Pedras,2023-12-31T00:00:00,1.5598885793871868,1.6157817840576172,3.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_do_Rio_Verde,2023-12-31T00:00:00,1.44,1.65853750705719,15.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_dos_Ouros,2023-12-31T00:00:00,1.422222222222222,1.3840937614440918,-2.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Coqueiral,2023-12-31T00:00:00,1.32,1.5512332916259766,17.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cordislandia,2023-12-31T00:00:00,1.14,1.395813226699829,22.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Corrego_Danta,2023-12-31T00:00:00,1.44,1.3450852632522583,-6.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristais,2023-12-31T00:00:00,1.5631111111111111,1.3319315910339355,-14.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristina,2023-12-31T00:00:00,1.68,1.948258876800537,15.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Delfinopolis,2023-12-31T00:00:00,1.501818181818182,1.3527487516403198,-9.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Desterro_de_Entre_Rios,2023-12-31T00:00:00,1.375,2.0345685482025146,47.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divinesia,2023-12-31T00:00:00,2.1,2.2184219360351562,5.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divisa_Nova,2023-12-31T00:00:00,1.680092592592593,1.4619946479797363,-12.98,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Dom_Vicoso,2023-12-31T00:00:00,1.32,1.4177625179290771,7.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Doresopolis,2023-12-31T00:00:00,1.501960784313725,1.3388445377349854,-10.86,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Eloi_Mendes,2023-12-31T00:00:00,1.577412806790921,1.4826595783233643,-6.01,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Entre_Rios_de_Minas,2023-12-31T00:00:00,1.56,53.416473388671875,3324.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Fama,2023-12-31T00:00:00,1.559677419354839,1.546114206314087,-0.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Grao_Mogol,2023-12-31T00:00:00,1.155555555555555,1.0481969118118286,-9.29,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Guape,2023-12-31T00:00:00,1.560674157303371,1.4350790977478027,-8.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Heliodora,2023-12-31T00:00:00,1.439872408293461,1.409593105316162,-2.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ibituruna,2023-12-31T00:00:00,1.8000000000000005,1.4977397918701172,-16.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ijaci,2023-12-31T00:00:00,1.971428571428572,1.3584871292114258,-31.09,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ilicinea,2023-12-31T00:00:00,1.560032362459547,1.5211832523345947,-2.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Inconfidentes,2023-12-31T00:00:00,1.7401360544217692,2.2109217643737793,27.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ingai,2023-12-31T00:00:00,1.6197718631178712,1.4368922710418701,-11.29,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itajuba,2023-12-31T00:00:00,1.5862068965517242,1.3696532249450684,-13.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itamogi,2023-12-31T00:00:00,1.740011254924029,1.486481785774231,-14.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itapecerica,2023-12-31T00:00:00,0.9037974683544304,1.2356938123703003,36.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itumirim,2023-12-31T00:00:00,1.532033426183844,1.3745014667510986,-10.28,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itutinga,2023-12-31T00:00:00,1.5,1.5712987184524536,4.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jacutinga,2023-12-31T00:00:00,1.160053262316911,1.6631534099578857,43.37,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jesuania,2023-12-31T00:00:00,1.26,1.4094352722167969,11.86,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lambari,2023-12-31T00:00:00,1.56,1.4894533157348633,-4.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lavras,2023-12-31T00:00:00,1.5,1.3664460182189941,-8.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Luminarias,2023-12-31T00:00:00,1.260224719101124,1.5100131034851074,19.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Machado,2023-12-31T00:00:00,1.500560931145702,1.5967814922332764,6.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Maria_da_Fe,2023-12-31T00:00:00,1.422222222222222,1.565781593322754,10.09,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Moeda,2023-12-31T00:00:00,1.0,1.4449714422225952,44.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monsenhor_Paulo,2023-12-31T00:00:00,1.389931972789116,1.5874614715576172,14.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Alegre_de_Minas,2023-12-31T00:00:00,2.338983050847458,2.742828130722046,17.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Formoso,2023-12-31T00:00:00,0.6000000000000001,0.5358765721321106,-10.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Siao,2023-12-31T00:00:00,1.5,2.4273784160614014,61.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Muzambinho,2023-12-31T00:00:00,1.68,1.4574247598648071,-13.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Natercia,2023-12-31T00:00:00,1.5,2.779888868331909,85.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nazareno,2023-12-31T00:00:00,1.5599056603773582,1.5973420143127441,2.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nepomuceno,2023-12-31T00:00:00,1.650045578851413,1.2815417051315308,-22.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Olimpio_Noronha,2023-12-31T00:00:00,1.2,1.4501757621765137,20.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Oliveira,2023-12-31T00:00:00,1.8240227434257288,1.6163568496704102,-11.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ouro_Fino,2023-12-31T00:00:00,1.68,1.7501158714294434,4.17,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraguacu,2023-12-31T00:00:00,1.298291457286432,1.423753261566162,9.66,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraisopolis,2023-12-31T00:00:00,1.2,1.8425108194351196,53.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pedralva,2023-12-31T00:00:00,1.67998417721519,1.590057373046875,-5.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Perdoes,2023-12-31T00:00:00,1.6799401197604789,1.4583641290664673,-13.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranga,2023-12-31T00:00:00,2.1,2.123805284500122,1.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pirangucu,2023-12-31T00:00:00,1.333333333333333,2.220494270324707,66.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranguinho,2023-12-31T00:00:00,1.460887949260042,1.4278466701507568,-2.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Poco_Fundo,2023-12-31T00:00:00,1.380160799652325,1.2695353031158447,-8.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pocos_de_Caldas,2023-12-31T00:00:00,1.5,1.491981863975525,-0.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alegre,2023-12-31T00:00:00,1.566666666666667,1.4756343364715576,-5.81,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alto,2023-12-31T00:00:00,1.8000000000000005,1.5225019454956055,-15.42,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Presidente_Bernardes,2023-12-31T00:00:00,1.320588235294118,1.5938818454742432,20.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ribeirao_Vermelho,2023-12-31T00:00:00,1.5,1.4114998579025269,-5.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Barbara_do_Monte_Verde,2023-12-31T00:00:00,1.4,1.521488904953003,8.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Rita_do_Sapucai,2023-12-31T00:00:00,1.3799999999999997,1.4458847045898438,4.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_da_Vargem,2023-12-31T00:00:00,1.5593593593593589,1.5166465044021606,-2.74,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_do_Jacare,2023-12-31T00:00:00,0.9011235955056182,1.4305609464645386,58.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Amparo,2023-12-31T00:00:00,1.8000000000000005,1.4762163162231445,-17.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Grama,2023-12-31T00:00:00,1.521739130434783,1.848900318145752,21.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Bento_Abade,2023-12-31T00:00:00,1.5,1.4171079397201538,-5.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Goncalo_do_Sapucai,2023-12-31T00:00:00,1.2,1.5979764461517334,33.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Joao_del_Rei,2023-12-31T00:00:00,1.799568965517241,1.655195951461792,-8.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Lourenco,2023-12-31T00:00:00,1.5,2.6605377197265625,77.37,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Roque_de_Minas,2023-12-31T00:00:00,1.8000000000000005,1.4084274768829346,-21.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_da_Bela_Vista,2023-12-31T00:00:00,1.5,1.4232691526412964,-5.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_do_Paraiso,2023-12-31T00:00:00,1.440017746228926,1.2912025451660156,-10.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tiago,2023-12-31T00:00:00,1.560526315789474,1.7020988464355469,9.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tomas_de_Aquino,2023-12-31T00:00:00,1.319968346082828,1.358074426651001,2.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tome_das_Letras,2023-12-31T00:00:00,1.09812734082397,1.6748008728027344,52.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senador_Jose_Bento,2023-12-31T00:00:00,1.2,1.4468764066696167,20.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senhora_de_Oliveira,2023-12-31T00:00:00,1.2,1.6337388753890991,36.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Serrania,2023-12-31T00:00:00,0.9,1.3804186582565308,53.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Silvianopolis,2023-12-31T00:00:00,1.13974358974359,1.1005994081497192,-3.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Soledade_de_Minas,2023-12-31T00:00:00,1.5,4.013644218444824,167.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Coracoes,2023-12-31T00:00:00,1.44,1.523097276687622,5.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Pontas,2023-12-31T00:00:00,1.5900000000000003,1.5615814924240112,-1.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Turvolandia,2023-12-31T00:00:00,1.259748427672956,1.4756840467453003,17.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Uberlandia,2023-12-31T00:00:00,1.8000000000000005,1.8856624364852905,4.76,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Vargem_Bonita,2023-12-31T00:00:00,1.487234042553192,1.476308822631836,-0.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Varginha,2023-12-31T00:00:00,1.679990280646337,1.440014362335205,-14.28,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Virginia,2023-12-31T00:00:00,1.5,1.941720962524414,29.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aguanil,2024-12-31T00:00:00,2.025039123630673,1.7354828119277954,-14.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aiuruoca,2024-12-31T00:00:00,1.8000000000000005,1.9615041017532349,8.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Albertina,2024-12-31T00:00:00,1.56,1.5763639211654663,1.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alfenas,2024-12-31T00:00:00,1.635197497066875,2.243652105331421,37.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alterosa,2024-12-31T00:00:00,1.110028116213683,1.3487861156463623,21.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andradas,2024-12-31T00:00:00,1.26,1.298614740371704,3.06,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andrelandia,2024-12-31T00:00:00,1.5,2.159245014190674,43.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Arceburgo,2024-12-31T00:00:00,1.7586206896551722,1.1219502687454224,-36.2,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Baependi,2024-12-31T00:00:00,1.439779005524862,1.4570038318634033,1.2,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bandeira_do_Sul,2024-12-31T00:00:00,1.5,2.0577871799468994,37.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Boa_Esperanca,2024-12-31T00:00:00,1.083893395133256,1.44428288936615,33.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bom_Sucesso,2024-12-31T00:00:00,1.6799426934097417,1.4229611158370972,-15.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Borda_da_Mata,2024-12-31T00:00:00,1.5,1.3214800357818604,-11.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Brazopolis,2024-12-31T00:00:00,1.501216545012166,1.6176142692565918,7.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bueno_Brandao,2024-12-31T00:00:00,1.8000000000000005,1.9466749429702759,8.15,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cachoeira_de_Minas,2024-12-31T00:00:00,1.73984375,1.6410863399505615,-5.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Caete,2024-12-31T00:00:00,1.8000000000000005,1.4508228302001953,-19.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Camacho,2024-12-31T00:00:00,1.5,1.658787727355957,10.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cambuquira,2024-12-31T00:00:00,1.7400000000000002,1.6707693338394165,-3.98,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campanha,2024-12-31T00:00:00,1.5689448441247,1.3194249868392944,-15.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campestre,2024-12-31T00:00:00,1.320032051282051,1.7363624572753906,31.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_Belo,2024-12-31T00:00:00,1.2,1.5178792476654053,26.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_do_Meio,2024-12-31T00:00:00,1.372340425531915,1.328741431236267,-3.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campos_Gerais,2024-12-31T00:00:00,1.611315789473684,1.5590214729309082,-3.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Candeias,2024-12-31T00:00:00,1.14003294892916,1.1617921590805054,1.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capetinga,2024-12-31T00:00:00,1.437691001697793,1.7921974658966064,24.66,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capitolio,2024-12-31T00:00:00,1.32,1.2573413848876953,-4.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_da_Cachoeira,2024-12-31T00:00:00,1.44,1.4600111246109009,1.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_de_Minas,2024-12-31T00:00:00,1.56,1.4920904636383057,-4.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_do_Rio_Claro,2024-12-31T00:00:00,1.7634782608695652,1.781855821609497,1.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carvalhopolis,2024-12-31T00:00:00,1.62,2.0728392601013184,27.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cassia,2024-12-31T00:00:00,1.7162790697674415,1.8571107387542725,8.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_da_Barra_de_Minas,2024-12-31T00:00:00,1.8000000000000005,1.4820911884307861,-17.66,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_das_Pedras,2024-12-31T00:00:00,1.5598885793871868,1.5813076496124268,1.37,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_do_Rio_Verde,2024-12-31T00:00:00,1.5,1.551936388015747,3.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_dos_Ouros,2024-12-31T00:00:00,1.8000000000000005,1.365939736366272,-24.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Coqueiral,2024-12-31T00:00:00,0.9,1.3565651178359985,50.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cordislandia,2024-12-31T00:00:00,1.439887640449438,1.3299238681793213,-7.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Corrego_Danta,2024-12-31T00:00:00,1.5,1.3003592491149902,-13.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristais,2024-12-31T00:00:00,1.439664804469274,1.2907345294952393,-10.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristina,2024-12-31T00:00:00,1.619736842105263,1.6252682209014893,0.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Delfinopolis,2024-12-31T00:00:00,1.5,2.008089065551758,33.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Desterro_de_Entre_Rios,2024-12-31T00:00:00,1.3780487804878048,1.789741039276123,29.88,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divinesia,2024-12-31T00:00:00,1.316666666666667,2.255891799926758,71.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divisa_Nova,2024-12-31T00:00:00,1.679831932773109,2.083073377609253,24.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Dom_Vicoso,2024-12-31T00:00:00,1.2,1.5106475353240967,25.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Doresopolis,2024-12-31T00:00:00,1.5615384615384622,1.3964178562164307,-10.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Eloi_Mendes,2024-12-31T00:00:00,1.421623345558922,2.260422706604004,59.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Entre_Rios_de_Minas,2024-12-31T00:00:00,1.88,1.950392246246338,3.74,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Fama,2024-12-31T00:00:00,1.679838709677419,1.5853420495986938,-5.63,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Grao_Mogol,2024-12-31T00:00:00,2.0333333333333337,1.1618201732635498,-42.86,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Guape,2024-12-31T00:00:00,1.7459340659340663,1.434865951538086,-17.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Heliodora,2024-12-31T00:00:00,1.5601255886970171,1.521967887878418,-2.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ibituruna,2024-12-31T00:00:00,1.8000000000000005,1.813439130783081,0.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ijaci,2024-12-31T00:00:00,2.19,1.8336952924728394,-16.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ilicinea,2024-12-31T00:00:00,1.5,2.024885654449463,34.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Inconfidentes,2024-12-31T00:00:00,1.8000000000000005,2.2417242527008057,24.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ingai,2024-12-31T00:00:00,1.621004566210046,1.5340797901153564,-5.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itajuba,2024-12-31T00:00:00,1.068965517241379,1.2210471630096436,14.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itamogi,2024-12-31T00:00:00,1.44,2.1561245918273926,49.73,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itapecerica,2024-12-31T00:00:00,1.615189873417721,1.1596068143844604,-28.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itumirim,2024-12-31T00:00:00,1.62116991643454,1.414079189300537,-12.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itutinga,2024-12-31T00:00:00,1.8000000000000005,2.0224900245666504,12.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jacutinga,2024-12-31T00:00:00,1.319874804381847,1.498155117034912,13.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jesuania,2024-12-31T00:00:00,1.5,1.2582261562347412,-16.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lambari,2024-12-31T00:00:00,1.56,1.4771771430969238,-5.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lavras,2024-12-31T00:00:00,1.55996015936255,1.4985606670379639,-3.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Luminarias,2024-12-31T00:00:00,1.380229885057471,1.5500648021697998,12.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Machado,2024-12-31T00:00:00,1.3347736360118658,2.222001075744629,66.47,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Maria_da_Fe,2024-12-31T00:00:00,1.477777777777778,1.4106419086456299,-4.54,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Moeda,2024-12-31T00:00:00,1.5,1.7056376934051514,13.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monsenhor_Paulo,2024-12-31T00:00:00,1.5899728997289972,1.481735110282898,-6.81,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Alegre_de_Minas,2024-12-31T00:00:00,2.378787878787879,2.797673463821411,17.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Formoso,2024-12-31T00:00:00,0.6800000000000002,0.582343339920044,-14.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Siao,2024-12-31T00:00:00,1.5,1.9133920669555664,27.56,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Muzambinho,2024-12-31T00:00:00,1.26,2.472846508026123,96.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Natercia,2024-12-31T00:00:00,1.8000000000000005,1.5055954456329346,-16.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nazareno,2024-12-31T00:00:00,1.619811320754717,1.5858664512634277,-2.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nepomuceno,2024-12-31T00:00:00,1.3800383877159308,1.6404528617858887,18.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Olimpio_Noronha,2024-12-31T00:00:00,1.5,1.2001638412475586,-19.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Oliveira,2024-12-31T00:00:00,1.8240227434257288,1.9261890649795532,5.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ouro_Fino,2024-12-31T00:00:00,1.5,2.3934497833251953,59.56,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraguacu,2024-12-31T00:00:00,1.323316582914573,1.5758639574050903,19.08,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraisopolis,2024-12-31T00:00:00,1.5066666666666668,1.3954882621765137,-7.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pedralva,2024-12-31T00:00:00,1.5600790513833993,1.5164875984191895,-2.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Perdoes,2024-12-31T00:00:00,1.6799401197604789,1.5574095249176025,-7.29,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranga,2024-12-31T00:00:00,1.8000000000000005,2.06821870803833,14.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pirangucu,2024-12-31T00:00:00,1.333333333333333,157.71234130859375,11728.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranguinho,2024-12-31T00:00:00,1.6807610993657498,0.8732947707176208,-48.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Poco_Fundo,2024-12-31T00:00:00,1.380192991366176,1.7832801342010498,29.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pocos_de_Caldas,2024-12-31T00:00:00,1.7400000000000002,1.5699386596679688,-9.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alegre,2024-12-31T00:00:00,1.7878787878787883,2.038188934326172,14.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alto,2024-12-31T00:00:00,1.53,1.7899627685546875,16.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Presidente_Bernardes,2024-12-31T00:00:00,1.110144927536232,1.9220151901245117,73.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ribeirao_Vermelho,2024-12-31T00:00:00,1.43859649122807,1.4366841316223145,-0.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Barbara_do_Monte_Verde,2024-12-31T00:00:00,1.3,1.4027643203735352,7.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Rita_do_Sapucai,2024-12-31T00:00:00,1.5,1.7066476345062256,13.78,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_da_Vargem,2024-12-31T00:00:00,1.44,1.3609158992767334,-5.49,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_do_Jacare,2024-12-31T00:00:00,1.2,1.334962010383606,11.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Amparo,2024-12-31T00:00:00,1.68,1.4244287014007568,-15.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Grama,2024-12-31T00:00:00,1.565217391304348,1.314424991607666,-16.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Bento_Abade,2024-12-31T00:00:00,1.5,1.637521505355835,9.17,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Goncalo_do_Sapucai,2024-12-31T00:00:00,1.5,1.5402698516845703,2.68,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Joao_del_Rei,2024-12-31T00:00:00,1.80168776371308,2.2891030311584473,27.05,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Lourenco,2024-12-31T00:00:00,1.3799999999999997,1.8378766775131226,33.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Roque_de_Minas,2024-12-31T00:00:00,1.58,1.316670298576355,-16.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_da_Bela_Vista,2024-12-31T00:00:00,1.44,1.819161295890808,26.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_do_Paraiso,2024-12-31T00:00:00,1.26,1.6066291332244873,27.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tiago,2024-12-31T00:00:00,1.621052631578947,1.323681354522705,-18.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tomas_de_Aquino,2024-12-31T00:00:00,1.619975932611312,1.3572938442230225,-16.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tome_das_Letras,2024-12-31T00:00:00,2.1,1.6440339088439941,-21.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senador_Jose_Bento,2024-12-31T00:00:00,0.9,1.4446625709533691,60.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senhora_de_Oliveira,2024-12-31T00:00:00,1.8000000000000005,1.6496764421463013,-8.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Serrania,2024-12-31T00:00:00,1.32,1.5786724090576172,19.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Silvianopolis,2024-12-31T00:00:00,1.56025641025641,1.338832139968872,-14.19,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Soledade_de_Minas,2024-12-31T00:00:00,1.5,1.6021069288253784,6.81,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Coracoes,2024-12-31T00:00:00,1.56,1.6788854598999023,7.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Pontas,2024-12-31T00:00:00,1.387228956803825,1.331913948059082,-3.99,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Turvolandia,2024-12-31T00:00:00,1.44,1.6169670820236206,12.29,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Uberlandia,2024-12-31T00:00:00,1.919565217391304,1.8397274017333984,-4.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Vargem_Bonita,2024-12-31T00:00:00,2.187804878048781,1.4721275568008423,-32.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Varginha,2024-12-31T00:00:00,1.3799999999999997,1.9048833847045898,38.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Virginia,2024-12-31T00:00:00,1.6599999999999997,1.7554346323013306,5.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aguanil,2025-12-31T00:00:00,0.0,2.259641170501709,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Aiuruoca,2025-12-31T00:00:00,0.0,1.9453972578048706,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Albertina,2025-12-31T00:00:00,0.0,1.6208289861679077,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alfenas,2025-12-31T00:00:00,0.0,1.9045863151550293,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Alterosa,2025-12-31T00:00:00,0.0,1.176208257675171,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andradas,2025-12-31T00:00:00,0.0,1.2965600490570068,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Andrelandia,2025-12-31T00:00:00,0.0,1.719443440437317,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Arceburgo,2025-12-31T00:00:00,0.0,1.604770541191101,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Baependi,2025-12-31T00:00:00,0.0,1.4120540618896484,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bandeira_do_Sul,2025-12-31T00:00:00,0.0,1.8263192176818848,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Boa_Esperanca,2025-12-31T00:00:00,0.0,1.1551928520202637,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bom_Sucesso,2025-12-31T00:00:00,0.0,1.6228822469711304,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Borda_da_Mata,2025-12-31T00:00:00,0.0,1.6977087259292603,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Brazopolis,2025-12-31T00:00:00,0.0,1.3753414154052734,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Bueno_Brandao,2025-12-31T00:00:00,0.0,2.2111878395080566,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cachoeira_de_Minas,2025-12-31T00:00:00,0.0,1.6625628471374512,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Caete,2025-12-31T00:00:00,0.0,1.4226129055023193,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Camacho,2025-12-31T00:00:00,0.0,1.5746396780014038,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cambuquira,2025-12-31T00:00:00,0.0,1.7552151679992676,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campanha,2025-12-31T00:00:00,0.0,1.6723542213439941,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campestre,2025-12-31T00:00:00,0.0,1.561266541481018,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_Belo,2025-12-31T00:00:00,0.0,1.3625023365020752,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campo_do_Meio,2025-12-31T00:00:00,0.0,1.8586217164993286,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Campos_Gerais,2025-12-31T00:00:00,0.0,1.7938588857650757,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Candeias,2025-12-31T00:00:00,0.0,1.1194511651992798,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capetinga,2025-12-31T00:00:00,0.0,1.4536322355270386,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Capitolio,2025-12-31T00:00:00,0.0,1.3400123119354248,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_da_Cachoeira,2025-12-31T00:00:00,0.0,1.5999815464019775,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_de_Minas,2025-12-31T00:00:00,0.0,1.6369400024414062,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carmo_do_Rio_Claro,2025-12-31T00:00:00,0.0,1.7853144407272339,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Carvalhopolis,2025-12-31T00:00:00,0.0,1.6322879791259766,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cassia,2025-12-31T00:00:00,0.0,1.9716055393218994,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_da_Barra_de_Minas,2025-12-31T00:00:00,0.0,2.1353490352630615,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_das_Pedras,2025-12-31T00:00:00,0.0,1.5419858694076538,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_do_Rio_Verde,2025-12-31T00:00:00,0.0,1.516248106956482,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Conceicao_dos_Ouros,2025-12-31T00:00:00,0.0,1.7482225894927979,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Coqueiral,2025-12-31T00:00:00,0.0,1.3538943529129028,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cordislandia,2025-12-31T00:00:00,0.0,1.603943943977356,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Corrego_Danta,2025-12-31T00:00:00,0.0,1.552788496017456,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristais,2025-12-31T00:00:00,0.0,1.5475716590881348,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Cristina,2025-12-31T00:00:00,0.0,1.7212519645690918,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Delfinopolis,2025-12-31T00:00:00,0.0,1.6005247831344604,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Desterro_de_Entre_Rios,2025-12-31T00:00:00,0.0,1.4361755847930908,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divinesia,2025-12-31T00:00:00,0.0,1.940748691558838,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Divisa_Nova,2025-12-31T00:00:00,0.0,1.8396120071411133,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Dom_Vicoso,2025-12-31T00:00:00,0.0,1.3110275268554688,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Doresopolis,2025-12-31T00:00:00,0.0,1.6958558559417725,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Eloi_Mendes,2025-12-31T00:00:00,0.0,1.5418939590454102,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Entre_Rios_de_Minas,2025-12-31T00:00:00,0.0,1.6882637739181519,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Fama,2025-12-31T00:00:00,0.0,1.8820509910583496,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Grao_Mogol,2025-12-31T00:00:00,0.0,1.8739432096481323,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Guape,2025-12-31T00:00:00,0.0,1.9066184759140015,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Heliodora,2025-12-31T00:00:00,0.0,1.704085111618042,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ibituruna,2025-12-31T00:00:00,0.0,2.165142059326172,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ijaci,2025-12-31T00:00:00,0.0,3.4634077548980713,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ilicinea,2025-12-31T00:00:00,0.0,1.3152295351028442,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Inconfidentes,2025-12-31T00:00:00,0.0,1.8566210269927979,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ingai,2025-12-31T00:00:00,0.0,1.7871605157852173,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itajuba,2025-12-31T00:00:00,0.0,1.2343043088912964,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itamogi,2025-12-31T00:00:00,0.0,1.717977523803711,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itapecerica,2025-12-31T00:00:00,0.0,1.7950477600097656,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itumirim,2025-12-31T00:00:00,0.0,1.670776605606079,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Itutinga,2025-12-31T00:00:00,0.0,2.0412371158599854,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jacutinga,2025-12-31T00:00:00,0.0,1.4787373542785645,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Jesuania,2025-12-31T00:00:00,0.0,1.6002501249313354,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lambari,2025-12-31T00:00:00,0.0,1.62447190284729,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Lavras,2025-12-31T00:00:00,0.0,1.5193758010864258,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Luminarias,2025-12-31T00:00:00,0.0,1.3613274097442627,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Machado,2025-12-31T00:00:00,0.0,1.332106590270996,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Maria_da_Fe,2025-12-31T00:00:00,0.0,1.5143225193023682,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Moeda,2025-12-31T00:00:00,0.0,1.1905887126922607,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monsenhor_Paulo,2025-12-31T00:00:00,0.0,1.7005541324615479,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Alegre_de_Minas,2025-12-31T00:00:00,0.0,2.394409656524658,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Formoso,2025-12-31T00:00:00,0.0,0.6798694133758545,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Monte_Siao,2025-12-31T00:00:00,0.0,1.5316530466079712,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Muzambinho,2025-12-31T00:00:00,0.0,1.985658884048462,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Natercia,2025-12-31T00:00:00,0.0,1.9181429147720337,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nazareno,2025-12-31T00:00:00,0.0,1.7198050022125244,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Nepomuceno,2025-12-31T00:00:00,0.0,2.2216782569885254,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Olimpio_Noronha,2025-12-31T00:00:00,0.0,1.6065998077392578,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Oliveira,2025-12-31T00:00:00,0.0,1.7255991697311401,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ouro_Fino,2025-12-31T00:00:00,0.0,1.9242421388626099,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraguacu,2025-12-31T00:00:00,0.0,1.2918003797531128,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Paraisopolis,2025-12-31T00:00:00,0.0,1.4159480333328247,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pedralva,2025-12-31T00:00:00,0.0,1.4594532251358032,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Perdoes,2025-12-31T00:00:00,0.0,1.9303227663040161,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranga,2025-12-31T00:00:00,0.0,1.8346956968307495,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pirangucu,2025-12-31T00:00:00,0.0,12.126218795776367,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Piranguinho,2025-12-31T00:00:00,0.0,1.7616418600082397,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Poco_Fundo,2025-12-31T00:00:00,0.0,1.3809268474578857,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pocos_de_Caldas,2025-12-31T00:00:00,0.0,1.6965467929840088,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alegre,2025-12-31T00:00:00,0.0,1.8757692575454712,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Pouso_Alto,2025-12-31T00:00:00,0.0,1.6045176982879639,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Presidente_Bernardes,2025-12-31T00:00:00,0.0,1.5605003833770752,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Ribeirao_Vermelho,2025-12-31T00:00:00,0.0,1.5185747146606445,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Barbara_do_Monte_Verde,2025-12-31T00:00:00,0.0,1.336195707321167,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santa_Rita_do_Sapucai,2025-12-31T00:00:00,0.0,1.4330453872680664,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_da_Vargem,2025-12-31T00:00:00,0.0,2.028367280960083,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santana_do_Jacare,2025-12-31T00:00:00,0.0,1.5879318714141846,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Amparo,2025-12-31T00:00:00,0.0,1.5999610424041748,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Santo_Antonio_do_Grama,2025-12-31T00:00:00,0.0,1.5418217182159424,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Bento_Abade,2025-12-31T00:00:00,0.0,1.454239845275879,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Goncalo_do_Sapucai,2025-12-31T00:00:00,0.0,1.709244966506958,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Joao_del_Rei,2025-12-31T00:00:00,0.0,1.8423000574111938,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Lourenco,2025-12-31T00:00:00,0.0,1.827489972114563,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Roque_de_Minas,2025-12-31T00:00:00,0.0,1.551810622215271,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_da_Bela_Vista,2025-12-31T00:00:00,0.0,1.5081326961517334,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Sebastiao_do_Paraiso,2025-12-31T00:00:00,0.0,1.5305832624435425,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tiago,2025-12-31T00:00:00,0.0,1.4992907047271729,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tomas_de_Aquino,2025-12-31T00:00:00,0.0,2.0541915893554688,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Sao_Tome_das_Letras,2025-12-31T00:00:00,0.0,2.05130934715271,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senador_Jose_Bento,2025-12-31T00:00:00,0.0,1.3084049224853516,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Senhora_de_Oliveira,2025-12-31T00:00:00,0.0,1.3936513662338257,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Serrania,2025-12-31T00:00:00,0.0,1.640437364578247,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Silvianopolis,2025-12-31T00:00:00,0.0,1.9484293460845947,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Soledade_de_Minas,2025-12-31T00:00:00,0.0,4.419686317443848,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Coracoes,2025-12-31T00:00:00,0.0,1.5594741106033325,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Tres_Pontas,2025-12-31T00:00:00,0.0,2.2925896644592285,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Turvolandia,2025-12-31T00:00:00,0.0,1.6185970306396484,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Uberlandia,2025-12-31T00:00:00,0.0,2.2204813957214355,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Vargem_Bonita,2025-12-31T00:00:00,0.0,2.5654964447021484,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Varginha,2025-12-31T00:00:00,0.0,1.567037582397461,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 3 (2020-225),Virginia,2025-12-31T00:00:00,0.0,1.7251241207122803,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0005337330841133377
encoder_hidden_size: 64
decoder_layers: 1
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.01
steps: 1000
",2025-09-23T22:59:26
cluster 4 (2020-225),Acucena,2012-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Acucena,2013-12-31T00:00:00,1.14,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Acucena,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Acucena,2015-12-31T00:00:00,1.2,1.0222727060317993,14.81,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Acucena,2016-12-31T00:00:00,1.2,1.112684965133667,7.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Acucena,2017-12-31T00:00:00,0.77,1.1026482582092285,43.2,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Acucena,2018-12-31T00:00:00,0.71,0.7479362487792969,5.34,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Araguari,2012-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Araguari,2013-12-31T00:00:00,1.92,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Araguari,2014-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Araguari,2015-12-31T00:00:00,1.68,1.5959992408752441,5.0,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Araguari,2016-12-31T00:00:00,2.58,2.5894522666931152,0.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Araguari,2017-12-31T00:00:00,2.76,2.357536792755127,14.58,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Araguari,2018-12-31T00:00:00,2.64,2.80525541305542,6.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bambui,2012-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bambui,2013-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bambui,2014-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bambui,2015-12-31T00:00:00,1.44,1.3598260879516602,5.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bambui,2016-12-31T00:00:00,1.8,1.833954095840454,1.89,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bambui,2017-12-31T00:00:00,1.5,1.6558849811553955,10.39,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bambui,2018-12-31T00:00:00,1.8,1.9442633390426636,8.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bonfinopolis_de_Minas,2012-12-31T00:00:00,2.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bonfinopolis_de_Minas,2013-12-31T00:00:00,2.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bonfinopolis_de_Minas,2014-12-31T00:00:00,2.64,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bonfinopolis_de_Minas,2015-12-31T00:00:00,1.8,1.4638030529022217,18.68,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bonfinopolis_de_Minas,2016-12-31T00:00:00,3.6,2.4343156814575195,32.38,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bonfinopolis_de_Minas,2017-12-31T00:00:00,2.0,1.7231799364089966,13.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bonfinopolis_de_Minas,2018-12-31T00:00:00,2.4,2.9735424518585205,23.9,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritis,2012-12-31T00:00:00,3.3,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritis,2013-12-31T00:00:00,3.3,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritis,2014-12-31T00:00:00,3.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritis,2015-12-31T00:00:00,1.92,2.911881923675537,51.66,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritis,2016-12-31T00:00:00,3.6,3.7239456176757812,3.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritis,2017-12-31T00:00:00,1.37,1.3368593454360962,2.42,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritis,2018-12-31T00:00:00,2.4,2.403837203979492,0.16,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritizeiro,2012-12-31T00:00:00,3.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritizeiro,2013-12-31T00:00:00,3.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritizeiro,2014-12-31T00:00:00,3.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritizeiro,2015-12-31T00:00:00,1.92,2.377516508102417,23.83,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritizeiro,2016-12-31T00:00:00,2.1,2.8626346588134766,36.32,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritizeiro,2017-12-31T00:00:00,1.97,1.862395167350769,5.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritizeiro,2018-12-31T00:00:00,2.1,1.9871711730957031,5.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Campos_Altos,2012-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Campos_Altos,2013-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Campos_Altos,2014-12-31T00:00:00,1.62,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Campos_Altos,2015-12-31T00:00:00,1.71,1.6177403926849365,5.4,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Campos_Altos,2016-12-31T00:00:00,1.92,1.7572264671325684,8.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Campos_Altos,2017-12-31T00:00:00,1.99,1.7636778354644775,11.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Campos_Altos,2018-12-31T00:00:00,1.92,2.040491819381714,6.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cana_Verde,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cana_Verde,2013-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cana_Verde,2014-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cana_Verde,2015-12-31T00:00:00,1.2,1.121680736541748,6.53,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cana_Verde,2016-12-31T00:00:00,1.56,1.6038029193878174,2.81,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cana_Verde,2017-12-31T00:00:00,1.34,1.5959899425506592,19.1,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cana_Verde,2018-12-31T00:00:00,1.5,1.5439236164093018,2.93,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Capitao_Eneas,2012-12-31T00:00:00,3.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Capitao_Eneas,2013-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Capitao_Eneas,2014-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Capitao_Eneas,2015-12-31T00:00:00,1.8,1.8376798629760742,2.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Capitao_Eneas,2016-12-31T00:00:00,2.1,2.3194451332092285,10.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Capitao_Eneas,2017-12-31T00:00:00,0.5,1.490874171257019,198.17,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Capitao_Eneas,2018-12-31T00:00:00,0.65,0.24393290281295776,62.47,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Careacu,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Careacu,2013-12-31T00:00:00,1.38,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Careacu,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Careacu,2015-12-31T00:00:00,1.2,1.2356798648834229,2.97,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Careacu,2016-12-31T00:00:00,1.5,1.331763744354248,11.22,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Careacu,2017-12-31T00:00:00,1.67,1.6783030033111572,0.5,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Careacu,2018-12-31T00:00:00,1.38,1.392539143562317,0.91,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmo_do_Paranaiba,2012-12-31T00:00:00,2.11,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmo_do_Paranaiba,2013-12-31T00:00:00,1.92,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmo_do_Paranaiba,2014-12-31T00:00:00,1.93,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmo_do_Paranaiba,2015-12-31T00:00:00,1.89,1.8640140295028687,1.37,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmo_do_Paranaiba,2016-12-31T00:00:00,2.37,1.9296185970306396,18.58,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmo_do_Paranaiba,2017-12-31T00:00:00,1.75,1.7021937370300293,2.73,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmo_do_Paranaiba,2018-12-31T00:00:00,2.21,2.193413257598877,0.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmopolis_de_Minas,2012-12-31T00:00:00,1.98,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmopolis_de_Minas,2013-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmopolis_de_Minas,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmopolis_de_Minas,2015-12-31T00:00:00,1.2,1.2038521766662598,0.32,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmopolis_de_Minas,2016-12-31T00:00:00,1.5,1.4737470149993896,1.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmopolis_de_Minas,2017-12-31T00:00:00,1.87,1.668889045715332,10.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmopolis_de_Minas,2018-12-31T00:00:00,1.8,1.7770663499832153,1.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cascalho_Rico,2012-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cascalho_Rico,2013-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cascalho_Rico,2014-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cascalho_Rico,2015-12-31T00:00:00,1.98,1.93558931350708,2.24,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cascalho_Rico,2016-12-31T00:00:00,2.1,2.24497127532959,6.9,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cascalho_Rico,2017-12-31T00:00:00,1.69,2.0551981925964355,21.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cascalho_Rico,2018-12-31T00:00:00,2.1,2.152863025665283,2.52,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Claraval,2012-12-31T00:00:00,2.55,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Claraval,2013-12-31T00:00:00,1.42,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Claraval,2014-12-31T00:00:00,2.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Claraval,2015-12-31T00:00:00,1.02,0.6008418798446655,41.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Claraval,2016-12-31T00:00:00,2.34,2.8885104656219482,23.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Claraval,2017-12-31T00:00:00,1.22,1.2667821645736694,3.83,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Claraval,2018-12-31T00:00:00,2.55,2.5381226539611816,0.47,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Congonhas_do_Norte,2012-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Congonhas_do_Norte,2013-12-31T00:00:00,1.51,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Congonhas_do_Norte,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Congonhas_do_Norte,2015-12-31T00:00:00,1.26,1.2504937648773193,0.75,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Congonhas_do_Norte,2016-12-31T00:00:00,1.5,1.4004848003387451,6.63,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Congonhas_do_Norte,2017-12-31T00:00:00,0.69,1.056150197982788,53.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Congonhas_do_Norte,2018-12-31T00:00:00,0.9,0.9919994473457336,10.22,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Indaia,2012-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Indaia,2013-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Indaia,2014-12-31T00:00:00,1.7,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Indaia,2015-12-31T00:00:00,1.5,1.626732587814331,8.45,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Indaia,2016-12-31T00:00:00,1.8,1.808557391166687,0.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Indaia,2017-12-31T00:00:00,1.2,1.4898278713226318,24.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Indaia,2018-12-31T00:00:00,1.8,1.8078573942184448,0.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Sul,2012-12-31T00:00:00,2.22,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Sul,2013-12-31T00:00:00,1.92,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Sul,2014-12-31T00:00:00,1.98,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Sul,2015-12-31T00:00:00,1.26,1.4532949924468994,15.34,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Sul,2016-12-31T00:00:00,2.52,2.7111568450927734,7.59,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Sul,2017-12-31T00:00:00,2.38,1.4813786745071411,37.76,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Sul,2018-12-31T00:00:00,2.22,2.670342445373535,20.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Frei_Gaspar,2012-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Frei_Gaspar,2013-12-31T00:00:00,1.08,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Frei_Gaspar,2014-12-31T00:00:00,0.9,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Frei_Gaspar,2015-12-31T00:00:00,1.14,1.1581202745437622,1.59,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Frei_Gaspar,2016-12-31T00:00:00,1.2,1.0936235189437866,8.86,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Frei_Gaspar,2017-12-31T00:00:00,0.39,0.7882266640663147,102.11,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Frei_Gaspar,2018-12-31T00:00:00,0.6,0.6230238676071167,3.84,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibia,2012-12-31T00:00:00,1.74,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibia,2013-12-31T00:00:00,1.74,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibia,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibia,2015-12-31T00:00:00,1.5,1.4334924221038818,4.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibia,2016-12-31T00:00:00,1.8,1.8550808429718018,3.06,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibia,2017-12-31T00:00:00,1.59,1.4903030395507812,6.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibia,2018-12-31T00:00:00,1.8,1.8167517185211182,0.93,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibiraci,2012-12-31T00:00:00,2.49,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibiraci,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibiraci,2014-12-31T00:00:00,2.01,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibiraci,2015-12-31T00:00:00,1.08,1.1337213516235352,4.97,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibiraci,2016-12-31T00:00:00,2.4,2.3812811374664307,0.78,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibiraci,2017-12-31T00:00:00,1.61,1.6609418392181396,3.16,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibiraci,2018-12-31T00:00:00,3.0,2.8419201374053955,5.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Indianopolis,2012-12-31T00:00:00,2.4,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Indianopolis,2013-12-31T00:00:00,1.92,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Indianopolis,2014-12-31T00:00:00,2.45,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Indianopolis,2015-12-31T00:00:00,1.68,1.602107048034668,4.64,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Indianopolis,2016-12-31T00:00:00,2.7,2.821105718612671,4.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Indianopolis,2017-12-31T00:00:00,1.67,1.809497356414795,8.35,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Indianopolis,2018-12-31T00:00:00,2.52,2.7927956581115723,10.83,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Itacambira,2012-12-31T00:00:00,3.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Itacambira,2013-12-31T00:00:00,3.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Itacambira,2014-12-31T00:00:00,3.6,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Itacambira,2015-12-31T00:00:00,1.8,0.33264365792274475,81.52,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Itacambira,2016-12-31T00:00:00,6.0,3.2241404056549072,46.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Itacambira,2017-12-31T00:00:00,1.8,1.9144160747528076,6.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Itacambira,2018-12-31T00:00:00,0.67,3.7314321994781494,456.93,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ituiutaba,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ituiutaba,2013-12-31T00:00:00,1.67,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ituiutaba,2014-12-31T00:00:00,0.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ituiutaba,2015-12-31T00:00:00,1.33,1.1552486419677734,13.14,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ituiutaba,2016-12-31T00:00:00,2.0,2.0052390098571777,0.26,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ituiutaba,2017-12-31T00:00:00,2.0,1.670300006866455,16.48,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ituiutaba,2018-12-31T00:00:00,0.77,1.9516745805740356,153.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Joao_Pinheiro,2012-12-31T00:00:00,2.82,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Joao_Pinheiro,2013-12-31T00:00:00,3.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Joao_Pinheiro,2014-12-31T00:00:00,2.7,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Joao_Pinheiro,2015-12-31T00:00:00,2.4,2.3522961139678955,1.99,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Joao_Pinheiro,2016-12-31T00:00:00,3.6,3.3291409015655518,7.52,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Joao_Pinheiro,2017-12-31T00:00:00,1.91,2.1244213581085205,11.23,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Joao_Pinheiro,2018-12-31T00:00:00,2.4,3.185330867767334,32.72,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Medeiros,2012-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Medeiros,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Medeiros,2014-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Medeiros,2015-12-31T00:00:00,1.5,1.4098215103149414,6.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Medeiros,2016-12-31T00:00:00,1.92,1.9407293796539307,1.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Medeiros,2017-12-31T00:00:00,1.79,1.7473280429840088,2.38,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Medeiros,2018-12-31T00:00:00,2.04,2.046100616455078,0.3,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Monte_Carmelo,2012-12-31T00:00:00,2.34,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Monte_Carmelo,2013-12-31T00:00:00,1.89,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Monte_Carmelo,2014-12-31T00:00:00,2.28,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Monte_Carmelo,2015-12-31T00:00:00,1.32,1.3133624792099,0.5,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Monte_Carmelo,2016-12-31T00:00:00,2.64,3.0626184940338135,16.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Monte_Carmelo,2017-12-31T00:00:00,1.84,1.4939631223678589,18.81,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Monte_Carmelo,2018-12-31T00:00:00,2.31,2.496687412261963,8.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ninheira,2012-12-31T00:00:00,3.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ninheira,2013-12-31T00:00:00,3.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ninheira,2014-12-31T00:00:00,3.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ninheira,2015-12-31T00:00:00,2.76,1.762190341949463,36.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ninheira,2016-12-31T00:00:00,3.0,3.0069315433502197,0.23,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ninheira,2017-12-31T00:00:00,2.34,2.6222524642944336,12.06,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ninheira,2018-12-31T00:00:00,3.0,2.969139575958252,1.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Nova_Era,2012-12-31T00:00:00,2.27,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Nova_Era,2013-12-31T00:00:00,2.27,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Nova_Era,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Nova_Era,2015-12-31T00:00:00,1.2,1.132339358329773,5.64,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Nova_Era,2016-12-31T00:00:00,1.2,1.847083568572998,53.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Nova_Era,2017-12-31T00:00:00,1.67,1.2792747020721436,23.4,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Nova_Era,2018-12-31T00:00:00,1.75,1.7249248027801514,1.43,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Padre_Paraiso,2012-12-31T00:00:00,0.78,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Padre_Paraiso,2013-12-31T00:00:00,0.78,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Padre_Paraiso,2014-12-31T00:00:00,0.72,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Padre_Paraiso,2015-12-31T00:00:00,0.84,0.7774941921234131,7.44,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Padre_Paraiso,2016-12-31T00:00:00,0.84,0.8411972522735596,0.14,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Padre_Paraiso,2017-12-31T00:00:00,0.6,0.6675232648849487,11.25,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Padre_Paraiso,2018-12-31T00:00:00,0.6,0.6087857484817505,1.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Paracatu,2012-12-31T00:00:00,3.45,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Paracatu,2013-12-31T00:00:00,3.45,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Paracatu,2014-12-31T00:00:00,3.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Paracatu,2015-12-31T00:00:00,1.8,2.4168384075164795,34.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Paracatu,2016-12-31T00:00:00,2.4,3.0790650844573975,28.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Paracatu,2017-12-31T00:00:00,2.4,2.19069766998291,8.72,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Paracatu,2018-12-31T00:00:00,3.0,2.4578869342803955,18.07,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Passa_Tempo,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Passa_Tempo,2013-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Passa_Tempo,2014-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Passa_Tempo,2015-12-31T00:00:00,1.32,1.306443452835083,1.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Passa_Tempo,2016-12-31T00:00:00,1.32,1.5425156354904175,16.86,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Passa_Tempo,2017-12-31T00:00:00,1.71,1.5120670795440674,11.58,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Passa_Tempo,2018-12-31T00:00:00,1.8,1.699258804321289,5.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Patrocinio,2012-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Patrocinio,2013-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Patrocinio,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Patrocinio,2015-12-31T00:00:00,1.25,1.3023253679275513,4.19,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Patrocinio,2016-12-31T00:00:00,2.64,2.3989384174346924,9.13,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Patrocinio,2017-12-31T00:00:00,1.45,1.3815125226974487,4.72,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Patrocinio,2018-12-31T00:00:00,2.21,2.290869951248169,3.66,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pedrinopolis,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pedrinopolis,2013-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pedrinopolis,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pedrinopolis,2015-12-31T00:00:00,1.68,1.475187063217163,12.19,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pedrinopolis,2016-12-31T00:00:00,1.8,1.840893268585205,2.27,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pedrinopolis,2017-12-31T00:00:00,0.42,1.706153154373169,306.23,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pedrinopolis,2018-12-31T00:00:00,1.2,2.1696391105651855,80.8,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Perdizes,2012-12-31T00:00:00,1.68,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Perdizes,2013-12-31T00:00:00,1.56,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Perdizes,2014-12-31T00:00:00,1.66,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Perdizes,2015-12-31T00:00:00,1.48,1.4660948514938354,0.94,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Perdizes,2016-12-31T00:00:00,1.98,1.7151422500610352,13.38,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Perdizes,2017-12-31T00:00:00,2.07,1.5238606929779053,26.38,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Perdizes,2018-12-31T00:00:00,2.1,2.1111302375793457,0.53,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pimenta,2012-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pimenta,2013-12-31T00:00:00,1.74,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pimenta,2014-12-31T00:00:00,1.62,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pimenta,2015-12-31T00:00:00,1.5,1.6739823818206787,11.6,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pimenta,2016-12-31T00:00:00,1.62,1.6248624324798584,0.3,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pimenta,2017-12-31T00:00:00,1.73,1.6376614570617676,5.34,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pimenta,2018-12-31T00:00:00,2.7,1.766000509262085,34.59,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pirapora,2012-12-31T00:00:00,4.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pirapora,2013-12-31T00:00:00,4.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pirapora,2014-12-31T00:00:00,4.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pirapora,2015-12-31T00:00:00,3.0,3.038334846496582,1.28,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pirapora,2016-12-31T00:00:00,3.0,3.866276979446411,28.88,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pirapora,2017-12-31T00:00:00,1.94,2.843154191970825,46.55,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pirapora,2018-12-31T00:00:00,2.4,2.4385600090026855,1.61,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Piumhi,2012-12-31T00:00:00,1.44,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Piumhi,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Piumhi,2014-12-31T00:00:00,1.2,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Piumhi,2015-12-31T00:00:00,1.2,1.2317860126495361,2.65,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Piumhi,2016-12-31T00:00:00,1.8,1.614790439605713,10.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Piumhi,2017-12-31T00:00:00,1.36,1.6811974048614502,23.62,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Piumhi,2018-12-31T00:00:00,1.8,1.7180006504058838,4.56,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Presidente_Olegario,2012-12-31T00:00:00,2.66,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Presidente_Olegario,2013-12-31T00:00:00,2.65,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Presidente_Olegario,2014-12-31T00:00:00,2.32,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Presidente_Olegario,2015-12-31T00:00:00,1.88,1.890797734260559,0.57,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Presidente_Olegario,2016-12-31T00:00:00,2.52,2.557748317718506,1.5,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Presidente_Olegario,2017-12-31T00:00:00,1.84,1.871357798576355,1.7,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Presidente_Olegario,2018-12-31T00:00:00,2.28,2.3320984840393066,2.29,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Rio_Paranaiba,2012-12-31T00:00:00,2.16,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Rio_Paranaiba,2013-12-31T00:00:00,2.07,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Rio_Paranaiba,2014-12-31T00:00:00,1.89,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Rio_Paranaiba,2015-12-31T00:00:00,1.42,1.711653470993042,20.54,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Rio_Paranaiba,2016-12-31T00:00:00,2.08,1.992984414100647,4.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Rio_Paranaiba,2017-12-31T00:00:00,1.39,1.364348292350769,1.85,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Rio_Paranaiba,2018-12-31T00:00:00,2.1,1.9492195844650269,7.18,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Romaria,2012-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Romaria,2013-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Romaria,2014-12-31T00:00:00,2.28,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Romaria,2015-12-31T00:00:00,1.2,1.8721725940704346,56.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Romaria,2016-12-31T00:00:00,2.4,2.7602620124816895,15.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Romaria,2017-12-31T00:00:00,1.53,1.5312438011169434,0.08,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Romaria,2018-12-31T00:00:00,2.1,2.2711191177368164,8.15,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sacramento,2012-12-31T00:00:00,2.22,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sacramento,2013-12-31T00:00:00,1.87,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sacramento,2014-12-31T00:00:00,1.53,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sacramento,2015-12-31T00:00:00,1.34,1.5224772691726685,13.62,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sacramento,2016-12-31T00:00:00,1.87,1.805234432220459,3.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sacramento,2017-12-31T00:00:00,1.31,1.6244758367538452,24.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sacramento,2018-12-31T00:00:00,1.87,1.7943298816680908,4.05,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Santa_Rosa_da_Serra,2012-12-31T00:00:00,3.17,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Santa_Rosa_da_Serra,2013-12-31T00:00:00,3.0,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Santa_Rosa_da_Serra,2014-12-31T00:00:00,2.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Santa_Rosa_da_Serra,2015-12-31T00:00:00,1.62,1.5079920291900635,6.91,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Santa_Rosa_da_Serra,2016-12-31T00:00:00,1.5,2.292369842529297,52.82,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Santa_Rosa_da_Serra,2017-12-31T00:00:00,1.63,1.5212855339050293,6.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Santa_Rosa_da_Serra,2018-12-31T00:00:00,1.8,1.6338878870010376,9.23,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sao_Gotardo,2012-12-31T00:00:00,1.92,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sao_Gotardo,2013-12-31T00:00:00,1.92,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sao_Gotardo,2014-12-31T00:00:00,1.71,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sao_Gotardo,2015-12-31T00:00:00,1.62,1.692701816558838,4.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sao_Gotardo,2016-12-31T00:00:00,1.92,1.823347806930542,5.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sao_Gotardo,2017-12-31T00:00:00,1.44,1.5857951641082764,10.12,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sao_Gotardo,2018-12-31T00:00:00,2.1,1.7671561241149902,15.85,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Senador_Firmino,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Senador_Firmino,2013-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Senador_Firmino,2014-12-31T00:00:00,1.7,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Senador_Firmino,2015-12-31T00:00:00,1.38,1.3278794288635254,3.78,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Senador_Firmino,2016-12-31T00:00:00,1.38,1.5386016368865967,11.49,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Senador_Firmino,2017-12-31T00:00:00,2.38,1.602057695388794,32.69,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Senador_Firmino,2018-12-31T00:00:00,1.89,1.396850347518921,26.09,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Serra_do_Salitre,2012-12-31T00:00:00,3.1,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Serra_do_Salitre,2013-12-31T00:00:00,1.74,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Serra_do_Salitre,2014-12-31T00:00:00,2.22,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Serra_do_Salitre,2015-12-31T00:00:00,1.29,1.6816539764404297,30.36,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Serra_do_Salitre,2016-12-31T00:00:00,3.0,2.9096100330352783,3.01,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Serra_do_Salitre,2017-12-31T00:00:00,1.12,1.1387442350387573,1.67,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Serra_do_Salitre,2018-12-31T00:00:00,2.34,2.9231746196746826,24.92,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Tiros,2012-12-31T00:00:00,2.28,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Tiros,2013-12-31T00:00:00,2.28,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Tiros,2014-12-31T00:00:00,1.79,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Tiros,2015-12-31T00:00:00,1.68,1.8462178707122803,9.89,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Tiros,2016-12-31T00:00:00,2.1,1.994410514831543,5.03,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Tiros,2017-12-31T00:00:00,3.0,1.7151894569396973,42.83,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Tiros,2018-12-31T00:00:00,1.92,2.855877637863159,48.74,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Turmalina,2012-12-31T00:00:00,1.5,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Turmalina,2013-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Turmalina,2014-12-31T00:00:00,1.8,-,-,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Turmalina,2015-12-31T00:00:00,0.42,1.1105667352676392,164.42,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Turmalina,2016-12-31T00:00:00,1.56,1.2408902645111084,20.46,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Turmalina,2017-12-31T00:00:00,0.45,0.3318750858306885,26.25,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Turmalina,2018-12-31T00:00:00,1.74,0.9585695266723633,44.91,treino,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Acucena,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Araguari,2019-12-31T00:00:00,2.0,3,50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bambui,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bonfinopolis_de_Minas,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritis,2019-12-31T00:00:00,3.0,3,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritizeiro,2019-12-31T00:00:00,3.0,2,-33.33,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Campos_Altos,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cana_Verde,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Capitao_Eneas,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Careacu,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmo_do_Paranaiba,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmopolis_de_Minas,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cascalho_Rico,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Claraval,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Congonhas_do_Norte,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Indaia,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Sul,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Frei_Gaspar,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibia,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibiraci,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Indianopolis,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Itacambira,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ituiutaba,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Joao_Pinheiro,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Medeiros,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Monte_Carmelo,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ninheira,2019-12-31T00:00:00,3.0,3,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Nova_Era,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Padre_Paraiso,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Paracatu,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Passa_Tempo,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Patrocinio,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pedrinopolis,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Perdizes,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pimenta,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pirapora,2019-12-31T00:00:00,3.0,3,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Piumhi,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Presidente_Olegario,2019-12-31T00:00:00,2.0,2,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Rio_Paranaiba,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Romaria,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sacramento,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Santa_Rosa_da_Serra,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sao_Gotardo,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Senador_Firmino,2019-12-31T00:00:00,1.0,2,100.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Serra_do_Salitre,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Tiros,2019-12-31T00:00:00,2.0,1,-50.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Turmalina,2019-12-31T00:00:00,1.0,1,0.0,validacao,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Acucena,2020-12-31T00:00:00,1.166666666666667,0.8674998879432678,-25.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Araguari,2020-12-31T00:00:00,1.9799646954986758,2.3118703365325928,16.76,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bambui,2020-12-31T00:00:00,1.7998829724985372,1.494767665863037,-16.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bonfinopolis_de_Minas,2020-12-31T00:00:00,3.3,2.569100856781006,-22.15,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritis,2020-12-31T00:00:00,3.3,4.118119239807129,24.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritizeiro,2020-12-31T00:00:00,3.0,3.1356143951416016,4.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Campos_Altos,2020-12-31T00:00:00,2.1000529941706416,2.3066563606262207,9.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cana_Verde,2020-12-31T00:00:00,1.5,1.44869065284729,-3.42,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Capitao_Eneas,2020-12-31T00:00:00,3.0,0.7265485525131226,-75.78,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Careacu,2020-12-31T00:00:00,1.320454545454546,1.3580470085144043,2.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmo_do_Paranaiba,2020-12-31T00:00:00,1.9474807856532879,1.9925310611724854,2.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmopolis_de_Minas,2020-12-31T00:00:00,1.981818181818182,1.7579703330993652,-11.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cascalho_Rico,2020-12-31T00:00:00,2.478504672897196,2.0163803100585938,-18.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Claraval,2020-12-31T00:00:00,2.796008294453085,1.9597522020339966,-29.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Congonhas_do_Norte,2020-12-31T00:00:00,1.2,0.9546267986297607,-20.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Indaia,2020-12-31T00:00:00,1.8000000000000003,2.0431530475616455,13.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Sul,2020-12-31T00:00:00,2.0627027027027025,2.525820732116699,22.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Frei_Gaspar,2020-12-31T00:00:00,1.08,1.7186064720153809,59.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibia,2020-12-31T00:00:00,1.746,1.6699192523956299,-4.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibiraci,2020-12-31T00:00:00,2.6315194346289745,1.609783411026001,-38.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Indianopolis,2020-12-31T00:00:00,1.8000000000000003,2.227236270904541,23.74,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Itacambira,2020-12-31T00:00:00,0.75,4.984838008880615,564.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ituiutaba,2020-12-31T00:00:00,0.75,1.543090581893921,105.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Joao_Pinheiro,2020-12-31T00:00:00,3.0,2.297818660736084,-23.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Medeiros,2020-12-31T00:00:00,1.9801136363636358,2.0543582439422607,3.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Monte_Carmelo,2020-12-31T00:00:00,1.9799999999999998,2.138864755630493,8.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ninheira,2020-12-31T00:00:00,3.0,3.043187379837036,1.44,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Nova_Era,2020-12-31T00:00:00,1.565217391304348,1.8684818744659424,19.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Padre_Paraiso,2020-12-31T00:00:00,0.7219512195121951,0.5997178554534912,-16.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Paracatu,2020-12-31T00:00:00,2.7000000000000006,2.756978988647461,2.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Passa_Tempo,2020-12-31T00:00:00,1.511111111111111,1.6358060836791992,8.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Patrocinio,2020-12-31T00:00:00,1.7479986236953782,1.6148786544799805,-7.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pedrinopolis,2020-12-31T00:00:00,2.158490566037736,2.7953124046325684,29.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Perdizes,2020-12-31T00:00:00,2.050845253576073,2.1305789947509766,3.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pimenta,2020-12-31T00:00:00,1.8000000000000003,2.6001102924346924,44.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pirapora,2020-12-31T00:00:00,3.6000000000000005,3.3716607093811035,-6.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Piumhi,2020-12-31T00:00:00,1.7399966931216928,1.7553021907806396,0.88,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Presidente_Olegario,2020-12-31T00:00:00,1.9814229249011865,2.414140224456787,21.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Rio_Paranaiba,2020-12-31T00:00:00,1.818897637795276,1.59328293800354,-12.4,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Romaria,2020-12-31T00:00:00,2.330769230769231,2.1338889598846436,-8.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sacramento,2020-12-31T00:00:00,1.8321428571428573,1.8909156322479248,3.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Santa_Rosa_da_Serra,2020-12-31T00:00:00,2.1,1.9452420473098755,-7.37,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sao_Gotardo,2020-12-31T00:00:00,1.38027397260274,2.2968533039093018,66.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Senador_Firmino,2020-12-31T00:00:00,1.3762376237623761,2.3744125366210938,72.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Serra_do_Salitre,2020-12-31T00:00:00,1.8000000000000003,1.452048420906067,-19.33,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Tiros,2020-12-31T00:00:00,1.7402352941176473,2.9352474212646484,68.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Turmalina,2020-12-31T00:00:00,1.320338983050847,1.7714358568191528,34.17,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Acucena,2021-12-31T00:00:00,0.9,1.2435685396194458,38.17,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Araguari,2021-12-31T00:00:00,1.8600340136054423,1.9734201431274414,6.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bambui,2021-12-31T00:00:00,1.439910025706941,1.7101147174835205,18.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bonfinopolis_de_Minas,2021-12-31T00:00:00,3.0,2.91711163520813,-2.76,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritis,2021-12-31T00:00:00,3.1195039458850062,2.33891224861145,-25.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritizeiro,2021-12-31T00:00:00,3.0,2.4949233531951904,-16.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Campos_Altos,2021-12-31T00:00:00,1.2,1.5226877927780151,26.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cana_Verde,2021-12-31T00:00:00,1.32,1.4136369228363037,7.09,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Capitao_Eneas,2021-12-31T00:00:00,1.9999999999999996,1.9942660331726074,-0.29,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Careacu,2021-12-31T00:00:00,1.2,1.2656540870666504,5.47,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmo_do_Paranaiba,2021-12-31T00:00:00,1.541078066914498,1.6630940437316895,7.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmopolis_de_Minas,2021-12-31T00:00:00,1.9799999999999998,1.452952265739441,-26.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cascalho_Rico,2021-12-31T00:00:00,1.9411764705882348,1.9338204860687256,-0.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Claraval,2021-12-31T00:00:00,1.050113378684807,1.907179832458496,81.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Congonhas_do_Norte,2021-12-31T00:00:00,0.7333333333333333,1.2961909770965576,76.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Indaia,2021-12-31T00:00:00,1.8000000000000003,1.7878968715667725,-0.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Sul,2021-12-31T00:00:00,1.8000000000000003,1.962080717086792,9.0,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Frei_Gaspar,2021-12-31T00:00:00,1.2,1.124348759651184,-6.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibia,2021-12-31T00:00:00,1.2,1.523056983947754,26.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibiraci,2021-12-31T00:00:00,1.327034071867436,1.8210176229476929,37.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Indianopolis,2021-12-31T00:00:00,1.9199563794983638,1.9158637523651123,-0.21,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Itacambira,2021-12-31T00:00:00,0.5,0.684028148651123,36.81,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ituiutaba,2021-12-31T00:00:00,2.25,1.0817434787750244,-51.92,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Joao_Pinheiro,2021-12-31T00:00:00,2.4,2.3512473106384277,-2.03,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Medeiros,2021-12-31T00:00:00,1.500161864681127,1.7135339975357056,14.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Monte_Carmelo,2021-12-31T00:00:00,1.8000000000000003,1.8417186737060547,2.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ninheira,2021-12-31T00:00:00,2.1002460024600236,3.193293571472168,52.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Nova_Era,2021-12-31T00:00:00,1.5,1.917973518371582,27.86,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Padre_Paraiso,2021-12-31T00:00:00,0.4787878787878789,0.6173543334007263,28.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Paracatu,2021-12-31T00:00:00,2.4,2.228001594543457,-7.17,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Passa_Tempo,2021-12-31T00:00:00,1.511627906976744,1.4289758205413818,-5.47,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Patrocinio,2021-12-31T00:00:00,1.4369951534733443,1.4192980527877808,-1.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pedrinopolis,2021-12-31T00:00:00,1.514285714285714,1.253414273262024,-17.23,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Perdizes,2021-12-31T00:00:00,1.261308677098151,1.3349136114120483,5.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pimenta,2021-12-31T00:00:00,1.3502325581395351,1.3763371706008911,1.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pirapora,2021-12-31T00:00:00,3.0,2.80039119720459,-6.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Piumhi,2021-12-31T00:00:00,1.5,1.6095912456512451,7.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Presidente_Olegario,2021-12-31T00:00:00,1.637164750957854,1.7333887815475464,5.88,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Rio_Paranaiba,2021-12-31T00:00:00,1.080031384856807,1.405803918838501,30.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Romaria,2021-12-31T00:00:00,2.052631578947369,2.086883544921875,1.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sacramento,2021-12-31T00:00:00,1.25645342312009,1.5112507343292236,20.28,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Santa_Rosa_da_Serra,2021-12-31T00:00:00,1.5,1.9899446964263916,32.66,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sao_Gotardo,2021-12-31T00:00:00,1.26027397260274,1.4537159204483032,15.35,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Senador_Firmino,2021-12-31T00:00:00,1.563636363636364,1.6880741119384766,7.96,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Serra_do_Salitre,2021-12-31T00:00:00,1.4676384839650147,1.373030662536621,-6.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Tiros,2021-12-31T00:00:00,1.2,1.4676616191864014,22.31,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Turmalina,2021-12-31T00:00:00,1.2797783933518008,2.034478187561035,58.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Acucena,2022-12-31T00:00:00,1.0,0.7382411956787109,-26.18,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Araguari,2022-12-31T00:00:00,1.919968366943456,1.8418372869491577,-4.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bambui,2022-12-31T00:00:00,1.5,1.5351524353027344,2.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bonfinopolis_de_Minas,2022-12-31T00:00:00,3.3,3.5224595069885254,6.74,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritis,2022-12-31T00:00:00,3.0,3.144524574279785,4.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritizeiro,2022-12-31T00:00:00,3.0,2.9282493591308594,-2.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Campos_Altos,2022-12-31T00:00:00,1.6800422386483633,1.804887294769287,7.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cana_Verde,2022-12-31T00:00:00,1.02,1.3336706161499023,30.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Capitao_Eneas,2022-12-31T00:00:00,2.6000000000000005,7.690102577209473,195.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Careacu,2022-12-31T00:00:00,1.2,1.294037938117981,7.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmo_do_Paranaiba,2022-12-31T00:00:00,1.392969472710453,1.668835163116455,19.8,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmopolis_de_Minas,2022-12-31T00:00:00,1.8000000000000003,2.240206241607666,24.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cascalho_Rico,2022-12-31T00:00:00,2.580434782608696,2.1560659408569336,-16.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Claraval,2022-12-31T00:00:00,1.289915966386555,2.6745543479919434,107.34,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Congonhas_do_Norte,2022-12-31T00:00:00,0.9,0.6058145761489868,-32.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Indaia,2022-12-31T00:00:00,1.8599999999999999,1.524493932723999,-18.04,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Sul,2022-12-31T00:00:00,1.460045146726862,1.9902968406677246,36.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Frei_Gaspar,2022-12-31T00:00:00,1.133333333333333,1.236753225326538,9.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibia,2022-12-31T00:00:00,1.14,1.5741348266601562,38.08,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibiraci,2022-12-31T00:00:00,1.377007874015748,2.325958251953125,68.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Indianopolis,2022-12-31T00:00:00,1.8000000000000003,1.872438907623291,4.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Itacambira,2022-12-31T00:00:00,0.75,0.730711817741394,-2.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ituiutaba,2022-12-31T00:00:00,1.75,2.402435064315796,37.28,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Joao_Pinheiro,2022-12-31T00:00:00,2.4,2.553370237350464,6.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Medeiros,2022-12-31T00:00:00,1.5,1.4661344289779663,-2.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Monte_Carmelo,2022-12-31T00:00:00,1.7458874458874463,1.781792163848877,2.06,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ninheira,2022-12-31T00:00:00,2.9401197604790417,2.6895999908447266,-8.52,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Nova_Era,2022-12-31T00:00:00,1.5,1.5085816383361816,0.57,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Padre_Paraiso,2022-12-31T00:00:00,0.6000000000000001,0.5343353748321533,-10.94,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Paracatu,2022-12-31T00:00:00,2.4,2.3008310794830322,-4.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Passa_Tempo,2022-12-31T00:00:00,1.507692307692308,1.5199816226959229,0.82,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Patrocinio,2022-12-31T00:00:00,0.9438953724513282,1.6259562969207764,72.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pedrinopolis,2022-12-31T00:00:00,2.02051282051282,2.5559639930725098,26.5,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Perdizes,2022-12-31T00:00:00,1.501272727272727,1.9770584106445312,31.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pimenta,2022-12-31T00:00:00,1.3502325581395351,1.696832537651062,25.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pirapora,2022-12-31T00:00:00,3.3595505617977524,3.0292575359344482,-9.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Piumhi,2022-12-31T00:00:00,1.2,1.619213342666626,34.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Presidente_Olegario,2022-12-31T00:00:00,1.8899598393574302,1.9201511144638062,1.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Rio_Paranaiba,2022-12-31T00:00:00,1.230031446540881,1.3051705360412598,6.11,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Romaria,2022-12-31T00:00:00,1.910526315789474,2.233488082885742,16.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sacramento,2022-12-31T00:00:00,1.5575892857142857,1.6349550485610962,4.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Santa_Rosa_da_Serra,2022-12-31T00:00:00,2.2199999999999998,1.7197351455688477,-22.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sao_Gotardo,2022-12-31T00:00:00,1.50025974025974,1.3226369619369507,-11.84,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Senador_Firmino,2022-12-31T00:00:00,1.565217391304348,1.2784374952316284,-18.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Serra_do_Salitre,2022-12-31T00:00:00,1.5168750000000002,1.4144132137298584,-6.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Tiros,2022-12-31T00:00:00,1.6801801801801797,1.3424639701843262,-20.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Turmalina,2022-12-31T00:00:00,1.205163043478261,1.2537623643875122,4.03,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Acucena,2023-12-31T00:00:00,1.6666666666666672,1.2548760175704956,-24.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Araguari,2023-12-31T00:00:00,3.0,2.0733590126037598,-30.89,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bambui,2023-12-31T00:00:00,1.5,2.098696708679199,39.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bonfinopolis_de_Minas,2023-12-31T00:00:00,3.0,3.5049638748168945,16.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritis,2023-12-31T00:00:00,3.0,3.322810649871826,10.76,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritizeiro,2023-12-31T00:00:00,3.0,96.82344055175781,3127.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Campos_Altos,2023-12-31T00:00:00,1.380040526849037,2.598266124725342,88.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cana_Verde,2023-12-31T00:00:00,1.3196428571428571,1.8448119163513184,39.8,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Capitao_Eneas,2023-12-31T00:00:00,3.0,2.8887441158294678,-3.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Careacu,2023-12-31T00:00:00,1.5,1.3556909561157227,-9.62,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmo_do_Paranaiba,2023-12-31T00:00:00,2.369801007771799,2.0032851696014404,-15.47,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmopolis_de_Minas,2023-12-31T00:00:00,1.5,2.2418384552001953,49.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cascalho_Rico,2023-12-31T00:00:00,2.4,3.2019429206848145,33.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Claraval,2023-12-31T00:00:00,1.8600214362272243,1.786195158958435,-3.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Congonhas_do_Norte,2023-12-31T00:00:00,1.111111111111111,1.723310112953186,55.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Indaia,2023-12-31T00:00:00,1.8000000000000003,1.8893166780471802,4.96,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Sul,2023-12-31T00:00:00,2.43015873015873,2.507298469543457,3.17,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Frei_Gaspar,2023-12-31T00:00:00,1.2,1.1835050582885742,-1.37,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibia,2023-12-31T00:00:00,1.668,1.8198058605194092,9.1,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibiraci,2023-12-31T00:00:00,1.8041698841698839,1.5089454650878906,-16.36,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Indianopolis,2023-12-31T00:00:00,2.7000000000000006,2.003831386566162,-25.78,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Itacambira,2023-12-31T00:00:00,0.75,0.8748297691345215,16.64,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ituiutaba,2023-12-31T00:00:00,1.0,3.037754774093628,203.78,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Joao_Pinheiro,2023-12-31T00:00:00,3.3,3.170405626296997,-3.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Medeiros,2023-12-31T00:00:00,1.8599999999999999,2.285522222518921,22.88,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Monte_Carmelo,2023-12-31T00:00:00,2.4900284900284904,2.0102505683898926,-19.27,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ninheira,2023-12-31T00:00:00,3.0,2.8134419918060303,-6.22,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Nova_Era,2023-12-31T00:00:00,2.636363636363636,1.5696090459823608,-40.46,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Padre_Paraiso,2023-12-31T00:00:00,0.7212121212121212,0.7480403184890747,3.72,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Paracatu,2023-12-31T00:00:00,2.4,3.04634428024292,26.93,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Passa_Tempo,2023-12-31T00:00:00,1.625,1.5142674446105957,-6.81,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Patrocinio,2023-12-31T00:00:00,2.4370275910039414,1.8227860927581787,-25.2,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pedrinopolis,2023-12-31T00:00:00,2.1878048780487815,2.5691494941711426,17.43,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Perdizes,2023-12-31T00:00:00,2.106451612903226,2.2597808837890625,7.28,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pimenta,2023-12-31T00:00:00,1.504186046511628,2.0782053470611572,38.16,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pirapora,2023-12-31T00:00:00,3.6000000000000005,4.1489338874816895,15.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Piumhi,2023-12-31T00:00:00,1.7399633363886338,2.0379159450531006,17.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Presidente_Olegario,2023-12-31T00:00:00,2.160074626865672,1.9571470022201538,-9.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Rio_Paranaiba,2023-12-31T00:00:00,1.8000000000000003,2.061584234237671,14.53,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Romaria,2023-12-31T00:00:00,2.131578947368421,2.627133846282959,23.25,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sacramento,2023-12-31T00:00:00,1.75,2.5945394039154053,48.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Santa_Rosa_da_Serra,2023-12-31T00:00:00,1.68,2.815666675567627,67.6,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sao_Gotardo,2023-12-31T00:00:00,1.5,1.6497464179992676,9.98,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Senador_Firmino,2023-12-31T00:00:00,1.436170212765957,1.8800321817398071,30.91,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Serra_do_Salitre,2023-12-31T00:00:00,1.489636363636364,1.7688016891479492,18.74,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Tiros,2023-12-31T00:00:00,2.382038834951457,2.2459983825683594,-5.71,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Turmalina,2023-12-31T00:00:00,1.360606060606061,1.3058730363845825,-4.02,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Acucena,2024-12-31T00:00:00,1.6666666666666672,1.5217933654785156,-8.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Araguari,2024-12-31T00:00:00,1.500037950664137,3.9320719242095947,162.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bambui,2024-12-31T00:00:00,1.8000000000000003,1.5373436212539673,-14.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bonfinopolis_de_Minas,2024-12-31T00:00:00,2.1,3.057316303253174,45.59,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritis,2024-12-31T00:00:00,2.69967707212056,3.052671432495117,13.08,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritizeiro,2024-12-31T00:00:00,2.7000000000000006,6.969907283782959,158.14,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Campos_Altos,2024-12-31T00:00:00,1.5,1.5115399360656738,0.77,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cana_Verde,2024-12-31T00:00:00,1.2,2.9318220615386963,144.32,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Capitao_Eneas,2024-12-31T00:00:00,3.0,3.199493646621704,6.65,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Careacu,2024-12-31T00:00:00,1.560169491525424,1.7627545595169067,12.98,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmo_do_Paranaiba,2024-12-31T00:00:00,2.103206239168111,2.2098562717437744,5.07,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmopolis_de_Minas,2024-12-31T00:00:00,1.9799999999999998,2.762225389480591,39.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cascalho_Rico,2024-12-31T00:00:00,1.8000000000000003,3.5356054306030273,96.42,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Claraval,2024-12-31T00:00:00,1.92,2.712918996810913,41.3,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Congonhas_do_Norte,2024-12-31T00:00:00,1.2,1.0490788221359253,-12.58,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Indaia,2024-12-31T00:00:00,1.56,1.823237657546997,16.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Sul,2024-12-31T00:00:00,1.7099999999999997,2.2980594635009766,34.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Frei_Gaspar,2024-12-31T00:00:00,1.2,1.1859471797943115,-1.17,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibia,2024-12-31T00:00:00,1.681621621621622,1.4945319890975952,-11.13,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibiraci,2024-12-31T00:00:00,1.978101265822785,1.861978530883789,-5.87,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Indianopolis,2024-12-31T00:00:00,1.8000000000000003,2.823225259780884,56.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Itacambira,2024-12-31T00:00:00,0.5,0.7773982286453247,55.48,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ituiutaba,2024-12-31T00:00:00,1.0,2.0637664794921875,106.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Joao_Pinheiro,2024-12-31T00:00:00,3.0,3.2521910667419434,8.41,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Medeiros,2024-12-31T00:00:00,1.8600609756097564,1.695863127708435,-8.83,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Monte_Carmelo,2024-12-31T00:00:00,1.56,2.732576847076416,75.17,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ninheira,2024-12-31T00:00:00,2.819758276405675,2.8187804222106934,-0.03,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Nova_Era,2024-12-31T00:00:00,1.5,2.4118361473083496,60.79,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Padre_Paraiso,2024-12-31T00:00:00,0.7212121212121212,0.7074884176254272,-1.9,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Paracatu,2024-12-31T00:00:00,2.7003236245954687,15.341047286987305,468.12,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Passa_Tempo,2024-12-31T00:00:00,1.5,1.7949028015136719,19.66,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Patrocinio,2024-12-31T00:00:00,1.4995330375904743,2.1819276809692383,45.51,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pedrinopolis,2024-12-31T00:00:00,1.8511627906976742,2.251269578933716,21.61,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Perdizes,2024-12-31T00:00:00,1.142806076854334,2.019904613494873,76.75,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pimenta,2024-12-31T00:00:00,1.715219421101774,1.683465838432312,-1.85,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pirapora,2024-12-31T00:00:00,2.7000000000000006,3.4900450706481934,29.26,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Piumhi,2024-12-31T00:00:00,1.3800316957210783,1.9730188846588135,42.97,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Presidente_Olegario,2024-12-31T00:00:00,1.9199630314232903,2.280540943145752,18.78,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Rio_Paranaiba,2024-12-31T00:00:00,1.680014776505357,1.7127070426940918,1.95,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Romaria,2024-12-31T00:00:00,2.052631578947369,2.2515172958374023,9.69,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sacramento,2024-12-31T00:00:00,1.7370212765957447,1.8133575916290283,4.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Santa_Rosa_da_Serra,2024-12-31T00:00:00,1.92,1.9478617906570435,1.45,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sao_Gotardo,2024-12-31T00:00:00,1.668148148148148,1.5568585395812988,-6.67,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Senador_Firmino,2024-12-31T00:00:00,1.804347826086957,1.6891052722930908,-6.39,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Serra_do_Salitre,2024-12-31T00:00:00,1.5168750000000002,1.5110777616500854,-0.38,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Tiros,2024-12-31T00:00:00,1.9499999999999997,2.873723030090332,47.37,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Turmalina,2024-12-31T00:00:00,1.8507692307692305,1.3432668447494507,-27.42,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Acucena,2025-12-31T00:00:00,0.0,1.7413997650146484,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Araguari,2025-12-31T00:00:00,0.0,2.5054874420166016,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bambui,2025-12-31T00:00:00,0.0,1.7580606937408447,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Bonfinopolis_de_Minas,2025-12-31T00:00:00,0.0,3.1380834579467773,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritis,2025-12-31T00:00:00,0.0,3.0709500312805176,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Buritizeiro,2025-12-31T00:00:00,0.0,2.782444715499878,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Campos_Altos,2025-12-31T00:00:00,0.0,1.4729392528533936,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cana_Verde,2025-12-31T00:00:00,0.0,1.3715248107910156,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Capitao_Eneas,2025-12-31T00:00:00,0.0,2.7044758796691895,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Careacu,2025-12-31T00:00:00,0.0,1.7307162284851074,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmo_do_Paranaiba,2025-12-31T00:00:00,0.0,2.3783388137817383,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Carmopolis_de_Minas,2025-12-31T00:00:00,0.0,1.9482803344726562,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Cascalho_Rico,2025-12-31T00:00:00,0.0,1.9143935441970825,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Claraval,2025-12-31T00:00:00,0.0,2.0376884937286377,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Congonhas_do_Norte,2025-12-31T00:00:00,0.0,1.1300270557403564,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Indaia,2025-12-31T00:00:00,0.0,1.7599873542785645,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Estrela_do_Sul,2025-12-31T00:00:00,0.0,1.9226446151733398,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Frei_Gaspar,2025-12-31T00:00:00,0.0,1.1620458364486694,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibia,2025-12-31T00:00:00,0.0,1.751581072807312,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ibiraci,2025-12-31T00:00:00,0.0,1.9689664840698242,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Indianopolis,2025-12-31T00:00:00,0.0,2.4317374229431152,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Itacambira,2025-12-31T00:00:00,0.0,0.638153612613678,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ituiutaba,2025-12-31T00:00:00,0.0,0.8084027767181396,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Joao_Pinheiro,2025-12-31T00:00:00,0.0,3.3022377490997314,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Medeiros,2025-12-31T00:00:00,0.0,1.873002290725708,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Monte_Carmelo,2025-12-31T00:00:00,0.0,2.3225948810577393,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Ninheira,2025-12-31T00:00:00,0.0,2.9337451457977295,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Nova_Era,2025-12-31T00:00:00,0.0,2.1226162910461426,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Padre_Paraiso,2025-12-31T00:00:00,0.0,0.780174732208252,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Paracatu,2025-12-31T00:00:00,0.0,2.6768343448638916,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Passa_Tempo,2025-12-31T00:00:00,0.0,1.5499389171600342,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Patrocinio,2025-12-31T00:00:00,0.0,2.143296718597412,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pedrinopolis,2025-12-31T00:00:00,0.0,2.0844600200653076,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Perdizes,2025-12-31T00:00:00,0.0,1.6890099048614502,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pimenta,2025-12-31T00:00:00,0.0,1.7386908531188965,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Pirapora,2025-12-31T00:00:00,0.0,3.0060808658599854,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Piumhi,2025-12-31T00:00:00,0.0,1.7435884475708008,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Presidente_Olegario,2025-12-31T00:00:00,0.0,2.115652084350586,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Rio_Paranaiba,2025-12-31T00:00:00,0.0,1.8586026430130005,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Romaria,2025-12-31T00:00:00,0.0,2.1039986610412598,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sacramento,2025-12-31T00:00:00,0.0,1.7627291679382324,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Santa_Rosa_da_Serra,2025-12-31T00:00:00,0.0,1.7893801927566528,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Sao_Gotardo,2025-12-31T00:00:00,0.0,1.6234495639801025,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Senador_Firmino,2025-12-31T00:00:00,0.0,1.8176379203796387,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Serra_do_Salitre,2025-12-31T00:00:00,0.0,1.5225660800933838,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Tiros,2025-12-31T00:00:00,0.0,2.2676756381988525,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
cluster 4 (2020-225),Turmalina,2025-12-31T00:00:00,0.0,1.8621635437011719,inf,teste,V29,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V28 V2, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0005330900684399468
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.3
weight_decay: 0.0001
steps: 300
",2025-09-23T23:00:26
