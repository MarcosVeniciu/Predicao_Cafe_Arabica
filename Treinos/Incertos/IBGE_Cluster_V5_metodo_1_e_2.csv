treino_id,unique_id,ds,y,y_pred,diferença_%,flag,dataset,modelo,comentario,data_treino
IBGE V5 - Metodo 1 (2023),Abre_Campo,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Abre_Campo,2013-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Abre_Campo,2014-12-31T00:00:00,0.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Abre_Campo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Abre_Campo,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Abre_Campo,2017-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Abre_Campo,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Abre_Campo,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Abre_Campo,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Abre_Campo,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alfenas,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alfenas,2013-12-31T00:00:00,1.98,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alfenas,2014-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alfenas,2015-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alfenas,2016-12-31T00:00:00,2.25,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alfenas,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alfenas,2018-12-31T00:00:00,2.34,2.0,14.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alfenas,2019-12-31T00:00:00,1.76,2.0,13.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alfenas,2020-12-31T00:00:00,2.36,2.0,15.25,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alfenas,2021-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alpinopolis,2012-12-31T00:00:00,2.7,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alpinopolis,2013-12-31T00:00:00,2.71,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alpinopolis,2014-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alpinopolis,2015-12-31T00:00:00,1.82,2.0,9.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alpinopolis,2016-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alpinopolis,2017-12-31T00:00:00,2.06,2.0,2.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alpinopolis,2018-12-31T00:00:00,2.23,2.0,10.31,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alpinopolis,2019-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alpinopolis,2020-12-31T00:00:00,2.77,2.0,27.8,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alpinopolis,2021-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alterosa,2012-12-31T00:00:00,1.59,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alterosa,2013-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alterosa,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alterosa,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alterosa,2016-12-31T00:00:00,1.92,1.0,47.92,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alterosa,2017-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alterosa,2018-12-31T00:00:00,1.96,2.0,2.04,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alterosa,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alterosa,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alterosa,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Caparao,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Caparao,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Caparao,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Caparao,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Caparao,2016-12-31T00:00:00,2.1,1.0,52.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Caparao,2017-12-31T00:00:00,1.37,1.0,27.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Caparao,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Caparao,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Caparao,2020-12-31T00:00:00,1.92,1.0,47.92,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Caparao,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Jequitiba,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Jequitiba,2013-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Jequitiba,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Jequitiba,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Jequitiba,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Jequitiba,2017-12-31T00:00:00,1.53,1.0,34.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Jequitiba,2018-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Jequitiba,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Jequitiba,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Jequitiba,2021-12-31T00:00:00,0.9,2.0,122.22,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Andradas,2012-12-31T00:00:00,1.98,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Andradas,2013-12-31T00:00:00,1.59,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Andradas,2014-12-31T00:00:00,1.21,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Andradas,2015-12-31T00:00:00,1.89,2.0,5.82,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Andradas,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Andradas,2017-12-31T00:00:00,1.34,2.0,49.25,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Andradas,2018-12-31T00:00:00,1.73,2.0,15.61,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Andradas,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Andradas,2020-12-31T00:00:00,2.57,2.0,22.18,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Andradas,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Angelandia,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Angelandia,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Angelandia,2014-12-31T00:00:00,1.15,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Angelandia,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Angelandia,2016-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Angelandia,2017-12-31T00:00:00,0.94,1.0,6.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Angelandia,2018-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Angelandia,2019-12-31T00:00:00,1.22,1.0,18.03,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Angelandia,2020-12-31T00:00:00,1.84,1.0,45.65,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Angelandia,2021-12-31T00:00:00,1.51,2.0,32.45,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araguari,2012-12-31T00:00:00,2.4,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araguari,2013-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araguari,2014-12-31T00:00:00,2.4,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araguari,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araguari,2016-12-31T00:00:00,2.58,2.0,22.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araguari,2017-12-31T00:00:00,2.76,2.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araguari,2018-12-31T00:00:00,2.64,2.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araguari,2019-12-31T00:00:00,1.68,3.0,78.57,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araguari,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araguari,2021-12-31T00:00:00,1.86,2.0,7.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araponga,2012-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araponga,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araponga,2014-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araponga,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araponga,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araponga,2017-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araponga,2018-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araponga,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araponga,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araponga,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araxa,2012-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araxa,2013-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araxa,2014-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araxa,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araxa,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araxa,2017-12-31T00:00:00,1.76,2.0,13.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araxa,2018-12-31T00:00:00,1.94,2.0,3.09,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araxa,2019-12-31T00:00:00,1.39,2.0,43.88,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araxa,2020-12-31T00:00:00,1.79,2.0,11.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araxa,2021-12-31T00:00:00,1.25,2.0,60.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Areado,2012-12-31T00:00:00,1.95,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Areado,2013-12-31T00:00:00,1.86,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Areado,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Areado,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Areado,2016-12-31T00:00:00,2.04,2.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Areado,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Areado,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Areado,2019-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Areado,2020-12-31T00:00:00,2.08,2.0,3.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Areado,2021-12-31T00:00:00,1.52,2.0,31.58,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bambui,2012-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bambui,2013-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bambui,2014-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bambui,2015-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bambui,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bambui,2017-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bambui,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bambui,2019-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bambui,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bambui,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Boa_Esperanca,2012-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Boa_Esperanca,2013-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Boa_Esperanca,2014-12-31T00:00:00,1.15,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Boa_Esperanca,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Boa_Esperanca,2016-12-31T00:00:00,1.92,1.0,47.92,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Boa_Esperanca,2017-12-31T00:00:00,2.53,1.0,60.47,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Boa_Esperanca,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Boa_Esperanca,2019-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Boa_Esperanca,2020-12-31T00:00:00,2.06,2.0,2.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Boa_Esperanca,2021-12-31T00:00:00,1.57,2.0,27.39,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Jesus_da_Penha,2012-12-31T00:00:00,2.58,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Jesus_da_Penha,2013-12-31T00:00:00,2.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Jesus_da_Penha,2014-12-31T00:00:00,2.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Jesus_da_Penha,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Jesus_da_Penha,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Jesus_da_Penha,2017-12-31T00:00:00,2.22,2.0,9.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Jesus_da_Penha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Jesus_da_Penha,2019-12-31T00:00:00,2.22,2.0,9.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Jesus_da_Penha,2020-12-31T00:00:00,1.99,2.0,0.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Jesus_da_Penha,2021-12-31T00:00:00,1.52,2.0,31.58,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Sucesso,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Sucesso,2013-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Sucesso,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Sucesso,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Sucesso,2016-12-31T00:00:00,1.92,1.0,47.92,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Sucesso,2017-12-31T00:00:00,1.48,1.0,32.43,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Sucesso,2018-12-31T00:00:00,1.7,2.0,17.65,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Sucesso,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Sucesso,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Sucesso,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Botelhos,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Botelhos,2013-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Botelhos,2014-12-31T00:00:00,1.74,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Botelhos,2015-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Botelhos,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Botelhos,2017-12-31T00:00:00,1.88,1.0,46.81,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Botelhos,2018-12-31T00:00:00,2.07,1.0,51.69,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Botelhos,2019-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Botelhos,2020-12-31T00:00:00,1.72,2.0,16.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Botelhos,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bueno_Brandao,2012-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bueno_Brandao,2013-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bueno_Brandao,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bueno_Brandao,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bueno_Brandao,2016-12-31T00:00:00,2.1,1.0,52.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bueno_Brandao,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bueno_Brandao,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bueno_Brandao,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bueno_Brandao,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bueno_Brandao,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Buritizeiro,2012-12-31T00:00:00,3.6,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Buritizeiro,2013-12-31T00:00:00,3.6,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Buritizeiro,2014-12-31T00:00:00,3.6,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Buritizeiro,2015-12-31T00:00:00,1.92,4.0,108.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Buritizeiro,2016-12-31T00:00:00,2.1,3.0,42.86,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Buritizeiro,2017-12-31T00:00:00,1.97,2.0,1.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Buritizeiro,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Buritizeiro,2019-12-31T00:00:00,2.88,2.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Buritizeiro,2020-12-31T00:00:00,3.0,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Buritizeiro,2021-12-31T00:00:00,3.0,3.0,0.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cabo_Verde,2012-12-31T00:00:00,2.07,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cabo_Verde,2013-12-31T00:00:00,2.04,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cabo_Verde,2014-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cabo_Verde,2015-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cabo_Verde,2016-12-31T00:00:00,1.65,2.0,21.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cabo_Verde,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cabo_Verde,2018-12-31T00:00:00,1.61,2.0,24.22,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cabo_Verde,2019-12-31T00:00:00,2.04,2.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cabo_Verde,2020-12-31T00:00:00,2.25,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cabo_Verde,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caiana,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caiana,2013-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caiana,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caiana,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caiana,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caiana,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caiana,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caiana,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caiana,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caiana,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cambuquira,2012-12-31T00:00:00,1.15,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cambuquira,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cambuquira,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cambuquira,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cambuquira,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cambuquira,2017-12-31T00:00:00,1.82,1.0,45.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cambuquira,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cambuquira,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cambuquira,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cambuquira,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campanha,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campanha,2013-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campanha,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campanha,2015-12-31T00:00:00,1.02,1.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campanha,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campanha,2017-12-31T00:00:00,1.88,1.0,46.81,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campanha,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campanha,2019-12-31T00:00:00,1.51,2.0,32.45,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campanha,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campanha,2021-12-31T00:00:00,1.33,2.0,50.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campestre,2012-12-31T00:00:00,1.35,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campestre,2013-12-31T00:00:00,1.31,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campestre,2014-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campestre,2015-12-31T00:00:00,1.91,1.0,47.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campestre,2016-12-31T00:00:00,1.85,2.0,8.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campestre,2017-12-31T00:00:00,2.11,2.0,5.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campestre,2018-12-31T00:00:00,1.54,2.0,29.87,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campestre,2019-12-31T00:00:00,1.55,2.0,29.03,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campestre,2020-12-31T00:00:00,2.14,2.0,6.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campestre,2021-12-31T00:00:00,1.29,2.0,55.04,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_Belo,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_Belo,2013-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_Belo,2014-12-31T00:00:00,0.63,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_Belo,2015-12-31T00:00:00,0.84,1.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_Belo,2016-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_Belo,2017-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_Belo,2018-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_Belo,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_Belo,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_Belo,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_do_Meio,2012-12-31T00:00:00,1.17,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_do_Meio,2013-12-31T00:00:00,1.17,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_do_Meio,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_do_Meio,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_do_Meio,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_do_Meio,2017-12-31T00:00:00,2.08,1.0,51.92,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_do_Meio,2018-12-31T00:00:00,1.82,2.0,9.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_do_Meio,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_do_Meio,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_do_Meio,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Altos,2012-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Altos,2013-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Altos,2014-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Altos,2015-12-31T00:00:00,1.71,2.0,16.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Altos,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Altos,2017-12-31T00:00:00,1.99,2.0,0.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Altos,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Altos,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Altos,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Altos,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Gerais,2012-12-31T00:00:00,1.65,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Gerais,2013-12-31T00:00:00,1.22,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Gerais,2014-12-31T00:00:00,1.15,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Gerais,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Gerais,2016-12-31T00:00:00,2.5,1.0,60.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Gerais,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Gerais,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Gerais,2019-12-31T00:00:00,1.59,2.0,25.79,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Gerais,2020-12-31T00:00:00,2.69,2.0,25.65,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Gerais,2021-12-31T00:00:00,1.59,2.0,25.79,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Candeias,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Candeias,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Candeias,2014-12-31T00:00:00,0.99,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Candeias,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Candeias,2016-12-31T00:00:00,2.28,1.0,56.14,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Candeias,2017-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Candeias,2018-12-31T00:00:00,2.04,2.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Candeias,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Candeias,2020-12-31T00:00:00,1.86,2.0,7.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Candeias,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caparao,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caparao,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caparao,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caparao,2015-12-31T00:00:00,1.02,1.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caparao,2016-12-31T00:00:00,1.92,1.0,47.92,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caparao,2017-12-31T00:00:00,1.45,1.0,31.03,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caparao,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caparao,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caparao,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caparao,2021-12-31T00:00:00,0.9,2.0,122.22,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capelinha,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capelinha,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capelinha,2014-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capelinha,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capelinha,2016-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capelinha,2017-12-31T00:00:00,1.16,1.0,13.79,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capelinha,2018-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capelinha,2019-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capelinha,2020-12-31T00:00:00,1.43,1.0,30.07,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capelinha,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capetinga,2012-12-31T00:00:00,1.81,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capetinga,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capetinga,2014-12-31T00:00:00,1.59,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capetinga,2015-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capetinga,2016-12-31T00:00:00,1.93,1.0,48.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capetinga,2017-12-31T00:00:00,2.09,2.0,4.31,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capetinga,2018-12-31T00:00:00,2.7,2.0,25.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capetinga,2019-12-31T00:00:00,2.04,2.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capetinga,2020-12-31T00:00:00,2.21,2.0,9.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capetinga,2021-12-31T00:00:00,1.64,2.0,21.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capitolio,2012-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capitolio,2013-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capitolio,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capitolio,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capitolio,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capitolio,2017-12-31T00:00:00,1.47,1.0,31.97,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capitolio,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capitolio,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capitolio,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capitolio,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caputira,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caputira,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caputira,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caputira,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caputira,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caputira,2017-12-31T00:00:00,0.77,1.0,29.87,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caputira,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caputira,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caputira,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caputira,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carangola,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carangola,2013-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carangola,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carangola,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carangola,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carangola,2017-12-31T00:00:00,1.1,1.0,9.09,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carangola,2018-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carangola,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carangola,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carangola,2021-12-31T00:00:00,1.02,1.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caratinga,2012-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caratinga,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caratinga,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caratinga,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caratinga,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caratinga,2017-12-31T00:00:00,1.46,1.0,31.51,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caratinga,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caratinga,2019-12-31T00:00:00,1.0,2.0,100.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caratinga,2020-12-31T00:00:00,2.04,1.0,50.98,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caratinga,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_da_Cachoeira,2012-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_da_Cachoeira,2013-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_da_Cachoeira,2014-12-31T00:00:00,1.15,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_da_Cachoeira,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_da_Cachoeira,2016-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_da_Cachoeira,2017-12-31T00:00:00,1.73,1.0,42.2,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_da_Cachoeira,2018-12-31T00:00:00,1.98,1.0,49.49,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_da_Cachoeira,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_da_Cachoeira,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_da_Cachoeira,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_de_Minas,2012-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_de_Minas,2013-12-31T00:00:00,1.74,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_de_Minas,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_de_Minas,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_de_Minas,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_de_Minas,2017-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_de_Minas,2018-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_de_Minas,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_de_Minas,2020-12-31T00:00:00,1.74,1.0,42.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_de_Minas,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Paranaiba,2012-12-31T00:00:00,2.11,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Paranaiba,2013-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Paranaiba,2014-12-31T00:00:00,1.93,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Paranaiba,2015-12-31T00:00:00,1.89,2.0,5.82,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Paranaiba,2016-12-31T00:00:00,2.37,2.0,15.61,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Paranaiba,2017-12-31T00:00:00,1.75,2.0,14.29,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Paranaiba,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Paranaiba,2019-12-31T00:00:00,1.79,2.0,11.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Paranaiba,2020-12-31T00:00:00,1.95,2.0,2.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Paranaiba,2021-12-31T00:00:00,1.54,2.0,29.87,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Rio_Claro,2012-12-31T00:00:00,1.66,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Rio_Claro,2013-12-31T00:00:00,1.94,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Rio_Claro,2014-12-31T00:00:00,1.65,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Rio_Claro,2015-12-31T00:00:00,1.53,2.0,30.72,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Rio_Claro,2016-12-31T00:00:00,2.37,2.0,15.61,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Rio_Claro,2017-12-31T00:00:00,1.82,2.0,9.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Rio_Claro,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Rio_Claro,2019-12-31T00:00:00,1.94,2.0,3.09,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Rio_Claro,2020-12-31T00:00:00,2.78,2.0,28.06,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Rio_Claro,2021-12-31T00:00:00,1.64,2.0,21.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cassia,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cassia,2013-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cassia,2014-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cassia,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cassia,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cassia,2017-12-31T00:00:00,1.6,1.0,37.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cassia,2018-12-31T00:00:00,2.1,1.0,52.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cassia,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cassia,2020-12-31T00:00:00,2.17,2.0,7.83,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cassia,2021-12-31T00:00:00,1.31,2.0,52.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Chale,2012-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Chale,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Chale,2014-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Chale,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Chale,2016-12-31T00:00:00,1.42,2.0,40.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Chale,2017-12-31T00:00:00,0.93,1.0,7.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Chale,2018-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Chale,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Chale,2020-12-31T00:00:00,1.71,1.0,41.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Chale,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Claraval,2012-12-31T00:00:00,2.55,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Claraval,2013-12-31T00:00:00,1.42,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Claraval,2014-12-31T00:00:00,2.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Claraval,2015-12-31T00:00:00,1.02,2.0,96.08,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Claraval,2016-12-31T00:00:00,2.34,2.0,14.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Claraval,2017-12-31T00:00:00,1.22,2.0,63.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Claraval,2018-12-31T00:00:00,2.55,2.0,21.57,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Claraval,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Claraval,2020-12-31T00:00:00,2.8,2.0,28.57,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Claraval,2021-12-31T00:00:00,1.05,2.0,90.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_da_Aparecida,2012-12-31T00:00:00,1.95,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_da_Aparecida,2013-12-31T00:00:00,1.7,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_da_Aparecida,2014-12-31T00:00:00,1.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_da_Aparecida,2015-12-31T00:00:00,2.52,2.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_da_Aparecida,2016-12-31T00:00:00,1.99,2.0,0.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_da_Aparecida,2017-12-31T00:00:00,1.85,2.0,8.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_da_Aparecida,2018-12-31T00:00:00,2.03,2.0,1.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_da_Aparecida,2019-12-31T00:00:00,1.97,2.0,1.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_da_Aparecida,2020-12-31T00:00:00,2.24,2.0,10.71,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_da_Aparecida,2021-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2015-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2016-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2017-12-31T00:00:00,2.04,1.0,50.98,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conselheiro_Pena,2012-12-31T00:00:00,1.48,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conselheiro_Pena,2013-12-31T00:00:00,1.3,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conselheiro_Pena,2014-12-31T00:00:00,1.3,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conselheiro_Pena,2015-12-31T00:00:00,1.06,1.0,5.66,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conselheiro_Pena,2016-12-31T00:00:00,1.18,1.0,15.25,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conselheiro_Pena,2017-12-31T00:00:00,1.29,1.0,22.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conselheiro_Pena,2018-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conselheiro_Pena,2019-12-31T00:00:00,1.3,1.0,23.08,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conselheiro_Pena,2020-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conselheiro_Pena,2021-12-31T00:00:00,1.29,1.0,22.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coqueiral,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coqueiral,2013-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coqueiral,2014-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coqueiral,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coqueiral,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coqueiral,2017-12-31T00:00:00,1.75,1.0,42.86,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coqueiral,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coqueiral,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coqueiral,2020-12-31T00:00:00,2.16,2.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coqueiral,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coromandel,2012-12-31T00:00:00,1.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coromandel,2013-12-31T00:00:00,2.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coromandel,2014-12-31T00:00:00,1.91,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coromandel,2015-12-31T00:00:00,1.91,2.0,4.71,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coromandel,2016-12-31T00:00:00,2.09,2.0,4.31,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coromandel,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coromandel,2018-12-31T00:00:00,2.65,2.0,24.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coromandel,2019-12-31T00:00:00,1.65,2.0,21.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coromandel,2020-12-31T00:00:00,1.82,2.0,9.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coromandel,2021-12-31T00:00:00,1.25,2.0,60.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cristais,2012-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cristais,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cristais,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cristais,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cristais,2016-12-31T00:00:00,1.92,1.0,47.92,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cristais,2017-12-31T00:00:00,2.4,1.0,58.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cristais,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cristais,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cristais,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cristais,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divino,2012-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divino,2013-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divino,2014-12-31T00:00:00,0.89,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divino,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divino,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divino,2017-12-31T00:00:00,1.19,1.0,15.97,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divino,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divino,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divino,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divino,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divisa_Nova,2012-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divisa_Nova,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divisa_Nova,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divisa_Nova,2015-12-31T00:00:00,1.08,2.0,85.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divisa_Nova,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divisa_Nova,2017-12-31T00:00:00,2.28,1.0,56.14,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divisa_Nova,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divisa_Nova,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divisa_Nova,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divisa_Nova,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Durande,2012-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Durande,2013-12-31T00:00:00,2.22,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Durande,2014-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Durande,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Durande,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Durande,2017-12-31T00:00:00,1.33,2.0,50.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Durande,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Durande,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Durande,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Durande,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Eloi_Mendes,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Eloi_Mendes,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Eloi_Mendes,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Eloi_Mendes,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Eloi_Mendes,2016-12-31T00:00:00,1.74,1.0,42.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Eloi_Mendes,2017-12-31T00:00:00,1.83,1.0,45.36,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Eloi_Mendes,2018-12-31T00:00:00,1.72,2.0,16.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Eloi_Mendes,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Eloi_Mendes,2020-12-31T00:00:00,2.11,2.0,5.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Eloi_Mendes,2021-12-31T00:00:00,1.04,2.0,92.31,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ervalia,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ervalia,2013-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ervalia,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ervalia,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ervalia,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ervalia,2017-12-31T00:00:00,1.36,1.0,26.47,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ervalia,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ervalia,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ervalia,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ervalia,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Espera_Feliz,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Espera_Feliz,2013-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Espera_Feliz,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Espera_Feliz,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Espera_Feliz,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Espera_Feliz,2017-12-31T00:00:00,1.21,1.0,17.36,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Espera_Feliz,2018-12-31T00:00:00,1.92,1.0,47.92,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Espera_Feliz,2019-12-31T00:00:00,0.9,2.0,122.22,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Espera_Feliz,2020-12-31T00:00:00,2.04,1.0,50.98,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Espera_Feliz,2021-12-31T00:00:00,0.78,2.0,156.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Estrela_do_Sul,2012-12-31T00:00:00,2.22,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Estrela_do_Sul,2013-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Estrela_do_Sul,2014-12-31T00:00:00,1.98,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Estrela_do_Sul,2015-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Estrela_do_Sul,2016-12-31T00:00:00,2.52,2.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Estrela_do_Sul,2017-12-31T00:00:00,2.38,2.0,15.97,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Estrela_do_Sul,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Estrela_do_Sul,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Estrela_do_Sul,2020-12-31T00:00:00,2.06,2.0,2.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Estrela_do_Sul,2021-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Fervedouro,2012-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Fervedouro,2013-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Fervedouro,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Fervedouro,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Fervedouro,2016-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Fervedouro,2017-12-31T00:00:00,1.06,1.0,5.66,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Fervedouro,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Fervedouro,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Fervedouro,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Fervedouro,2021-12-31T00:00:00,1.15,1.0,13.04,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Formiga,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Formiga,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Formiga,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Formiga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Formiga,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Formiga,2017-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Formiga,2018-12-31T00:00:00,2.4,1.0,58.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Formiga,2019-12-31T00:00:00,1.24,2.0,61.29,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Formiga,2020-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Formiga,2021-12-31T00:00:00,1.35,2.0,48.15,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guape,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guape,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guape,2014-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guape,2015-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guape,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guape,2017-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guape,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guape,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guape,2020-12-31T00:00:00,1.85,2.0,8.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guape,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaranesia,2012-12-31T00:00:00,1.98,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaranesia,2013-12-31T00:00:00,1.52,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaranesia,2014-12-31T00:00:00,1.86,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaranesia,2015-12-31T00:00:00,1.65,2.0,21.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaranesia,2016-12-31T00:00:00,2.63,2.0,23.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaranesia,2017-12-31T00:00:00,1.36,2.0,47.06,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaranesia,2018-12-31T00:00:00,1.7,2.0,17.65,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaranesia,2019-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaranesia,2020-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaranesia,2021-12-31T00:00:00,1.03,1.0,2.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaxupe,2012-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaxupe,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaxupe,2014-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaxupe,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaxupe,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaxupe,2017-12-31T00:00:00,1.51,2.0,32.45,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaxupe,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaxupe,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaxupe,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaxupe,2021-12-31T00:00:00,1.02,2.0,96.08,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guimarania,2012-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guimarania,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guimarania,2014-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guimarania,2015-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guimarania,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guimarania,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guimarania,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guimarania,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guimarania,2020-12-31T00:00:00,2.15,2.0,6.98,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guimarania,2021-12-31T00:00:00,1.54,2.0,29.87,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Heliodora,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Heliodora,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Heliodora,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Heliodora,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Heliodora,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Heliodora,2017-12-31T00:00:00,1.57,1.0,36.31,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Heliodora,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Heliodora,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Heliodora,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Heliodora,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibia,2012-12-31T00:00:00,1.74,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibia,2013-12-31T00:00:00,1.74,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibia,2014-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibia,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibia,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibia,2017-12-31T00:00:00,1.59,2.0,25.79,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibia,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibia,2020-12-31T00:00:00,1.75,2.0,14.29,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibia,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibiraci,2012-12-31T00:00:00,2.49,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibiraci,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibiraci,2014-12-31T00:00:00,2.01,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibiraci,2015-12-31T00:00:00,1.08,2.0,85.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibiraci,2016-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibiraci,2017-12-31T00:00:00,1.61,2.0,24.22,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibiraci,2018-12-31T00:00:00,3.0,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibiraci,2019-12-31T00:00:00,1.28,2.0,56.25,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibiraci,2020-12-31T00:00:00,2.63,2.0,23.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibiraci,2021-12-31T00:00:00,1.33,2.0,50.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibitiura_de_Minas,2012-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibitiura_de_Minas,2013-12-31T00:00:00,2.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibitiura_de_Minas,2014-12-31T00:00:00,2.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibitiura_de_Minas,2015-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibitiura_de_Minas,2016-12-31T00:00:00,1.76,2.0,13.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibitiura_de_Minas,2017-12-31T00:00:00,1.3,2.0,53.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibitiura_de_Minas,2018-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibitiura_de_Minas,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibitiura_de_Minas,2020-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibitiura_de_Minas,2021-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ilicinea,2012-12-31T00:00:00,1.69,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ilicinea,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ilicinea,2014-12-31T00:00:00,1.35,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ilicinea,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ilicinea,2016-12-31T00:00:00,2.46,1.0,59.35,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ilicinea,2017-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ilicinea,2018-12-31T00:00:00,2.46,2.0,18.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ilicinea,2019-12-31T00:00:00,1.86,2.0,7.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ilicinea,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ilicinea,2021-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Imbe_de_Minas,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Imbe_de_Minas,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Imbe_de_Minas,2014-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Imbe_de_Minas,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Imbe_de_Minas,2016-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Imbe_de_Minas,2017-12-31T00:00:00,1.29,1.0,22.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Imbe_de_Minas,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Imbe_de_Minas,2019-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Imbe_de_Minas,2020-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Imbe_de_Minas,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inconfidentes,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inconfidentes,2013-12-31T00:00:00,1.4,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inconfidentes,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inconfidentes,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inconfidentes,2016-12-31T00:00:00,2.1,1.0,52.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inconfidentes,2017-12-31T00:00:00,1.3,1.0,23.08,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inconfidentes,2018-12-31T00:00:00,2.88,2.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inconfidentes,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inconfidentes,2020-12-31T00:00:00,2.46,2.0,18.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inconfidentes,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Indianopolis,2012-12-31T00:00:00,2.4,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Indianopolis,2013-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Indianopolis,2014-12-31T00:00:00,2.45,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Indianopolis,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Indianopolis,2016-12-31T00:00:00,2.7,2.0,25.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Indianopolis,2017-12-31T00:00:00,1.67,2.0,19.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Indianopolis,2018-12-31T00:00:00,2.52,2.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Indianopolis,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Indianopolis,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Indianopolis,2021-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inhapim,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inhapim,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inhapim,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inhapim,2015-12-31T00:00:00,1.21,1.0,17.36,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inhapim,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inhapim,2017-12-31T00:00:00,1.12,1.0,10.71,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inhapim,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inhapim,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inhapim,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inhapim,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Itamogi,2012-12-31T00:00:00,1.95,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Itamogi,2013-12-31T00:00:00,1.17,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Itamogi,2014-12-31T00:00:00,1.6,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Itamogi,2015-12-31T00:00:00,1.58,2.0,26.58,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Itamogi,2016-12-31T00:00:00,1.95,1.0,48.72,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Itamogi,2017-12-31T00:00:00,1.42,2.0,40.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Itamogi,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Itamogi,2019-12-31T00:00:00,1.85,2.0,8.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Itamogi,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Itamogi,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacui,2012-12-31T00:00:00,1.65,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacui,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacui,2014-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacui,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacui,2016-12-31T00:00:00,1.7,1.0,41.18,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacui,2017-12-31T00:00:00,1.73,2.0,15.61,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacui,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacui,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacui,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacui,2021-12-31T00:00:00,0.96,2.0,108.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacutinga,2012-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacutinga,2013-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacutinga,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacutinga,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacutinga,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacutinga,2017-12-31T00:00:00,1.37,1.0,27.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacutinga,2018-12-31T00:00:00,1.74,1.0,42.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacutinga,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacutinga,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacutinga,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jequeri,2012-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jequeri,2013-12-31T00:00:00,1.86,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jequeri,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jequeri,2015-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jequeri,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jequeri,2017-12-31T00:00:00,1.28,1.0,21.88,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jequeri,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jequeri,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jequeri,2020-12-31T00:00:00,2.1,1.0,52.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jequeri,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Joao_Pinheiro,2012-12-31T00:00:00,2.82,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Joao_Pinheiro,2013-12-31T00:00:00,3.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Joao_Pinheiro,2014-12-31T00:00:00,2.7,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Joao_Pinheiro,2015-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Joao_Pinheiro,2016-12-31T00:00:00,3.6,3.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Joao_Pinheiro,2017-12-31T00:00:00,1.91,3.0,57.07,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Joao_Pinheiro,2018-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Joao_Pinheiro,2019-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Joao_Pinheiro,2020-12-31T00:00:00,3.0,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Joao_Pinheiro,2021-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Juruaia,2012-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Juruaia,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Juruaia,2014-12-31T00:00:00,1.72,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Juruaia,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Juruaia,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Juruaia,2017-12-31T00:00:00,1.69,2.0,18.34,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Juruaia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Juruaia,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Juruaia,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Juruaia,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lajinha,2012-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lajinha,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lajinha,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lajinha,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lajinha,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lajinha,2017-12-31T00:00:00,1.04,1.0,3.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lajinha,2018-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lajinha,2019-12-31T00:00:00,1.06,1.0,5.66,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lajinha,2020-12-31T00:00:00,1.92,1.0,47.92,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lajinha,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lambari,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lambari,2013-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lambari,2014-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lambari,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lambari,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lambari,2017-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lambari,2018-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lambari,2019-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lambari,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lambari,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lavras,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lavras,2013-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lavras,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lavras,2015-12-31T00:00:00,0.84,1.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lavras,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lavras,2017-12-31T00:00:00,2.25,1.0,55.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lavras,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lavras,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lavras,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lavras,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Luisburgo,2012-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Luisburgo,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Luisburgo,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Luisburgo,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Luisburgo,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Luisburgo,2017-12-31T00:00:00,1.53,1.0,34.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Luisburgo,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Luisburgo,2019-12-31T00:00:00,1.64,2.0,21.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Luisburgo,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Luisburgo,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Machado,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Machado,2013-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Machado,2014-12-31T00:00:00,1.15,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Machado,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Machado,2016-12-31T00:00:00,1.86,1.0,46.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Machado,2017-12-31T00:00:00,1.51,1.0,33.77,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Machado,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Machado,2019-12-31T00:00:00,1.53,2.0,30.72,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Machado,2020-12-31T00:00:00,2.21,2.0,9.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Machado,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhuacu,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhuacu,2013-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhuacu,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhuacu,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhuacu,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhuacu,2017-12-31T00:00:00,1.43,1.0,30.07,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhuacu,2018-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhuacu,2019-12-31T00:00:00,1.02,1.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhuacu,2020-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhuacu,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhumirim,2012-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhumirim,2013-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhumirim,2014-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhumirim,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhumirim,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhumirim,2017-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhumirim,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhumirim,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhumirim,2020-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhumirim,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Martins_Soares,2012-12-31T00:00:00,2.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Martins_Soares,2013-12-31T00:00:00,1.98,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Martins_Soares,2014-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Martins_Soares,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Martins_Soares,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Martins_Soares,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Martins_Soares,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Martins_Soares,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Martins_Soares,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Martins_Soares,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Matipo,2012-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Matipo,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Matipo,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Matipo,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Matipo,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Matipo,2017-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Matipo,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Matipo,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Matipo,2020-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Matipo,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Medeiros,2012-12-31T00:00:00,2.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Medeiros,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Medeiros,2014-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Medeiros,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Medeiros,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Medeiros,2017-12-31T00:00:00,1.79,2.0,11.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Medeiros,2018-12-31T00:00:00,2.04,2.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Medeiros,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Medeiros,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Medeiros,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Miradouro,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Miradouro,2013-12-31T00:00:00,2.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Miradouro,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Miradouro,2015-12-31T00:00:00,1.59,1.0,37.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Miradouro,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Miradouro,2017-12-31T00:00:00,1.16,1.0,13.79,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Miradouro,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Miradouro,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Miradouro,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Miradouro,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monsenhor_Paulo,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monsenhor_Paulo,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monsenhor_Paulo,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monsenhor_Paulo,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monsenhor_Paulo,2016-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monsenhor_Paulo,2017-12-31T00:00:00,1.72,1.0,41.86,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monsenhor_Paulo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monsenhor_Paulo,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monsenhor_Paulo,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monsenhor_Paulo,2021-12-31T00:00:00,1.41,2.0,41.84,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Belo,2012-12-31T00:00:00,1.63,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Belo,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Belo,2014-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Belo,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Belo,2016-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Belo,2017-12-31T00:00:00,2.43,1.0,58.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Belo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Belo,2019-12-31T00:00:00,1.64,2.0,21.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Belo,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Belo,2021-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Carmelo,2012-12-31T00:00:00,2.34,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Carmelo,2013-12-31T00:00:00,1.89,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Carmelo,2014-12-31T00:00:00,2.28,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Carmelo,2015-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Carmelo,2016-12-31T00:00:00,2.64,2.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Carmelo,2017-12-31T00:00:00,1.84,2.0,8.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Carmelo,2018-12-31T00:00:00,2.31,2.0,13.42,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Carmelo,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Carmelo,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Carmelo,2021-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Santo_de_Minas,2012-12-31T00:00:00,1.89,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Santo_de_Minas,2013-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Santo_de_Minas,2014-12-31T00:00:00,1.6,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Santo_de_Minas,2015-12-31T00:00:00,1.35,2.0,48.15,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Santo_de_Minas,2016-12-31T00:00:00,2.04,2.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Santo_de_Minas,2017-12-31T00:00:00,1.08,2.0,85.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Santo_de_Minas,2018-12-31T00:00:00,1.88,1.0,46.81,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Santo_de_Minas,2019-12-31T00:00:00,1.59,2.0,25.79,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Santo_de_Minas,2020-12-31T00:00:00,2.12,2.0,5.66,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Santo_de_Minas,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Siao,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Siao,2013-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Siao,2014-12-31T00:00:00,0.66,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Siao,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Siao,2016-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Siao,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Siao,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Siao,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Siao,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Siao,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Mutum,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Mutum,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Mutum,2014-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Mutum,2015-12-31T00:00:00,1.15,1.0,13.04,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Mutum,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Mutum,2017-12-31T00:00:00,1.45,1.0,31.03,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Mutum,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Mutum,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Mutum,2020-12-31T00:00:00,1.65,1.0,39.39,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Mutum,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Muzambinho,2012-12-31T00:00:00,1.74,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Muzambinho,2013-12-31T00:00:00,1.35,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Muzambinho,2014-12-31T00:00:00,1.35,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Muzambinho,2015-12-31T00:00:00,1.48,1.0,32.43,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Muzambinho,2016-12-31T00:00:00,1.98,1.0,49.49,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Muzambinho,2017-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Muzambinho,2018-12-31T00:00:00,1.3,2.0,53.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Muzambinho,2019-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Muzambinho,2020-12-31T00:00:00,1.76,2.0,13.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Muzambinho,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Natercia,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Natercia,2013-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Natercia,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Natercia,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Natercia,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Natercia,2017-12-31T00:00:00,1.6,1.0,37.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Natercia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Natercia,2019-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Natercia,2020-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Natercia,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nazareno,2012-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nazareno,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nazareno,2014-12-31T00:00:00,1.3,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nazareno,2015-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nazareno,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nazareno,2017-12-31T00:00:00,1.59,2.0,25.79,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nazareno,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nazareno,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nazareno,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nazareno,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nepomuceno,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nepomuceno,2013-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nepomuceno,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nepomuceno,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nepomuceno,2016-12-31T00:00:00,1.74,1.0,42.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nepomuceno,2017-12-31T00:00:00,1.69,1.0,40.83,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nepomuceno,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nepomuceno,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nepomuceno,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nepomuceno,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ninheira,2012-12-31T00:00:00,3.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ninheira,2013-12-31T00:00:00,3.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ninheira,2014-12-31T00:00:00,3.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ninheira,2015-12-31T00:00:00,2.76,3.0,8.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ninheira,2016-12-31T00:00:00,3.0,3.0,0.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ninheira,2017-12-31T00:00:00,2.34,3.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ninheira,2018-12-31T00:00:00,3.0,3.0,0.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ninheira,2019-12-31T00:00:00,2.7,3.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ninheira,2020-12-31T00:00:00,3.0,3.0,0.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ninheira,2021-12-31T00:00:00,2.1,3.0,42.86,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Belem,2012-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Belem,2013-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Belem,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Belem,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Belem,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Belem,2017-12-31T00:00:00,0.48,1.0,108.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Belem,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Belem,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Belem,2020-12-31T00:00:00,1.1,1.0,9.09,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Belem,2021-12-31T00:00:00,1.0,1.0,0.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Resende,2012-12-31T00:00:00,2.88,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Resende,2013-12-31T00:00:00,2.18,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Resende,2014-12-31T00:00:00,1.7,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Resende,2015-12-31T00:00:00,2.2,2.0,9.09,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Resende,2016-12-31T00:00:00,2.08,2.0,3.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Resende,2017-12-31T00:00:00,1.76,2.0,13.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Resende,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Resende,2019-12-31T00:00:00,2.12,2.0,5.66,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Resende,2020-12-31T00:00:00,2.35,2.0,14.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Resende,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Oliveira,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Oliveira,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Oliveira,2014-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Oliveira,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Oliveira,2016-12-31T00:00:00,1.74,1.0,42.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Oliveira,2017-12-31T00:00:00,1.4,1.0,28.57,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Oliveira,2018-12-31T00:00:00,2.1,1.0,52.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Oliveira,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Oliveira,2020-12-31T00:00:00,1.95,2.0,2.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Oliveira,2021-12-31T00:00:00,1.54,2.0,29.87,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Orizania,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Orizania,2013-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Orizania,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Orizania,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Orizania,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Orizania,2017-12-31T00:00:00,1.47,1.0,31.97,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Orizania,2018-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Orizania,2019-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Orizania,2020-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Orizania,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ouro_Fino,2012-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ouro_Fino,2013-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ouro_Fino,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ouro_Fino,2015-12-31T00:00:00,1.29,1.0,22.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ouro_Fino,2016-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ouro_Fino,2017-12-31T00:00:00,1.42,1.0,29.58,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ouro_Fino,2018-12-31T00:00:00,1.69,1.0,40.83,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ouro_Fino,2019-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ouro_Fino,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ouro_Fino,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paracatu,2012-12-31T00:00:00,3.45,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paracatu,2013-12-31T00:00:00,3.45,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paracatu,2014-12-31T00:00:00,3.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paracatu,2015-12-31T00:00:00,1.8,3.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paracatu,2016-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paracatu,2017-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paracatu,2018-12-31T00:00:00,3.0,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paracatu,2019-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paracatu,2020-12-31T00:00:00,2.7,3.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paracatu,2021-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paraguacu,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paraguacu,2013-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paraguacu,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paraguacu,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paraguacu,2016-12-31T00:00:00,1.98,1.0,49.49,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paraguacu,2017-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paraguacu,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paraguacu,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paraguacu,2020-12-31T00:00:00,2.34,2.0,14.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paraguacu,2021-12-31T00:00:00,0.97,2.0,106.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Passos,2012-12-31T00:00:00,2.16,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Passos,2013-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Passos,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Passos,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Passos,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Passos,2017-12-31T00:00:00,1.92,1.0,47.92,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Passos,2018-12-31T00:00:00,2.1,1.0,52.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Passos,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Passos,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Passos,2021-12-31T00:00:00,0.99,2.0,102.02,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patos_de_Minas,2012-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patos_de_Minas,2013-12-31T00:00:00,2.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patos_de_Minas,2014-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patos_de_Minas,2015-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patos_de_Minas,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patos_de_Minas,2017-12-31T00:00:00,1.7,2.0,17.65,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patos_de_Minas,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patos_de_Minas,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patos_de_Minas,2020-12-31T00:00:00,1.94,2.0,3.09,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patos_de_Minas,2021-12-31T00:00:00,1.72,2.0,16.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patrocinio,2012-12-31T00:00:00,2.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patrocinio,2013-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patrocinio,2014-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patrocinio,2015-12-31T00:00:00,1.25,2.0,60.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patrocinio,2016-12-31T00:00:00,2.64,2.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patrocinio,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patrocinio,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patrocinio,2019-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patrocinio,2020-12-31T00:00:00,1.75,2.0,14.29,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patrocinio,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedra_Bonita,2012-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedra_Bonita,2013-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedra_Bonita,2014-12-31T00:00:00,0.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedra_Bonita,2015-12-31T00:00:00,0.6,1.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedra_Bonita,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedra_Bonita,2017-12-31T00:00:00,0.86,1.0,16.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedra_Bonita,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedra_Bonita,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedra_Bonita,2020-12-31T00:00:00,1.74,1.0,42.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedra_Bonita,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedralva,2012-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedralva,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedralva,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedralva,2015-12-31T00:00:00,0.96,2.0,108.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedralva,2016-12-31T00:00:00,2.28,1.0,56.14,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedralva,2017-12-31T00:00:00,1.43,1.0,30.07,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedralva,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedralva,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedralva,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedralva,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdizes,2012-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdizes,2013-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdizes,2014-12-31T00:00:00,1.66,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdizes,2015-12-31T00:00:00,1.48,2.0,35.14,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdizes,2016-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdizes,2017-12-31T00:00:00,2.07,2.0,3.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdizes,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdizes,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdizes,2020-12-31T00:00:00,2.05,2.0,2.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdizes,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdoes,2012-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdoes,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdoes,2014-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdoes,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdoes,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdoes,2017-12-31T00:00:00,1.84,1.0,45.65,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdoes,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdoes,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdoes,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdoes,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piedade_de_Caratinga,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piedade_de_Caratinga,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piedade_de_Caratinga,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piedade_de_Caratinga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piedade_de_Caratinga,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piedade_de_Caratinga,2017-12-31T00:00:00,1.59,1.0,37.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piedade_de_Caratinga,2018-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piedade_de_Caratinga,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piedade_de_Caratinga,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piedade_de_Caratinga,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pimenta,2012-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pimenta,2013-12-31T00:00:00,1.74,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pimenta,2014-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pimenta,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pimenta,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pimenta,2017-12-31T00:00:00,1.73,2.0,15.61,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pimenta,2018-12-31T00:00:00,2.7,2.0,25.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pimenta,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pimenta,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pimenta,2021-12-31T00:00:00,1.35,2.0,48.15,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piumhi,2012-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piumhi,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piumhi,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piumhi,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piumhi,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piumhi,2017-12-31T00:00:00,1.36,1.0,26.47,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piumhi,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piumhi,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piumhi,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piumhi,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Poco_Fundo,2012-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Poco_Fundo,2013-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Poco_Fundo,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Poco_Fundo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Poco_Fundo,2016-12-31T00:00:00,1.41,1.0,29.08,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Poco_Fundo,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Poco_Fundo,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Poco_Fundo,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Poco_Fundo,2020-12-31T00:00:00,1.74,1.0,42.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Poco_Fundo,2021-12-31T00:00:00,0.96,2.0,108.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pocos_de_Caldas,2012-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pocos_de_Caldas,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pocos_de_Caldas,2014-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pocos_de_Caldas,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pocos_de_Caldas,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pocos_de_Caldas,2017-12-31T00:00:00,1.77,1.0,43.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pocos_de_Caldas,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pocos_de_Caldas,2019-12-31T00:00:00,1.08,2.0,85.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pocos_de_Caldas,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pocos_de_Caldas,2021-12-31T00:00:00,1.42,1.0,29.58,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pratinha,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pratinha,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pratinha,2014-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pratinha,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pratinha,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pratinha,2017-12-31T00:00:00,1.72,2.0,16.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pratinha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pratinha,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pratinha,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pratinha,2021-12-31T00:00:00,1.35,2.0,48.15,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Presidente_Olegario,2012-12-31T00:00:00,2.66,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Presidente_Olegario,2013-12-31T00:00:00,2.65,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Presidente_Olegario,2014-12-31T00:00:00,2.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Presidente_Olegario,2015-12-31T00:00:00,1.88,3.0,59.57,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Presidente_Olegario,2016-12-31T00:00:00,2.52,2.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Presidente_Olegario,2017-12-31T00:00:00,1.84,2.0,8.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Presidente_Olegario,2018-12-31T00:00:00,2.28,2.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Presidente_Olegario,2019-12-31T00:00:00,1.87,2.0,6.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Presidente_Olegario,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Presidente_Olegario,2021-12-31T00:00:00,1.64,2.0,21.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Raul_Soares,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Raul_Soares,2013-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Raul_Soares,2014-12-31T00:00:00,0.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Raul_Soares,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Raul_Soares,2016-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Raul_Soares,2017-12-31T00:00:00,0.86,1.0,16.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Raul_Soares,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Raul_Soares,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Raul_Soares,2020-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Raul_Soares,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Reduto,2012-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Reduto,2013-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Reduto,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Reduto,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Reduto,2016-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Reduto,2017-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Reduto,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Reduto,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Reduto,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Reduto,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Rio_Paranaiba,2012-12-31T00:00:00,2.16,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Rio_Paranaiba,2013-12-31T00:00:00,2.07,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Rio_Paranaiba,2014-12-31T00:00:00,1.89,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Rio_Paranaiba,2015-12-31T00:00:00,1.42,2.0,40.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Rio_Paranaiba,2016-12-31T00:00:00,2.08,2.0,3.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Rio_Paranaiba,2017-12-31T00:00:00,1.39,2.0,43.88,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Rio_Paranaiba,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Rio_Paranaiba,2019-12-31T00:00:00,1.14,2.0,75.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Rio_Paranaiba,2020-12-31T00:00:00,1.82,2.0,9.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Rio_Paranaiba,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Romaria,2012-12-31T00:00:00,2.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Romaria,2013-12-31T00:00:00,2.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Romaria,2014-12-31T00:00:00,2.28,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Romaria,2015-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Romaria,2016-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Romaria,2017-12-31T00:00:00,1.53,2.0,30.72,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Romaria,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Romaria,2019-12-31T00:00:00,2.2,2.0,9.09,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Romaria,2020-12-31T00:00:00,2.33,2.0,14.16,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Romaria,2021-12-31T00:00:00,2.05,2.0,2.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sacramento,2012-12-31T00:00:00,2.22,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sacramento,2013-12-31T00:00:00,1.87,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sacramento,2014-12-31T00:00:00,1.53,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sacramento,2015-12-31T00:00:00,1.34,2.0,49.25,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sacramento,2016-12-31T00:00:00,1.87,2.0,6.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sacramento,2017-12-31T00:00:00,1.31,2.0,52.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sacramento,2018-12-31T00:00:00,1.87,2.0,6.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sacramento,2019-12-31T00:00:00,1.23,2.0,62.6,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sacramento,2020-12-31T00:00:00,1.83,1.0,45.36,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sacramento,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Barbara_do_Leste,2012-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Barbara_do_Leste,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Barbara_do_Leste,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Barbara_do_Leste,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Barbara_do_Leste,2016-12-31T00:00:00,1.02,1.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Barbara_do_Leste,2017-12-31T00:00:00,0.91,1.0,9.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Barbara_do_Leste,2018-12-31T00:00:00,1.74,1.0,42.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Barbara_do_Leste,2019-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Barbara_do_Leste,2020-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Barbara_do_Leste,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Margarida,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Margarida,2013-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Margarida,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Margarida,2015-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Margarida,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Margarida,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Margarida,2018-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Margarida,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Margarida,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Margarida,2021-12-31T00:00:00,0.9,2.0,122.22,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_de_Minas,2012-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_de_Minas,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_de_Minas,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_de_Minas,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_de_Minas,2016-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_de_Minas,2017-12-31T00:00:00,1.25,1.0,20.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_de_Minas,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_de_Minas,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_de_Minas,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_de_Minas,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Itueto,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Itueto,2013-12-31T00:00:00,1.17,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Itueto,2014-12-31T00:00:00,1.11,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Itueto,2015-12-31T00:00:00,0.92,1.0,8.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Itueto,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Itueto,2017-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Itueto,2018-12-31T00:00:00,1.35,1.0,25.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Itueto,2019-12-31T00:00:00,1.65,1.0,39.39,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Itueto,2020-12-31T00:00:00,1.77,1.0,43.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Itueto,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2017-12-31T00:00:00,1.79,1.0,44.13,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rosa_da_Serra,2012-12-31T00:00:00,3.17,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rosa_da_Serra,2013-12-31T00:00:00,3.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rosa_da_Serra,2014-12-31T00:00:00,2.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rosa_da_Serra,2015-12-31T00:00:00,1.62,3.0,85.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rosa_da_Serra,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rosa_da_Serra,2017-12-31T00:00:00,1.63,2.0,22.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rosa_da_Serra,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rosa_da_Serra,2019-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rosa_da_Serra,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rosa_da_Serra,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_da_Vargem,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_da_Vargem,2013-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_da_Vargem,2014-12-31T00:00:00,1.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_da_Vargem,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_da_Vargem,2016-12-31T00:00:00,1.74,1.0,42.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_da_Vargem,2017-12-31T00:00:00,2.09,1.0,52.15,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_da_Vargem,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_da_Vargem,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_da_Vargem,2020-12-31T00:00:00,2.22,2.0,9.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_da_Vargem,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_do_Manhuacu,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_do_Manhuacu,2013-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_do_Manhuacu,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_do_Manhuacu,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_do_Manhuacu,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_do_Manhuacu,2017-12-31T00:00:00,1.28,1.0,21.88,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_do_Manhuacu,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_do_Manhuacu,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_do_Manhuacu,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_do_Manhuacu,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2012-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2013-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2014-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2017-12-31T00:00:00,1.34,1.0,25.37,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2018-12-31T00:00:00,2.16,1.0,53.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2019-12-31T00:00:00,1.64,2.0,21.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Domingos_das_Dores,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Domingos_das_Dores,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Domingos_das_Dores,2014-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Domingos_das_Dores,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Domingos_das_Dores,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Domingos_das_Dores,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Domingos_das_Dores,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Domingos_das_Dores,2019-12-31T00:00:00,1.3,2.0,53.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Domingos_das_Dores,2020-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Domingos_das_Dores,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Francisco_de_Paula,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Francisco_de_Paula,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Francisco_de_Paula,2014-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Francisco_de_Paula,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Francisco_de_Paula,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Francisco_de_Paula,2017-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Francisco_de_Paula,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Francisco_de_Paula,2019-12-31T00:00:00,1.64,2.0,21.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Francisco_de_Paula,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Francisco_de_Paula,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2012-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2013-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2017-12-31T00:00:00,2.04,1.0,50.98,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2021-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Gotardo,2012-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Gotardo,2013-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Gotardo,2014-12-31T00:00:00,1.71,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Gotardo,2015-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Gotardo,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Gotardo,2017-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Gotardo,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Gotardo,2019-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Gotardo,2020-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Gotardo,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2012-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2013-12-31T00:00:00,1.74,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2017-12-31T00:00:00,1.7,1.0,41.18,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2018-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2021-12-31T00:00:00,0.9,2.0,122.22,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Jose_da_Barra,2012-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Jose_da_Barra,2013-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Jose_da_Barra,2014-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Jose_da_Barra,2015-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Jose_da_Barra,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Jose_da_Barra,2017-12-31T00:00:00,1.91,1.0,47.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Jose_da_Barra,2018-12-31T00:00:00,2.1,1.0,52.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Jose_da_Barra,2019-12-31T00:00:00,1.7,2.0,17.65,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Jose_da_Barra,2020-12-31T00:00:00,2.54,2.0,21.26,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Jose_da_Barra,2021-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2012-12-31T00:00:00,2.28,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2014-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2016-12-31T00:00:00,1.92,1.0,47.92,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2017-12-31T00:00:00,1.69,2.0,18.34,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2021-12-31T00:00:00,1.48,2.0,35.14,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Roque_de_Minas,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Roque_de_Minas,2013-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Roque_de_Minas,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Roque_de_Minas,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Roque_de_Minas,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Roque_de_Minas,2017-12-31T00:00:00,1.46,1.0,31.51,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Roque_de_Minas,2018-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Roque_de_Minas,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Roque_de_Minas,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Roque_de_Minas,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2013-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2017-12-31T00:00:00,1.78,1.0,43.82,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2018-12-31T00:00:00,1.64,1.0,39.02,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2012-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2013-12-31T00:00:00,1.71,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2014-12-31T00:00:00,1.42,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2015-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2016-12-31T00:00:00,1.7,2.0,17.65,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2017-12-31T00:00:00,1.54,1.0,35.06,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2018-12-31T00:00:00,1.89,2.0,5.82,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2019-12-31T00:00:00,1.47,2.0,36.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2020-12-31T00:00:00,1.93,2.0,3.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2021-12-31T00:00:00,1.14,2.0,75.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2012-12-31T00:00:00,1.75,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2013-12-31T00:00:00,1.05,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2014-12-31T00:00:00,1.35,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2017-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2018-12-31T00:00:00,2.1,1.0,52.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sericita,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sericita,2013-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sericita,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sericita,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sericita,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sericita,2017-12-31T00:00:00,1.12,1.0,10.71,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sericita,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sericita,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sericita,2020-12-31T00:00:00,2.52,1.0,60.32,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sericita,2021-12-31T00:00:00,0.9,2.0,122.22,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serra_do_Salitre,2012-12-31T00:00:00,3.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serra_do_Salitre,2013-12-31T00:00:00,1.74,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serra_do_Salitre,2014-12-31T00:00:00,2.22,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serra_do_Salitre,2015-12-31T00:00:00,1.29,2.0,55.04,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serra_do_Salitre,2016-12-31T00:00:00,3.0,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serra_do_Salitre,2017-12-31T00:00:00,1.12,2.0,78.57,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serra_do_Salitre,2018-12-31T00:00:00,2.34,2.0,14.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serra_do_Salitre,2019-12-31T00:00:00,1.49,2.0,34.23,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serra_do_Salitre,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serra_do_Salitre,2021-12-31T00:00:00,1.47,2.0,36.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serrania,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serrania,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serrania,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serrania,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serrania,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serrania,2017-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serrania,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serrania,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serrania,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serrania,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Simonesia,2012-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Simonesia,2013-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Simonesia,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Simonesia,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Simonesia,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Simonesia,2017-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Simonesia,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Simonesia,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Simonesia,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Simonesia,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tiros,2012-12-31T00:00:00,2.28,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tiros,2013-12-31T00:00:00,2.28,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tiros,2014-12-31T00:00:00,1.79,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tiros,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tiros,2016-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tiros,2017-12-31T00:00:00,3.0,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tiros,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tiros,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tiros,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tiros,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Coracoes,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Coracoes,2013-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Coracoes,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Coracoes,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Coracoes,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Coracoes,2017-12-31T00:00:00,2.67,1.0,62.55,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Coracoes,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Coracoes,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Coracoes,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Coracoes,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Pontas,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Pontas,2013-12-31T00:00:00,1.53,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Pontas,2014-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Pontas,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Pontas,2016-12-31T00:00:00,1.86,1.0,46.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Pontas,2017-12-31T00:00:00,1.88,1.0,46.81,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Pontas,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Pontas,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Pontas,2020-12-31T00:00:00,2.26,2.0,11.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Pontas,2021-12-31T00:00:00,1.35,2.0,48.15,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Turvolandia,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Turvolandia,2013-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Turvolandia,2014-12-31T00:00:00,0.84,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Turvolandia,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Turvolandia,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Turvolandia,2017-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Turvolandia,2018-12-31T00:00:00,2.29,1.0,56.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Turvolandia,2019-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Turvolandia,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Turvolandia,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ubaporanga,2012-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ubaporanga,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ubaporanga,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ubaporanga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ubaporanga,2016-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ubaporanga,2017-12-31T00:00:00,1.03,1.0,2.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ubaporanga,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ubaporanga,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ubaporanga,2020-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ubaporanga,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Unai,2012-12-31T00:00:00,2.94,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Unai,2013-12-31T00:00:00,3.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Unai,2014-12-31T00:00:00,2.94,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Unai,2015-12-31T00:00:00,2.64,3.0,13.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Unai,2016-12-31T00:00:00,2.7,3.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Unai,2017-12-31T00:00:00,2.6,3.0,15.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Unai,2018-12-31T00:00:00,2.52,3.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Unai,2019-12-31T00:00:00,2.82,3.0,6.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Unai,2020-12-31T00:00:00,2.34,3.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Unai,2021-12-31T00:00:00,2.64,3.0,13.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varginha,2012-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varginha,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varginha,2014-12-31T00:00:00,1.03,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varginha,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varginha,2016-12-31T00:00:00,1.74,1.0,42.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varginha,2017-12-31T00:00:00,2.12,1.0,52.83,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varginha,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varginha,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varginha,2020-12-31T00:00:00,1.9,2.0,5.26,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varginha,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varjao_de_Minas,2012-12-31T00:00:00,2.4,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varjao_de_Minas,2013-12-31T00:00:00,2.4,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varjao_de_Minas,2014-12-31T00:00:00,3.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varjao_de_Minas,2015-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varjao_de_Minas,2016-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varjao_de_Minas,2017-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varjao_de_Minas,2018-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varjao_de_Minas,2019-12-31T00:00:00,2.49,2.0,19.68,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varjao_de_Minas,2020-12-31T00:00:00,2.52,2.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varjao_de_Minas,2021-12-31T00:00:00,2.39,2.0,16.32,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Vermelho_Novo,2012-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Vermelho_Novo,2013-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Vermelho_Novo,2014-12-31T00:00:00,0.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Vermelho_Novo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Vermelho_Novo,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Vermelho_Novo,2017-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Vermelho_Novo,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Vermelho_Novo,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Vermelho_Novo,2020-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Vermelho_Novo,2021-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Abre_Campo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alfenas,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alpinopolis,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alterosa,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Caparao,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Jequitiba,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Andradas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Angelandia,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araguari,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araponga,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araxa,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Areado,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bambui,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Boa_Esperanca,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Jesus_da_Penha,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Sucesso,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Botelhos,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bueno_Brandao,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Buritizeiro,2022-12-31T00:00:00,3.0,3,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cabo_Verde,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caiana,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cambuquira,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campanha,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campestre,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_Belo,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_do_Meio,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Altos,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Gerais,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Candeias,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caparao,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capelinha,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capetinga,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capitolio,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caputira,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carangola,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caratinga,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_da_Cachoeira,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_de_Minas,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Paranaiba,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Rio_Claro,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cassia,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Chale,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Claraval,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_da_Aparecida,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conselheiro_Pena,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coqueiral,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coromandel,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cristais,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divino,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divisa_Nova,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Durande,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Eloi_Mendes,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ervalia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Espera_Feliz,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Estrela_do_Sul,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Fervedouro,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Formiga,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guape,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaranesia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaxupe,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guimarania,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Heliodora,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibiraci,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibitiura_de_Minas,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ilicinea,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Imbe_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inconfidentes,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Indianopolis,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inhapim,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Itamogi,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacui,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacutinga,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jequeri,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Joao_Pinheiro,2022-12-31T00:00:00,2.0,3,50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Juruaia,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lajinha,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lambari,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lavras,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Luisburgo,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Machado,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhuacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhumirim,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Martins_Soares,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Matipo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Medeiros,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Miradouro,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monsenhor_Paulo,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Belo,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Carmelo,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Santo_de_Minas,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Siao,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Mutum,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Muzambinho,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Natercia,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nazareno,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nepomuceno,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ninheira,2022-12-31T00:00:00,3.0,3,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Belem,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Resende,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Oliveira,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Orizania,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ouro_Fino,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paracatu,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paraguacu,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Passos,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patos_de_Minas,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patrocinio,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedra_Bonita,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedralva,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdizes,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdoes,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piedade_de_Caratinga,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pimenta,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piumhi,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Poco_Fundo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pocos_de_Caldas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pratinha,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Presidente_Olegario,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Raul_Soares,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Reduto,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Rio_Paranaiba,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Romaria,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sacramento,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Barbara_do_Leste,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Margarida,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Itueto,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rosa_da_Serra,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_da_Vargem,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_do_Manhuacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Domingos_das_Dores,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Francisco_de_Paula,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Gotardo,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Jose_da_Barra,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Roque_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sericita,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serra_do_Salitre,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serrania,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Simonesia,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tiros,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Coracoes,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Pontas,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Turvolandia,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ubaporanga,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Unai,2022-12-31T00:00:00,3.0,3,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varginha,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varjao_de_Minas,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Vermelho_Novo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Abre_Campo,2023-12-31T00:00:00,1.8000000000000005,1.2899550199508667,-28.34,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alfenas,2023-12-31T00:00:00,1.698430922311519,1.5097988843917847,-11.11,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alpinopolis,2023-12-31T00:00:00,1.8000000000000005,1.8698647022247314,3.88,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alterosa,2023-12-31T00:00:00,1.2603960396039595,1.319612979888916,4.7,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Caparao,2023-12-31T00:00:00,1.32,1.4759234189987183,11.81,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Alto_Jequitiba,2023-12-31T00:00:00,1.08,1.15726637840271,7.15,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Andradas,2023-12-31T00:00:00,1.22,1.5481799840927124,26.9,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Angelandia,2023-12-31T00:00:00,2.095663265306122,1.7510454654693604,-16.44,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araguari,2023-12-31T00:00:00,3.0,1.9146002531051636,-36.18,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araponga,2023-12-31T00:00:00,1.5,1.3939964771270752,-7.07,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Araxa,2023-12-31T00:00:00,1.638401296246287,1.3249577283859253,-19.13,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Areado,2023-12-31T00:00:00,1.590034364261168,1.4167265892028809,-10.9,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bambui,2023-12-31T00:00:00,1.5,1.5514246225357056,3.43,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Boa_Esperanca,2023-12-31T00:00:00,1.08,1.4626785516738892,35.43,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Jesus_da_Penha,2023-12-31T00:00:00,1.2,1.68565034866333,40.47,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bom_Sucesso,2023-12-31T00:00:00,1.5,1.6029759645462036,6.87,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Botelhos,2023-12-31T00:00:00,1.080052666227781,1.2373164892196655,14.56,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Bueno_Brandao,2023-12-31T00:00:00,1.379901960784314,1.4351856708526611,4.01,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Buritizeiro,2023-12-31T00:00:00,3.0,3.109193801879883,3.64,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cabo_Verde,2023-12-31T00:00:00,1.319976635514019,1.5842440128326416,20.02,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caiana,2023-12-31T00:00:00,1.08,1.0674805641174316,-1.16,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cambuquira,2023-12-31T00:00:00,1.56,1.4226386547088623,-8.81,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campanha,2023-12-31T00:00:00,1.327075098814229,1.4061943292617798,5.96,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campestre,2023-12-31T00:00:00,1.319964428634949,1.3979016542434692,5.9,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_Belo,2023-12-31T00:00:00,1.5,1.4669597148895264,-2.2,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campo_do_Meio,2023-12-31T00:00:00,0.8771626297577856,1.6633801460266113,89.63,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Altos,2023-12-31T00:00:00,1.380040526849037,1.5890133380889893,15.14,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Campos_Gerais,2023-12-31T00:00:00,1.560242401107029,1.8556216955184937,18.93,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Candeias,2023-12-31T00:00:00,1.080032206119163,1.295609712600708,19.96,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caparao,2023-12-31T00:00:00,1.32,1.3356883525848389,1.19,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capelinha,2023-12-31T00:00:00,1.541750580945004,1.4766207933425903,-4.22,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capetinga,2023-12-31T00:00:00,1.4484046164290565,1.707514762878418,17.89,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Capitolio,2023-12-31T00:00:00,1.32,1.3868685960769653,5.07,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caputira,2023-12-31T00:00:00,1.380065005417118,1.5314505100250244,10.97,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carangola,2023-12-31T00:00:00,1.140118343195266,1.165980577468872,2.27,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Caratinga,2023-12-31T00:00:00,1.3799999999999997,1.6385600566864014,18.74,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_da_Cachoeira,2023-12-31T00:00:00,1.32,1.4607988595962524,10.67,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_de_Minas,2023-12-31T00:00:00,1.5,1.4103597402572632,-5.98,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Paranaiba,2023-12-31T00:00:00,2.369801007771799,1.5986251831054688,-32.54,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Carmo_do_Rio_Claro,2023-12-31T00:00:00,1.816856256463288,1.8628231287002563,2.53,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cassia,2023-12-31T00:00:00,1.6377649325626198,1.5349254608154297,-6.28,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Chale,2023-12-31T00:00:00,1.3799999999999997,1.453654170036316,5.34,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Claraval,2023-12-31T00:00:00,1.8600214362272245,1.5653905868530273,-15.84,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_da_Aparecida,2023-12-31T00:00:00,1.679948420373952,1.7627496719360352,4.93,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2023-12-31T00:00:00,1.44,1.5926971435546875,10.6,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Conselheiro_Pena,2023-12-31T00:00:00,1.320048602673147,1.5290343761444092,15.83,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coqueiral,2023-12-31T00:00:00,1.32,1.5585464239120483,18.07,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Coromandel,2023-12-31T00:00:00,2.0926575541308825,1.4164230823516846,-32.31,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Cristais,2023-12-31T00:00:00,1.5631111111111111,1.3872462511062622,-11.25,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divino,2023-12-31T00:00:00,1.02,1.288161039352417,26.29,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Divisa_Nova,2023-12-31T00:00:00,1.680092592592593,1.4152312278747559,-15.76,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Durande,2023-12-31T00:00:00,1.3799999999999997,1.6775177717208862,21.56,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Eloi_Mendes,2023-12-31T00:00:00,1.577412806790921,1.283216953277588,-18.65,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ervalia,2023-12-31T00:00:00,1.380044843049327,1.3359594345092773,-3.19,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Espera_Feliz,2023-12-31T00:00:00,1.08,1.2666468620300293,17.28,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Estrela_do_Sul,2023-12-31T00:00:00,2.43015873015873,1.7596380710601807,-27.59,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Fervedouro,2023-12-31T00:00:00,1.2,1.3520805835723877,12.67,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Formiga,2023-12-31T00:00:00,2.021543985637344,1.500115990638733,-25.79,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guape,2023-12-31T00:00:00,1.560674157303371,1.45491623878479,-6.78,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaranesia,2023-12-31T00:00:00,1.1961439588688951,1.1958738565444946,-0.02,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guaxupe,2023-12-31T00:00:00,1.2,1.246545672416687,3.88,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Guimarania,2023-12-31T00:00:00,1.7704600484261497,1.5722482204437256,-11.2,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Heliodora,2023-12-31T00:00:00,1.439872408293461,1.307704210281372,-9.18,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibia,2023-12-31T00:00:00,1.668,1.3222473859786987,-20.73,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibiraci,2023-12-31T00:00:00,1.804169884169884,1.689163088798523,-6.37,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ibitiura_de_Minas,2023-12-31T00:00:00,1.150344827586207,1.6265387535095215,41.4,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ilicinea,2023-12-31T00:00:00,1.560032362459547,1.5210890769958496,-2.5,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Imbe_de_Minas,2023-12-31T00:00:00,1.3799999999999997,1.3361889123916626,-3.17,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inconfidentes,2023-12-31T00:00:00,1.7401360544217692,1.8421506881713867,5.86,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Indianopolis,2023-12-31T00:00:00,2.7000000000000006,1.833746314048767,-32.08,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Inhapim,2023-12-31T00:00:00,2.7000000000000006,1.5790001153945923,-41.52,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Itamogi,2023-12-31T00:00:00,1.740011254924029,1.550778865814209,-10.88,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacui,2023-12-31T00:00:00,1.32,1.190833330154419,-9.79,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jacutinga,2023-12-31T00:00:00,1.160053262316911,1.437361478805542,23.9,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Jequeri,2023-12-31T00:00:00,1.5,1.7797802686691284,18.65,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Joao_Pinheiro,2023-12-31T00:00:00,3.3,2.6015121936798096,-21.17,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Juruaia,2023-12-31T00:00:00,1.5,1.6143440008163452,7.62,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lajinha,2023-12-31T00:00:00,1.4399536768963517,1.5047791004180908,4.5,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lambari,2023-12-31T00:00:00,1.56,1.376190185546875,-11.78,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Lavras,2023-12-31T00:00:00,1.5,1.4165844917297363,-5.56,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Luisburgo,2023-12-31T00:00:00,1.44,1.6404917240142822,13.92,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Machado,2023-12-31T00:00:00,1.500560931145702,1.4070897102355957,-6.23,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhuacu,2023-12-31T00:00:00,1.2,1.3065345287322998,8.88,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Manhumirim,2023-12-31T00:00:00,1.44,1.3970952033996582,-2.98,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Martins_Soares,2023-12-31T00:00:00,1.2,1.4297124147415161,19.14,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Matipo,2023-12-31T00:00:00,1.25990675990676,1.2809226512908936,1.67,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Medeiros,2023-12-31T00:00:00,1.86,1.6263303756713867,-12.56,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Miradouro,2023-12-31T00:00:00,1.140084388185654,1.201630711555481,5.4,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monsenhor_Paulo,2023-12-31T00:00:00,1.389931972789116,1.4996944665908813,7.9,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Belo,2023-12-31T00:00:00,1.3799999999999997,1.4420336484909058,4.5,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Carmelo,2023-12-31T00:00:00,2.4900284900284904,1.8335530757904053,-26.36,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Santo_de_Minas,2023-12-31T00:00:00,1.620050377833753,1.531531572341919,-5.46,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Monte_Siao,2023-12-31T00:00:00,1.5,1.4713994264602661,-1.91,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Mutum,2023-12-31T00:00:00,1.019985196150999,1.4295058250427246,40.15,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Muzambinho,2023-12-31T00:00:00,1.68,1.4410098791122437,-14.23,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Natercia,2023-12-31T00:00:00,1.5,1.1822335720062256,-21.18,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nazareno,2023-12-31T00:00:00,1.5599056603773582,1.6075950860977173,3.06,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nepomuceno,2023-12-31T00:00:00,1.650045578851413,1.3259341716766357,-19.64,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ninheira,2023-12-31T00:00:00,3.0,2.631999969482422,-12.27,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Belem,2023-12-31T00:00:00,0.96,1.0180249214172363,6.04,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Nova_Resende,2023-12-31T00:00:00,1.56,1.6864831447601318,8.11,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Oliveira,2023-12-31T00:00:00,1.8240227434257288,1.6448514461517334,-9.82,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Orizania,2023-12-31T00:00:00,1.2,1.4007678031921387,16.73,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ouro_Fino,2023-12-31T00:00:00,1.68,1.413787841796875,-15.85,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paracatu,2023-12-31T00:00:00,2.4,2.5020177364349365,4.25,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Paraguacu,2023-12-31T00:00:00,1.298291457286432,1.3293788433074951,2.39,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Passos,2023-12-31T00:00:00,1.829801324503311,1.2697455883026123,-30.61,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patos_de_Minas,2023-12-31T00:00:00,1.782452316076294,1.8523324728012085,3.92,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Patrocinio,2023-12-31T00:00:00,2.4370275910039414,1.330832600593567,-45.39,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedra_Bonita,2023-12-31T00:00:00,1.08,1.513371467590332,40.13,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pedralva,2023-12-31T00:00:00,1.67998417721519,1.475059151649475,-12.2,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdizes,2023-12-31T00:00:00,2.106451612903226,1.557926893234253,-26.04,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Perdoes,2023-12-31T00:00:00,1.6799401197604789,1.510292887687683,-10.1,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piedade_de_Caratinga,2023-12-31T00:00:00,2.4,1.5046944618225098,-37.3,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pimenta,2023-12-31T00:00:00,1.504186046511628,1.4773900508880615,-1.78,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Piumhi,2023-12-31T00:00:00,1.7399633363886338,1.4497411251068115,-16.68,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Poco_Fundo,2023-12-31T00:00:00,1.380160799652325,1.1986538171768188,-13.15,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pocos_de_Caldas,2023-12-31T00:00:00,1.5,1.4338631629943848,-4.41,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Pratinha,2023-12-31T00:00:00,2.13,1.677393913269043,-21.25,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Presidente_Olegario,2023-12-31T00:00:00,2.160074626865672,1.8244401216506958,-15.54,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Raul_Soares,2023-12-31T00:00:00,1.5,1.1491389274597168,-23.39,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Reduto,2023-12-31T00:00:00,0.9,1.3865445852279663,54.06,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Rio_Paranaiba,2023-12-31T00:00:00,1.8000000000000005,1.3332395553588867,-25.93,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Romaria,2023-12-31T00:00:00,2.131578947368421,2.080015182495117,-2.42,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sacramento,2023-12-31T00:00:00,1.75,1.5175960063934326,-13.28,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Barbara_do_Leste,2023-12-31T00:00:00,1.08,1.2664592266082764,17.26,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Margarida,2023-12-31T00:00:00,1.32,1.3206801414489746,0.05,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_de_Minas,2023-12-31T00:00:00,1.019930675909879,1.2369052171707153,21.27,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Itueto,2023-12-31T00:00:00,1.2,1.580857515335083,31.74,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2023-12-31T00:00:00,1.3799999999999997,1.4166191816329956,2.65,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santa_Rosa_da_Serra,2023-12-31T00:00:00,1.68,1.896300196647644,12.88,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_da_Vargem,2023-12-31T00:00:00,1.5593593593593589,1.46563720703125,-6.01,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santana_do_Manhuacu,2023-12-31T00:00:00,1.32,1.4406359195709229,9.14,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2023-12-31T00:00:00,1.8000000000000005,1.5194296836853027,-15.59,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Domingos_das_Dores,2023-12-31T00:00:00,1.296078431372549,1.3122954368591309,1.25,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Francisco_de_Paula,2023-12-31T00:00:00,1.56,1.5081435441970825,-3.32,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2023-12-31T00:00:00,1.2,1.6115047931671143,34.29,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Gotardo,2023-12-31T00:00:00,1.5,1.3765336275100708,-8.23,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2023-12-31T00:00:00,1.2,1.2692151069641113,5.77,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Jose_da_Barra,2023-12-31T00:00:00,2.2413698630136984,2.1227893829345703,-5.29,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2023-12-31T00:00:00,1.5,1.6403261423110962,9.36,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Roque_de_Minas,2023-12-31T00:00:00,1.8000000000000005,1.4019579887390137,-22.11,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2023-12-31T00:00:00,1.7998212689901698,1.6933132410049438,-5.92,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2023-12-31T00:00:00,1.440017746228926,1.4018924236297607,-2.65,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2023-12-31T00:00:00,1.319968346082828,1.4449403285980225,9.47,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Sericita,2023-12-31T00:00:00,1.26,1.4915472269058228,18.38,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serra_do_Salitre,2023-12-31T00:00:00,1.489636363636364,1.5744117498397827,5.69,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Serrania,2023-12-31T00:00:00,0.9,1.3191581964492798,46.57,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Simonesia,2023-12-31T00:00:00,1.5,1.5167325735092163,1.12,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tiros,2023-12-31T00:00:00,2.382038834951457,1.521489143371582,-36.13,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Coracoes,2023-12-31T00:00:00,1.44,1.488911747932434,3.4,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Tres_Pontas,2023-12-31T00:00:00,1.5900000000000003,1.4282562732696533,-10.17,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Turvolandia,2023-12-31T00:00:00,1.259748427672956,1.476154088973999,17.18,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Ubaporanga,2023-12-31T00:00:00,1.5,1.3683886528015137,-8.77,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Unai,2023-12-31T00:00:00,2.520095187731359,2.4979817867279053,-0.88,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varginha,2023-12-31T00:00:00,1.679990280646337,1.3153603076934814,-21.7,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Varjao_de_Minas,2023-12-31T00:00:00,2.3892857142857142,2.4334123134613037,1.85,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 1 (2023),Vermelho_Novo,2023-12-31T00:00:00,1.5,1.0774619579315186,-28.17,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0001010150679062233
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T15:42:01
IBGE V5 - Metodo 2 (2023),Abre_Campo,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Abre_Campo,2013-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Abre_Campo,2014-12-31T00:00:00,0.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Abre_Campo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Abre_Campo,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Abre_Campo,2017-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Abre_Campo,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Abre_Campo,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Abre_Campo,2020-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Abre_Campo,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alfenas,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alfenas,2013-12-31T00:00:00,1.98,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alfenas,2014-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alfenas,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alfenas,2016-12-31T00:00:00,2.25,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alfenas,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alfenas,2018-12-31T00:00:00,2.34,2.0,14.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alfenas,2019-12-31T00:00:00,1.76,2.0,13.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alfenas,2020-12-31T00:00:00,2.36,3.0,27.12,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alfenas,2021-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alpinopolis,2012-12-31T00:00:00,2.7,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alpinopolis,2013-12-31T00:00:00,2.71,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alpinopolis,2014-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alpinopolis,2015-12-31T00:00:00,1.82,2.0,9.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alpinopolis,2016-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alpinopolis,2017-12-31T00:00:00,2.06,2.0,2.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alpinopolis,2018-12-31T00:00:00,2.23,2.0,10.31,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alpinopolis,2019-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alpinopolis,2020-12-31T00:00:00,2.77,2.0,27.8,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alpinopolis,2021-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alterosa,2012-12-31T00:00:00,1.59,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alterosa,2013-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alterosa,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alterosa,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alterosa,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alterosa,2017-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alterosa,2018-12-31T00:00:00,1.96,2.0,2.04,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alterosa,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alterosa,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alterosa,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Caparao,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Caparao,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Caparao,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Caparao,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Caparao,2016-12-31T00:00:00,2.1,1.0,52.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Caparao,2017-12-31T00:00:00,1.37,2.0,45.99,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Caparao,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Caparao,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Caparao,2020-12-31T00:00:00,1.92,1.0,47.92,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Caparao,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Jequitiba,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Jequitiba,2013-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Jequitiba,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Jequitiba,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Jequitiba,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Jequitiba,2017-12-31T00:00:00,1.53,1.0,34.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Jequitiba,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Jequitiba,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Jequitiba,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Jequitiba,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Andradas,2012-12-31T00:00:00,1.98,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Andradas,2013-12-31T00:00:00,1.59,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Andradas,2014-12-31T00:00:00,1.21,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Andradas,2015-12-31T00:00:00,1.89,1.0,47.09,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Andradas,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Andradas,2017-12-31T00:00:00,1.34,2.0,49.25,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Andradas,2018-12-31T00:00:00,1.73,2.0,15.61,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Andradas,2019-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Andradas,2020-12-31T00:00:00,2.57,2.0,22.18,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Andradas,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Angelandia,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Angelandia,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Angelandia,2014-12-31T00:00:00,1.15,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Angelandia,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Angelandia,2016-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Angelandia,2017-12-31T00:00:00,0.94,1.0,6.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Angelandia,2018-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Angelandia,2019-12-31T00:00:00,1.22,1.0,18.03,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Angelandia,2020-12-31T00:00:00,1.84,2.0,8.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Angelandia,2021-12-31T00:00:00,1.51,1.0,33.77,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araguari,2012-12-31T00:00:00,2.4,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araguari,2013-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araguari,2014-12-31T00:00:00,2.4,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araguari,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araguari,2016-12-31T00:00:00,2.58,3.0,16.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araguari,2017-12-31T00:00:00,2.76,2.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araguari,2018-12-31T00:00:00,2.64,3.0,13.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araguari,2019-12-31T00:00:00,1.68,3.0,78.57,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araguari,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araguari,2021-12-31T00:00:00,1.86,1.0,46.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araponga,2012-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araponga,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araponga,2014-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araponga,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araponga,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araponga,2017-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araponga,2018-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araponga,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araponga,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araponga,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araxa,2012-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araxa,2013-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araxa,2014-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araxa,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araxa,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araxa,2017-12-31T00:00:00,1.76,2.0,13.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araxa,2018-12-31T00:00:00,1.94,2.0,3.09,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araxa,2019-12-31T00:00:00,1.39,2.0,43.88,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araxa,2020-12-31T00:00:00,1.79,2.0,11.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araxa,2021-12-31T00:00:00,1.25,1.0,20.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Areado,2012-12-31T00:00:00,1.95,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Areado,2013-12-31T00:00:00,1.86,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Areado,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Areado,2015-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Areado,2016-12-31T00:00:00,2.04,2.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Areado,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Areado,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Areado,2019-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Areado,2020-12-31T00:00:00,2.08,2.0,3.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Areado,2021-12-31T00:00:00,1.52,1.0,34.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bambui,2012-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bambui,2013-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bambui,2014-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bambui,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bambui,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bambui,2017-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bambui,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bambui,2019-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bambui,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bambui,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Boa_Esperanca,2012-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Boa_Esperanca,2013-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Boa_Esperanca,2014-12-31T00:00:00,1.15,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Boa_Esperanca,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Boa_Esperanca,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Boa_Esperanca,2017-12-31T00:00:00,2.53,2.0,20.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Boa_Esperanca,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Boa_Esperanca,2019-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Boa_Esperanca,2020-12-31T00:00:00,2.06,3.0,45.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Boa_Esperanca,2021-12-31T00:00:00,1.57,2.0,27.39,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Jesus_da_Penha,2012-12-31T00:00:00,2.58,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Jesus_da_Penha,2013-12-31T00:00:00,2.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Jesus_da_Penha,2014-12-31T00:00:00,2.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Jesus_da_Penha,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Jesus_da_Penha,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Jesus_da_Penha,2017-12-31T00:00:00,2.22,2.0,9.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Jesus_da_Penha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Jesus_da_Penha,2019-12-31T00:00:00,2.22,2.0,9.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Jesus_da_Penha,2020-12-31T00:00:00,1.99,2.0,0.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Jesus_da_Penha,2021-12-31T00:00:00,1.52,2.0,31.58,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Sucesso,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Sucesso,2013-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Sucesso,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Sucesso,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Sucesso,2016-12-31T00:00:00,1.92,1.0,47.92,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Sucesso,2017-12-31T00:00:00,1.48,2.0,35.14,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Sucesso,2018-12-31T00:00:00,1.7,2.0,17.65,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Sucesso,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Sucesso,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Sucesso,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Botelhos,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Botelhos,2013-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Botelhos,2014-12-31T00:00:00,1.74,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Botelhos,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Botelhos,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Botelhos,2017-12-31T00:00:00,1.88,2.0,6.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Botelhos,2018-12-31T00:00:00,2.07,2.0,3.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Botelhos,2019-12-31T00:00:00,1.92,1.0,47.92,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Botelhos,2020-12-31T00:00:00,1.72,2.0,16.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Botelhos,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bueno_Brandao,2012-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bueno_Brandao,2013-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bueno_Brandao,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bueno_Brandao,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bueno_Brandao,2016-12-31T00:00:00,2.1,1.0,52.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bueno_Brandao,2017-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bueno_Brandao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bueno_Brandao,2019-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bueno_Brandao,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bueno_Brandao,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Buritizeiro,2012-12-31T00:00:00,3.6,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Buritizeiro,2013-12-31T00:00:00,3.6,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Buritizeiro,2014-12-31T00:00:00,3.6,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Buritizeiro,2015-12-31T00:00:00,1.92,0.0,100.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Buritizeiro,2016-12-31T00:00:00,2.1,3.0,42.86,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Buritizeiro,2017-12-31T00:00:00,1.97,2.0,1.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Buritizeiro,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Buritizeiro,2019-12-31T00:00:00,2.88,2.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Buritizeiro,2020-12-31T00:00:00,3.0,3.0,0.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Buritizeiro,2021-12-31T00:00:00,3.0,3.0,0.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cabo_Verde,2012-12-31T00:00:00,2.07,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cabo_Verde,2013-12-31T00:00:00,2.04,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cabo_Verde,2014-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cabo_Verde,2015-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cabo_Verde,2016-12-31T00:00:00,1.65,2.0,21.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cabo_Verde,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cabo_Verde,2018-12-31T00:00:00,1.61,2.0,24.22,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cabo_Verde,2019-12-31T00:00:00,2.04,2.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cabo_Verde,2020-12-31T00:00:00,2.25,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cabo_Verde,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caiana,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caiana,2013-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caiana,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caiana,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caiana,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caiana,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caiana,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caiana,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caiana,2020-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caiana,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cambuquira,2012-12-31T00:00:00,1.15,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cambuquira,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cambuquira,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cambuquira,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cambuquira,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cambuquira,2017-12-31T00:00:00,1.82,2.0,9.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cambuquira,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cambuquira,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cambuquira,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cambuquira,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campanha,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campanha,2013-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campanha,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campanha,2015-12-31T00:00:00,1.02,1.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campanha,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campanha,2017-12-31T00:00:00,1.88,1.0,46.81,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campanha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campanha,2019-12-31T00:00:00,1.51,2.0,32.45,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campanha,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campanha,2021-12-31T00:00:00,1.33,1.0,24.81,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campestre,2012-12-31T00:00:00,1.35,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campestre,2013-12-31T00:00:00,1.31,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campestre,2014-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campestre,2015-12-31T00:00:00,1.91,1.0,47.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campestre,2016-12-31T00:00:00,1.85,2.0,8.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campestre,2017-12-31T00:00:00,2.11,2.0,5.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campestre,2018-12-31T00:00:00,1.54,2.0,29.87,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campestre,2019-12-31T00:00:00,1.55,2.0,29.03,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campestre,2020-12-31T00:00:00,2.14,2.0,6.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campestre,2021-12-31T00:00:00,1.29,1.0,22.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_Belo,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_Belo,2013-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_Belo,2014-12-31T00:00:00,0.63,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_Belo,2015-12-31T00:00:00,0.84,1.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_Belo,2016-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_Belo,2017-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_Belo,2018-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_Belo,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_Belo,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_Belo,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_do_Meio,2012-12-31T00:00:00,1.17,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_do_Meio,2013-12-31T00:00:00,1.17,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_do_Meio,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_do_Meio,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_do_Meio,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_do_Meio,2017-12-31T00:00:00,2.08,2.0,3.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_do_Meio,2018-12-31T00:00:00,1.82,2.0,9.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_do_Meio,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_do_Meio,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_do_Meio,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Altos,2012-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Altos,2013-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Altos,2014-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Altos,2015-12-31T00:00:00,1.71,2.0,16.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Altos,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Altos,2017-12-31T00:00:00,1.99,2.0,0.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Altos,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Altos,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Altos,2020-12-31T00:00:00,2.1,3.0,42.86,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Altos,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Gerais,2012-12-31T00:00:00,1.65,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Gerais,2013-12-31T00:00:00,1.22,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Gerais,2014-12-31T00:00:00,1.15,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Gerais,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Gerais,2016-12-31T00:00:00,2.5,1.0,60.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Gerais,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Gerais,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Gerais,2019-12-31T00:00:00,1.59,2.0,25.79,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Gerais,2020-12-31T00:00:00,2.69,2.0,25.65,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Gerais,2021-12-31T00:00:00,1.59,1.0,37.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Candeias,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Candeias,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Candeias,2014-12-31T00:00:00,0.99,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Candeias,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Candeias,2016-12-31T00:00:00,2.28,2.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Candeias,2017-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Candeias,2018-12-31T00:00:00,2.04,2.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Candeias,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Candeias,2020-12-31T00:00:00,1.86,2.0,7.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Candeias,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caparao,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caparao,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caparao,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caparao,2015-12-31T00:00:00,1.02,0.0,100.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caparao,2016-12-31T00:00:00,1.92,1.0,47.92,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caparao,2017-12-31T00:00:00,1.45,1.0,31.03,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caparao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caparao,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caparao,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caparao,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capelinha,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capelinha,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capelinha,2014-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capelinha,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capelinha,2016-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capelinha,2017-12-31T00:00:00,1.16,1.0,13.79,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capelinha,2018-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capelinha,2019-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capelinha,2020-12-31T00:00:00,1.43,1.0,30.07,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capelinha,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capetinga,2012-12-31T00:00:00,1.81,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capetinga,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capetinga,2014-12-31T00:00:00,1.59,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capetinga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capetinga,2016-12-31T00:00:00,1.93,2.0,3.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capetinga,2017-12-31T00:00:00,2.09,2.0,4.31,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capetinga,2018-12-31T00:00:00,2.7,2.0,25.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capetinga,2019-12-31T00:00:00,2.04,2.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capetinga,2020-12-31T00:00:00,2.21,3.0,35.75,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capetinga,2021-12-31T00:00:00,1.64,2.0,21.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capitolio,2012-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capitolio,2013-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capitolio,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capitolio,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capitolio,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capitolio,2017-12-31T00:00:00,1.47,1.0,31.97,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capitolio,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capitolio,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capitolio,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capitolio,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caputira,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caputira,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caputira,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caputira,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caputira,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caputira,2017-12-31T00:00:00,0.77,1.0,29.87,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caputira,2018-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caputira,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caputira,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caputira,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carangola,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carangola,2013-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carangola,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carangola,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carangola,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carangola,2017-12-31T00:00:00,1.1,1.0,9.09,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carangola,2018-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carangola,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carangola,2020-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carangola,2021-12-31T00:00:00,1.02,1.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caratinga,2012-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caratinga,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caratinga,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caratinga,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caratinga,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caratinga,2017-12-31T00:00:00,1.46,1.0,31.51,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caratinga,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caratinga,2019-12-31T00:00:00,1.0,2.0,100.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caratinga,2020-12-31T00:00:00,2.04,2.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caratinga,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_da_Cachoeira,2012-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_da_Cachoeira,2013-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_da_Cachoeira,2014-12-31T00:00:00,1.15,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_da_Cachoeira,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_da_Cachoeira,2016-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_da_Cachoeira,2017-12-31T00:00:00,1.73,1.0,42.2,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_da_Cachoeira,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_da_Cachoeira,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_da_Cachoeira,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_da_Cachoeira,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_de_Minas,2012-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_de_Minas,2013-12-31T00:00:00,1.74,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_de_Minas,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_de_Minas,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_de_Minas,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_de_Minas,2017-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_de_Minas,2018-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_de_Minas,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_de_Minas,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_de_Minas,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Paranaiba,2012-12-31T00:00:00,2.11,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Paranaiba,2013-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Paranaiba,2014-12-31T00:00:00,1.93,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Paranaiba,2015-12-31T00:00:00,1.89,2.0,5.82,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Paranaiba,2016-12-31T00:00:00,2.37,2.0,15.61,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Paranaiba,2017-12-31T00:00:00,1.75,2.0,14.29,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Paranaiba,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Paranaiba,2019-12-31T00:00:00,1.79,2.0,11.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Paranaiba,2020-12-31T00:00:00,1.95,2.0,2.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Paranaiba,2021-12-31T00:00:00,1.54,2.0,29.87,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Rio_Claro,2012-12-31T00:00:00,1.66,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Rio_Claro,2013-12-31T00:00:00,1.94,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Rio_Claro,2014-12-31T00:00:00,1.65,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Rio_Claro,2015-12-31T00:00:00,1.53,2.0,30.72,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Rio_Claro,2016-12-31T00:00:00,2.37,2.0,15.61,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Rio_Claro,2017-12-31T00:00:00,1.82,2.0,9.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Rio_Claro,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Rio_Claro,2019-12-31T00:00:00,1.94,2.0,3.09,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Rio_Claro,2020-12-31T00:00:00,2.78,2.0,28.06,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Rio_Claro,2021-12-31T00:00:00,1.64,1.0,39.02,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cassia,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cassia,2013-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cassia,2014-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cassia,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cassia,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cassia,2017-12-31T00:00:00,1.6,2.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cassia,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cassia,2019-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cassia,2020-12-31T00:00:00,2.17,2.0,7.83,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cassia,2021-12-31T00:00:00,1.31,1.0,23.66,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Chale,2012-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Chale,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Chale,2014-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Chale,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Chale,2016-12-31T00:00:00,1.42,1.0,29.58,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Chale,2017-12-31T00:00:00,0.93,1.0,7.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Chale,2018-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Chale,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Chale,2020-12-31T00:00:00,1.71,2.0,16.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Chale,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Claraval,2012-12-31T00:00:00,2.55,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Claraval,2013-12-31T00:00:00,1.42,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Claraval,2014-12-31T00:00:00,2.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Claraval,2015-12-31T00:00:00,1.02,1.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Claraval,2016-12-31T00:00:00,2.34,3.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Claraval,2017-12-31T00:00:00,1.22,2.0,63.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Claraval,2018-12-31T00:00:00,2.55,2.0,21.57,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Claraval,2019-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Claraval,2020-12-31T00:00:00,2.8,3.0,7.14,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Claraval,2021-12-31T00:00:00,1.05,1.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_da_Aparecida,2012-12-31T00:00:00,1.95,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_da_Aparecida,2013-12-31T00:00:00,1.7,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_da_Aparecida,2014-12-31T00:00:00,1.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_da_Aparecida,2015-12-31T00:00:00,2.52,2.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_da_Aparecida,2016-12-31T00:00:00,1.99,3.0,50.75,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_da_Aparecida,2017-12-31T00:00:00,1.85,2.0,8.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_da_Aparecida,2018-12-31T00:00:00,2.03,2.0,1.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_da_Aparecida,2019-12-31T00:00:00,1.97,2.0,1.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_da_Aparecida,2020-12-31T00:00:00,2.24,2.0,10.71,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_da_Aparecida,2021-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2015-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2017-12-31T00:00:00,2.04,2.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conselheiro_Pena,2012-12-31T00:00:00,1.48,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conselheiro_Pena,2013-12-31T00:00:00,1.3,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conselheiro_Pena,2014-12-31T00:00:00,1.3,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conselheiro_Pena,2015-12-31T00:00:00,1.06,1.0,5.66,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conselheiro_Pena,2016-12-31T00:00:00,1.18,1.0,15.25,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conselheiro_Pena,2017-12-31T00:00:00,1.29,1.0,22.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conselheiro_Pena,2018-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conselheiro_Pena,2019-12-31T00:00:00,1.3,1.0,23.08,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conselheiro_Pena,2020-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conselheiro_Pena,2021-12-31T00:00:00,1.29,1.0,22.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coqueiral,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coqueiral,2013-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coqueiral,2014-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coqueiral,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coqueiral,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coqueiral,2017-12-31T00:00:00,1.75,1.0,42.86,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coqueiral,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coqueiral,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coqueiral,2020-12-31T00:00:00,2.16,2.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coqueiral,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coromandel,2012-12-31T00:00:00,1.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coromandel,2013-12-31T00:00:00,2.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coromandel,2014-12-31T00:00:00,1.91,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coromandel,2015-12-31T00:00:00,1.91,2.0,4.71,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coromandel,2016-12-31T00:00:00,2.09,2.0,4.31,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coromandel,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coromandel,2018-12-31T00:00:00,2.65,2.0,24.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coromandel,2019-12-31T00:00:00,1.65,2.0,21.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coromandel,2020-12-31T00:00:00,1.82,3.0,64.84,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coromandel,2021-12-31T00:00:00,1.25,1.0,20.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cristais,2012-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cristais,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cristais,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cristais,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cristais,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cristais,2017-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cristais,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cristais,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cristais,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cristais,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divino,2012-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divino,2013-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divino,2014-12-31T00:00:00,0.89,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divino,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divino,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divino,2017-12-31T00:00:00,1.19,1.0,15.97,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divino,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divino,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divino,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divino,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divisa_Nova,2012-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divisa_Nova,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divisa_Nova,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divisa_Nova,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divisa_Nova,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divisa_Nova,2017-12-31T00:00:00,2.28,1.0,56.14,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divisa_Nova,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divisa_Nova,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divisa_Nova,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divisa_Nova,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Durande,2012-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Durande,2013-12-31T00:00:00,2.22,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Durande,2014-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Durande,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Durande,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Durande,2017-12-31T00:00:00,1.33,2.0,50.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Durande,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Durande,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Durande,2020-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Durande,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Eloi_Mendes,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Eloi_Mendes,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Eloi_Mendes,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Eloi_Mendes,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Eloi_Mendes,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Eloi_Mendes,2017-12-31T00:00:00,1.83,2.0,9.29,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Eloi_Mendes,2018-12-31T00:00:00,1.72,2.0,16.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Eloi_Mendes,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Eloi_Mendes,2020-12-31T00:00:00,2.11,2.0,5.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Eloi_Mendes,2021-12-31T00:00:00,1.04,1.0,3.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ervalia,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ervalia,2013-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ervalia,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ervalia,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ervalia,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ervalia,2017-12-31T00:00:00,1.36,1.0,26.47,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ervalia,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ervalia,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ervalia,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ervalia,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Espera_Feliz,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Espera_Feliz,2013-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Espera_Feliz,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Espera_Feliz,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Espera_Feliz,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Espera_Feliz,2017-12-31T00:00:00,1.21,1.0,17.36,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Espera_Feliz,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Espera_Feliz,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Espera_Feliz,2020-12-31T00:00:00,2.04,2.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Espera_Feliz,2021-12-31T00:00:00,0.78,1.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Estrela_do_Sul,2012-12-31T00:00:00,2.22,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Estrela_do_Sul,2013-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Estrela_do_Sul,2014-12-31T00:00:00,1.98,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Estrela_do_Sul,2015-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Estrela_do_Sul,2016-12-31T00:00:00,2.52,2.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Estrela_do_Sul,2017-12-31T00:00:00,2.38,2.0,15.97,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Estrela_do_Sul,2018-12-31T00:00:00,2.22,3.0,35.14,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Estrela_do_Sul,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Estrela_do_Sul,2020-12-31T00:00:00,2.06,3.0,45.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Estrela_do_Sul,2021-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Fervedouro,2012-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Fervedouro,2013-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Fervedouro,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Fervedouro,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Fervedouro,2016-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Fervedouro,2017-12-31T00:00:00,1.06,1.0,5.66,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Fervedouro,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Fervedouro,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Fervedouro,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Fervedouro,2021-12-31T00:00:00,1.15,1.0,13.04,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Formiga,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Formiga,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Formiga,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Formiga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Formiga,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Formiga,2017-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Formiga,2018-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Formiga,2019-12-31T00:00:00,1.24,1.0,19.35,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Formiga,2020-12-31T00:00:00,1.44,3.0,108.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Formiga,2021-12-31T00:00:00,1.35,1.0,25.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guape,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guape,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guape,2014-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guape,2015-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guape,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guape,2017-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guape,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guape,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guape,2020-12-31T00:00:00,1.85,2.0,8.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guape,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaranesia,2012-12-31T00:00:00,1.98,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaranesia,2013-12-31T00:00:00,1.52,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaranesia,2014-12-31T00:00:00,1.86,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaranesia,2015-12-31T00:00:00,1.65,2.0,21.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaranesia,2016-12-31T00:00:00,2.63,2.0,23.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaranesia,2017-12-31T00:00:00,1.36,2.0,47.06,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaranesia,2018-12-31T00:00:00,1.7,2.0,17.65,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaranesia,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaranesia,2020-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaranesia,2021-12-31T00:00:00,1.03,1.0,2.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaxupe,2012-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaxupe,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaxupe,2014-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaxupe,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaxupe,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaxupe,2017-12-31T00:00:00,1.51,2.0,32.45,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaxupe,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaxupe,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaxupe,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaxupe,2021-12-31T00:00:00,1.02,1.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guimarania,2012-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guimarania,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guimarania,2014-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guimarania,2015-12-31T00:00:00,2.4,1.0,58.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guimarania,2016-12-31T00:00:00,1.8,3.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guimarania,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guimarania,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guimarania,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guimarania,2020-12-31T00:00:00,2.15,2.0,6.98,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guimarania,2021-12-31T00:00:00,1.54,2.0,29.87,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Heliodora,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Heliodora,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Heliodora,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Heliodora,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Heliodora,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Heliodora,2017-12-31T00:00:00,1.57,2.0,27.39,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Heliodora,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Heliodora,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Heliodora,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Heliodora,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibia,2012-12-31T00:00:00,1.74,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibia,2013-12-31T00:00:00,1.74,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibia,2014-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibia,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibia,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibia,2017-12-31T00:00:00,1.59,2.0,25.79,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibia,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibia,2020-12-31T00:00:00,1.75,2.0,14.29,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibia,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibiraci,2012-12-31T00:00:00,2.49,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibiraci,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibiraci,2014-12-31T00:00:00,2.01,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibiraci,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibiraci,2016-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibiraci,2017-12-31T00:00:00,1.61,2.0,24.22,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibiraci,2018-12-31T00:00:00,3.0,3.0,0.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibiraci,2019-12-31T00:00:00,1.28,2.0,56.25,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibiraci,2020-12-31T00:00:00,2.63,3.0,14.07,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibiraci,2021-12-31T00:00:00,1.33,1.0,24.81,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibitiura_de_Minas,2012-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibitiura_de_Minas,2013-12-31T00:00:00,2.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibitiura_de_Minas,2014-12-31T00:00:00,2.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibitiura_de_Minas,2015-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibitiura_de_Minas,2016-12-31T00:00:00,1.76,2.0,13.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibitiura_de_Minas,2017-12-31T00:00:00,1.3,2.0,53.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibitiura_de_Minas,2018-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibitiura_de_Minas,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibitiura_de_Minas,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibitiura_de_Minas,2021-12-31T00:00:00,1.92,1.0,47.92,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ilicinea,2012-12-31T00:00:00,1.69,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ilicinea,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ilicinea,2014-12-31T00:00:00,1.35,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ilicinea,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ilicinea,2016-12-31T00:00:00,2.46,2.0,18.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ilicinea,2017-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ilicinea,2018-12-31T00:00:00,2.46,2.0,18.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ilicinea,2019-12-31T00:00:00,1.86,2.0,7.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ilicinea,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ilicinea,2021-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Imbe_de_Minas,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Imbe_de_Minas,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Imbe_de_Minas,2014-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Imbe_de_Minas,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Imbe_de_Minas,2016-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Imbe_de_Minas,2017-12-31T00:00:00,1.29,1.0,22.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Imbe_de_Minas,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Imbe_de_Minas,2019-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Imbe_de_Minas,2020-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Imbe_de_Minas,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inconfidentes,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inconfidentes,2013-12-31T00:00:00,1.4,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inconfidentes,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inconfidentes,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inconfidentes,2016-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inconfidentes,2017-12-31T00:00:00,1.3,2.0,53.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inconfidentes,2018-12-31T00:00:00,2.88,2.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inconfidentes,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inconfidentes,2020-12-31T00:00:00,2.46,3.0,21.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inconfidentes,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Indianopolis,2012-12-31T00:00:00,2.4,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Indianopolis,2013-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Indianopolis,2014-12-31T00:00:00,2.45,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Indianopolis,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Indianopolis,2016-12-31T00:00:00,2.7,3.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Indianopolis,2017-12-31T00:00:00,1.67,2.0,19.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Indianopolis,2018-12-31T00:00:00,2.52,3.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Indianopolis,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Indianopolis,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Indianopolis,2021-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inhapim,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inhapim,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inhapim,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inhapim,2015-12-31T00:00:00,1.21,2.0,65.29,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inhapim,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inhapim,2017-12-31T00:00:00,1.12,1.0,10.71,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inhapim,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inhapim,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inhapim,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inhapim,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Itamogi,2012-12-31T00:00:00,1.95,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Itamogi,2013-12-31T00:00:00,1.17,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Itamogi,2014-12-31T00:00:00,1.6,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Itamogi,2015-12-31T00:00:00,1.58,1.0,36.71,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Itamogi,2016-12-31T00:00:00,1.95,2.0,2.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Itamogi,2017-12-31T00:00:00,1.42,2.0,40.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Itamogi,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Itamogi,2019-12-31T00:00:00,1.85,2.0,8.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Itamogi,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Itamogi,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacui,2012-12-31T00:00:00,1.65,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacui,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacui,2014-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacui,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacui,2016-12-31T00:00:00,1.7,2.0,17.65,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacui,2017-12-31T00:00:00,1.73,2.0,15.61,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacui,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacui,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacui,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacui,2021-12-31T00:00:00,0.96,2.0,108.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacutinga,2012-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacutinga,2013-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacutinga,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacutinga,2015-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacutinga,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacutinga,2017-12-31T00:00:00,1.37,1.0,27.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacutinga,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacutinga,2019-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacutinga,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacutinga,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jequeri,2012-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jequeri,2013-12-31T00:00:00,1.86,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jequeri,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jequeri,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jequeri,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jequeri,2017-12-31T00:00:00,1.28,1.0,21.88,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jequeri,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jequeri,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jequeri,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jequeri,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Joao_Pinheiro,2012-12-31T00:00:00,2.82,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Joao_Pinheiro,2013-12-31T00:00:00,3.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Joao_Pinheiro,2014-12-31T00:00:00,2.7,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Joao_Pinheiro,2015-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Joao_Pinheiro,2016-12-31T00:00:00,3.6,3.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Joao_Pinheiro,2017-12-31T00:00:00,1.91,2.0,4.71,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Joao_Pinheiro,2018-12-31T00:00:00,2.4,4.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Joao_Pinheiro,2019-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Joao_Pinheiro,2020-12-31T00:00:00,3.0,3.0,0.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Joao_Pinheiro,2021-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Juruaia,2012-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Juruaia,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Juruaia,2014-12-31T00:00:00,1.72,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Juruaia,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Juruaia,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Juruaia,2017-12-31T00:00:00,1.69,2.0,18.34,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Juruaia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Juruaia,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Juruaia,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Juruaia,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lajinha,2012-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lajinha,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lajinha,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lajinha,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lajinha,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lajinha,2017-12-31T00:00:00,1.04,1.0,3.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lajinha,2018-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lajinha,2019-12-31T00:00:00,1.06,1.0,5.66,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lajinha,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lajinha,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lambari,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lambari,2013-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lambari,2014-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lambari,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lambari,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lambari,2017-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lambari,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lambari,2019-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lambari,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lambari,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lavras,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lavras,2013-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lavras,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lavras,2015-12-31T00:00:00,0.84,1.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lavras,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lavras,2017-12-31T00:00:00,2.25,1.0,55.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lavras,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lavras,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lavras,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lavras,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Luisburgo,2012-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Luisburgo,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Luisburgo,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Luisburgo,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Luisburgo,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Luisburgo,2017-12-31T00:00:00,1.53,1.0,34.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Luisburgo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Luisburgo,2019-12-31T00:00:00,1.64,1.0,39.02,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Luisburgo,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Luisburgo,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Machado,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Machado,2013-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Machado,2014-12-31T00:00:00,1.15,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Machado,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Machado,2016-12-31T00:00:00,1.86,2.0,7.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Machado,2017-12-31T00:00:00,1.51,2.0,32.45,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Machado,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Machado,2019-12-31T00:00:00,1.53,2.0,30.72,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Machado,2020-12-31T00:00:00,2.21,2.0,9.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Machado,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhuacu,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhuacu,2013-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhuacu,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhuacu,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhuacu,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhuacu,2017-12-31T00:00:00,1.43,1.0,30.07,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhuacu,2018-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhuacu,2019-12-31T00:00:00,1.02,1.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhuacu,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhuacu,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhumirim,2012-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhumirim,2013-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhumirim,2014-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhumirim,2015-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhumirim,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhumirim,2017-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhumirim,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhumirim,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhumirim,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhumirim,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Martins_Soares,2012-12-31T00:00:00,2.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Martins_Soares,2013-12-31T00:00:00,1.98,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Martins_Soares,2014-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Martins_Soares,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Martins_Soares,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Martins_Soares,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Martins_Soares,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Martins_Soares,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Martins_Soares,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Martins_Soares,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Matipo,2012-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Matipo,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Matipo,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Matipo,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Matipo,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Matipo,2017-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Matipo,2018-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Matipo,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Matipo,2020-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Matipo,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Medeiros,2012-12-31T00:00:00,2.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Medeiros,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Medeiros,2014-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Medeiros,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Medeiros,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Medeiros,2017-12-31T00:00:00,1.79,2.0,11.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Medeiros,2018-12-31T00:00:00,2.04,2.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Medeiros,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Medeiros,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Medeiros,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Miradouro,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Miradouro,2013-12-31T00:00:00,2.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Miradouro,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Miradouro,2015-12-31T00:00:00,1.59,1.0,37.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Miradouro,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Miradouro,2017-12-31T00:00:00,1.16,1.0,13.79,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Miradouro,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Miradouro,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Miradouro,2020-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Miradouro,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monsenhor_Paulo,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monsenhor_Paulo,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monsenhor_Paulo,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monsenhor_Paulo,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monsenhor_Paulo,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monsenhor_Paulo,2017-12-31T00:00:00,1.72,2.0,16.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monsenhor_Paulo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monsenhor_Paulo,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monsenhor_Paulo,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monsenhor_Paulo,2021-12-31T00:00:00,1.41,1.0,29.08,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Belo,2012-12-31T00:00:00,1.63,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Belo,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Belo,2014-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Belo,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Belo,2016-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Belo,2017-12-31T00:00:00,2.43,2.0,17.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Belo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Belo,2019-12-31T00:00:00,1.64,2.0,21.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Belo,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Belo,2021-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Carmelo,2012-12-31T00:00:00,2.34,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Carmelo,2013-12-31T00:00:00,1.89,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Carmelo,2014-12-31T00:00:00,2.28,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Carmelo,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Carmelo,2016-12-31T00:00:00,2.64,3.0,13.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Carmelo,2017-12-31T00:00:00,1.84,2.0,8.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Carmelo,2018-12-31T00:00:00,2.31,3.0,29.87,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Carmelo,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Carmelo,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Carmelo,2021-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Santo_de_Minas,2012-12-31T00:00:00,1.89,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Santo_de_Minas,2013-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Santo_de_Minas,2014-12-31T00:00:00,1.6,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Santo_de_Minas,2015-12-31T00:00:00,1.35,2.0,48.15,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Santo_de_Minas,2016-12-31T00:00:00,2.04,2.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Santo_de_Minas,2017-12-31T00:00:00,1.08,2.0,85.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Santo_de_Minas,2018-12-31T00:00:00,1.88,2.0,6.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Santo_de_Minas,2019-12-31T00:00:00,1.59,1.0,37.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Santo_de_Minas,2020-12-31T00:00:00,2.12,2.0,5.66,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Santo_de_Minas,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Siao,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Siao,2013-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Siao,2014-12-31T00:00:00,0.66,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Siao,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Siao,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Siao,2017-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Siao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Siao,2019-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Siao,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Siao,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Mutum,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Mutum,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Mutum,2014-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Mutum,2015-12-31T00:00:00,1.15,1.0,13.04,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Mutum,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Mutum,2017-12-31T00:00:00,1.45,1.0,31.03,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Mutum,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Mutum,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Mutum,2020-12-31T00:00:00,1.65,2.0,21.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Mutum,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Muzambinho,2012-12-31T00:00:00,1.74,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Muzambinho,2013-12-31T00:00:00,1.35,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Muzambinho,2014-12-31T00:00:00,1.35,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Muzambinho,2015-12-31T00:00:00,1.48,1.0,32.43,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Muzambinho,2016-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Muzambinho,2017-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Muzambinho,2018-12-31T00:00:00,1.3,2.0,53.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Muzambinho,2019-12-31T00:00:00,2.1,1.0,52.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Muzambinho,2020-12-31T00:00:00,1.76,2.0,13.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Muzambinho,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Natercia,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Natercia,2013-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Natercia,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Natercia,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Natercia,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Natercia,2017-12-31T00:00:00,1.6,2.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Natercia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Natercia,2019-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Natercia,2020-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Natercia,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nazareno,2012-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nazareno,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nazareno,2014-12-31T00:00:00,1.3,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nazareno,2015-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nazareno,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nazareno,2017-12-31T00:00:00,1.59,2.0,25.79,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nazareno,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nazareno,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nazareno,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nazareno,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nepomuceno,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nepomuceno,2013-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nepomuceno,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nepomuceno,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nepomuceno,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nepomuceno,2017-12-31T00:00:00,1.69,2.0,18.34,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nepomuceno,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nepomuceno,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nepomuceno,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nepomuceno,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ninheira,2012-12-31T00:00:00,3.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ninheira,2013-12-31T00:00:00,3.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ninheira,2014-12-31T00:00:00,3.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ninheira,2015-12-31T00:00:00,2.76,0.0,100.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ninheira,2016-12-31T00:00:00,3.0,3.0,0.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ninheira,2017-12-31T00:00:00,2.34,3.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ninheira,2018-12-31T00:00:00,3.0,3.0,0.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ninheira,2019-12-31T00:00:00,2.7,3.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ninheira,2020-12-31T00:00:00,3.0,3.0,0.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ninheira,2021-12-31T00:00:00,2.1,3.0,42.86,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Belem,2012-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Belem,2013-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Belem,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Belem,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Belem,2016-12-31T00:00:00,1.2,0.0,100.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Belem,2017-12-31T00:00:00,0.48,1.0,108.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Belem,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Belem,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Belem,2020-12-31T00:00:00,1.1,1.0,9.09,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Belem,2021-12-31T00:00:00,1.0,1.0,0.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Resende,2012-12-31T00:00:00,2.88,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Resende,2013-12-31T00:00:00,2.18,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Resende,2014-12-31T00:00:00,1.7,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Resende,2015-12-31T00:00:00,2.2,2.0,9.09,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Resende,2016-12-31T00:00:00,2.08,3.0,44.23,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Resende,2017-12-31T00:00:00,1.76,2.0,13.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Resende,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Resende,2019-12-31T00:00:00,2.12,2.0,5.66,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Resende,2020-12-31T00:00:00,2.35,2.0,14.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Resende,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Oliveira,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Oliveira,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Oliveira,2014-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Oliveira,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Oliveira,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Oliveira,2017-12-31T00:00:00,1.4,2.0,42.86,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Oliveira,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Oliveira,2019-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Oliveira,2020-12-31T00:00:00,1.95,2.0,2.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Oliveira,2021-12-31T00:00:00,1.54,1.0,35.06,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Orizania,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Orizania,2013-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Orizania,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Orizania,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Orizania,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Orizania,2017-12-31T00:00:00,1.47,1.0,31.97,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Orizania,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Orizania,2019-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Orizania,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Orizania,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ouro_Fino,2012-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ouro_Fino,2013-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ouro_Fino,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ouro_Fino,2015-12-31T00:00:00,1.29,1.0,22.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ouro_Fino,2016-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ouro_Fino,2017-12-31T00:00:00,1.42,1.0,29.58,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ouro_Fino,2018-12-31T00:00:00,1.69,1.0,40.83,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ouro_Fino,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ouro_Fino,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ouro_Fino,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paracatu,2012-12-31T00:00:00,3.45,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paracatu,2013-12-31T00:00:00,3.45,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paracatu,2014-12-31T00:00:00,3.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paracatu,2015-12-31T00:00:00,1.8,3.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paracatu,2016-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paracatu,2017-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paracatu,2018-12-31T00:00:00,3.0,3.0,0.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paracatu,2019-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paracatu,2020-12-31T00:00:00,2.7,3.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paracatu,2021-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paraguacu,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paraguacu,2013-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paraguacu,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paraguacu,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paraguacu,2016-12-31T00:00:00,1.98,1.0,49.49,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paraguacu,2017-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paraguacu,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paraguacu,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paraguacu,2020-12-31T00:00:00,2.34,2.0,14.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paraguacu,2021-12-31T00:00:00,0.97,1.0,3.09,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Passos,2012-12-31T00:00:00,2.16,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Passos,2013-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Passos,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Passos,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Passos,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Passos,2017-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Passos,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Passos,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Passos,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Passos,2021-12-31T00:00:00,0.99,1.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patos_de_Minas,2012-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patos_de_Minas,2013-12-31T00:00:00,2.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patos_de_Minas,2014-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patos_de_Minas,2015-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patos_de_Minas,2016-12-31T00:00:00,1.5,3.0,100.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patos_de_Minas,2017-12-31T00:00:00,1.7,2.0,17.65,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patos_de_Minas,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patos_de_Minas,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patos_de_Minas,2020-12-31T00:00:00,1.94,2.0,3.09,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patos_de_Minas,2021-12-31T00:00:00,1.72,2.0,16.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patrocinio,2012-12-31T00:00:00,2.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patrocinio,2013-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patrocinio,2014-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patrocinio,2015-12-31T00:00:00,1.25,1.0,20.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patrocinio,2016-12-31T00:00:00,2.64,2.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patrocinio,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patrocinio,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patrocinio,2019-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patrocinio,2020-12-31T00:00:00,1.75,2.0,14.29,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patrocinio,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedra_Bonita,2012-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedra_Bonita,2013-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedra_Bonita,2014-12-31T00:00:00,0.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedra_Bonita,2015-12-31T00:00:00,0.6,1.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedra_Bonita,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedra_Bonita,2017-12-31T00:00:00,0.86,1.0,16.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedra_Bonita,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedra_Bonita,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedra_Bonita,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedra_Bonita,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedralva,2012-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedralva,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedralva,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedralva,2015-12-31T00:00:00,0.96,2.0,108.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedralva,2016-12-31T00:00:00,2.28,2.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedralva,2017-12-31T00:00:00,1.43,2.0,39.86,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedralva,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedralva,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedralva,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedralva,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdizes,2012-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdizes,2013-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdizes,2014-12-31T00:00:00,1.66,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdizes,2015-12-31T00:00:00,1.48,2.0,35.14,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdizes,2016-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdizes,2017-12-31T00:00:00,2.07,2.0,3.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdizes,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdizes,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdizes,2020-12-31T00:00:00,2.05,2.0,2.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdizes,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdoes,2012-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdoes,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdoes,2014-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdoes,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdoes,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdoes,2017-12-31T00:00:00,1.84,1.0,45.65,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdoes,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdoes,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdoes,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdoes,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piedade_de_Caratinga,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piedade_de_Caratinga,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piedade_de_Caratinga,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piedade_de_Caratinga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piedade_de_Caratinga,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piedade_de_Caratinga,2017-12-31T00:00:00,1.59,1.0,37.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piedade_de_Caratinga,2018-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piedade_de_Caratinga,2019-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piedade_de_Caratinga,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piedade_de_Caratinga,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pimenta,2012-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pimenta,2013-12-31T00:00:00,1.74,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pimenta,2014-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pimenta,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pimenta,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pimenta,2017-12-31T00:00:00,1.73,2.0,15.61,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pimenta,2018-12-31T00:00:00,2.7,2.0,25.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pimenta,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pimenta,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pimenta,2021-12-31T00:00:00,1.35,1.0,25.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piumhi,2012-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piumhi,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piumhi,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piumhi,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piumhi,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piumhi,2017-12-31T00:00:00,1.36,2.0,47.06,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piumhi,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piumhi,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piumhi,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piumhi,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Poco_Fundo,2012-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Poco_Fundo,2013-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Poco_Fundo,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Poco_Fundo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Poco_Fundo,2016-12-31T00:00:00,1.41,1.0,29.08,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Poco_Fundo,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Poco_Fundo,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Poco_Fundo,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Poco_Fundo,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Poco_Fundo,2021-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pocos_de_Caldas,2012-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pocos_de_Caldas,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pocos_de_Caldas,2014-12-31T00:00:00,1.14,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pocos_de_Caldas,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pocos_de_Caldas,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pocos_de_Caldas,2017-12-31T00:00:00,1.77,2.0,12.99,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pocos_de_Caldas,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pocos_de_Caldas,2019-12-31T00:00:00,1.08,2.0,85.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pocos_de_Caldas,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pocos_de_Caldas,2021-12-31T00:00:00,1.42,1.0,29.58,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pratinha,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pratinha,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pratinha,2014-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pratinha,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pratinha,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pratinha,2017-12-31T00:00:00,1.72,2.0,16.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pratinha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pratinha,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pratinha,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pratinha,2021-12-31T00:00:00,1.35,2.0,48.15,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Presidente_Olegario,2012-12-31T00:00:00,2.66,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Presidente_Olegario,2013-12-31T00:00:00,2.65,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Presidente_Olegario,2014-12-31T00:00:00,2.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Presidente_Olegario,2015-12-31T00:00:00,1.88,2.0,6.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Presidente_Olegario,2016-12-31T00:00:00,2.52,3.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Presidente_Olegario,2017-12-31T00:00:00,1.84,2.0,8.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Presidente_Olegario,2018-12-31T00:00:00,2.28,2.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Presidente_Olegario,2019-12-31T00:00:00,1.87,2.0,6.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Presidente_Olegario,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Presidente_Olegario,2021-12-31T00:00:00,1.64,2.0,21.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Raul_Soares,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Raul_Soares,2013-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Raul_Soares,2014-12-31T00:00:00,0.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Raul_Soares,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Raul_Soares,2016-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Raul_Soares,2017-12-31T00:00:00,0.86,1.0,16.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Raul_Soares,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Raul_Soares,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Raul_Soares,2020-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Raul_Soares,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Reduto,2012-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Reduto,2013-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Reduto,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Reduto,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Reduto,2016-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Reduto,2017-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Reduto,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Reduto,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Reduto,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Reduto,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Rio_Paranaiba,2012-12-31T00:00:00,2.16,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Rio_Paranaiba,2013-12-31T00:00:00,2.07,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Rio_Paranaiba,2014-12-31T00:00:00,1.89,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Rio_Paranaiba,2015-12-31T00:00:00,1.42,2.0,40.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Rio_Paranaiba,2016-12-31T00:00:00,2.08,2.0,3.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Rio_Paranaiba,2017-12-31T00:00:00,1.39,2.0,43.88,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Rio_Paranaiba,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Rio_Paranaiba,2019-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Rio_Paranaiba,2020-12-31T00:00:00,1.82,2.0,9.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Rio_Paranaiba,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Romaria,2012-12-31T00:00:00,2.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Romaria,2013-12-31T00:00:00,2.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Romaria,2014-12-31T00:00:00,2.28,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Romaria,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Romaria,2016-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Romaria,2017-12-31T00:00:00,1.53,2.0,30.72,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Romaria,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Romaria,2019-12-31T00:00:00,2.2,1.0,54.55,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Romaria,2020-12-31T00:00:00,2.33,2.0,14.16,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Romaria,2021-12-31T00:00:00,2.05,2.0,2.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sacramento,2012-12-31T00:00:00,2.22,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sacramento,2013-12-31T00:00:00,1.87,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sacramento,2014-12-31T00:00:00,1.53,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sacramento,2015-12-31T00:00:00,1.34,1.0,25.37,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sacramento,2016-12-31T00:00:00,1.87,2.0,6.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sacramento,2017-12-31T00:00:00,1.31,2.0,52.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sacramento,2018-12-31T00:00:00,1.87,2.0,6.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sacramento,2019-12-31T00:00:00,1.23,1.0,18.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sacramento,2020-12-31T00:00:00,1.83,2.0,9.29,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sacramento,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Barbara_do_Leste,2012-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Barbara_do_Leste,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Barbara_do_Leste,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Barbara_do_Leste,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Barbara_do_Leste,2016-12-31T00:00:00,1.02,1.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Barbara_do_Leste,2017-12-31T00:00:00,0.91,1.0,9.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Barbara_do_Leste,2018-12-31T00:00:00,1.74,1.0,42.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Barbara_do_Leste,2019-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Barbara_do_Leste,2020-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Barbara_do_Leste,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Margarida,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Margarida,2013-12-31T00:00:00,1.68,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Margarida,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Margarida,2015-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Margarida,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Margarida,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Margarida,2018-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Margarida,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Margarida,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Margarida,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_de_Minas,2012-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_de_Minas,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_de_Minas,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_de_Minas,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_de_Minas,2016-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_de_Minas,2017-12-31T00:00:00,1.25,1.0,20.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_de_Minas,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_de_Minas,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_de_Minas,2020-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_de_Minas,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Itueto,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Itueto,2013-12-31T00:00:00,1.17,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Itueto,2014-12-31T00:00:00,1.11,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Itueto,2015-12-31T00:00:00,0.92,1.0,8.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Itueto,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Itueto,2017-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Itueto,2018-12-31T00:00:00,1.35,1.0,25.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Itueto,2019-12-31T00:00:00,1.65,1.0,39.39,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Itueto,2020-12-31T00:00:00,1.77,2.0,12.99,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Itueto,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2017-12-31T00:00:00,1.79,2.0,11.73,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rosa_da_Serra,2012-12-31T00:00:00,3.17,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rosa_da_Serra,2013-12-31T00:00:00,3.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rosa_da_Serra,2014-12-31T00:00:00,2.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rosa_da_Serra,2015-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rosa_da_Serra,2016-12-31T00:00:00,1.5,3.0,100.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rosa_da_Serra,2017-12-31T00:00:00,1.63,2.0,22.7,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rosa_da_Serra,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rosa_da_Serra,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rosa_da_Serra,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rosa_da_Serra,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_da_Vargem,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_da_Vargem,2013-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_da_Vargem,2014-12-31T00:00:00,1.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_da_Vargem,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_da_Vargem,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_da_Vargem,2017-12-31T00:00:00,2.09,2.0,4.31,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_da_Vargem,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_da_Vargem,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_da_Vargem,2020-12-31T00:00:00,2.22,2.0,9.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_da_Vargem,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_do_Manhuacu,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_do_Manhuacu,2013-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_do_Manhuacu,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_do_Manhuacu,2015-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_do_Manhuacu,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_do_Manhuacu,2017-12-31T00:00:00,1.28,1.0,21.88,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_do_Manhuacu,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_do_Manhuacu,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_do_Manhuacu,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_do_Manhuacu,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2012-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2013-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2014-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2017-12-31T00:00:00,1.34,2.0,49.25,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2018-12-31T00:00:00,2.16,2.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2019-12-31T00:00:00,1.64,1.0,39.02,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Domingos_das_Dores,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Domingos_das_Dores,2013-12-31T00:00:00,1.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Domingos_das_Dores,2014-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Domingos_das_Dores,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Domingos_das_Dores,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Domingos_das_Dores,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Domingos_das_Dores,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Domingos_das_Dores,2019-12-31T00:00:00,1.3,2.0,53.85,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Domingos_das_Dores,2020-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Domingos_das_Dores,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Francisco_de_Paula,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Francisco_de_Paula,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Francisco_de_Paula,2014-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Francisco_de_Paula,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Francisco_de_Paula,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Francisco_de_Paula,2017-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Francisco_de_Paula,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Francisco_de_Paula,2019-12-31T00:00:00,1.64,1.0,39.02,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Francisco_de_Paula,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Francisco_de_Paula,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2012-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2013-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2017-12-31T00:00:00,2.04,2.0,1.96,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2021-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Gotardo,2012-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Gotardo,2013-12-31T00:00:00,1.92,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Gotardo,2014-12-31T00:00:00,1.71,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Gotardo,2015-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Gotardo,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Gotardo,2017-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Gotardo,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Gotardo,2019-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Gotardo,2020-12-31T00:00:00,1.38,2.0,44.93,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Gotardo,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2012-12-31T00:00:00,1.26,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2013-12-31T00:00:00,1.74,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2015-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2017-12-31T00:00:00,1.7,1.0,41.18,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Jose_da_Barra,2012-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Jose_da_Barra,2013-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Jose_da_Barra,2014-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Jose_da_Barra,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Jose_da_Barra,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Jose_da_Barra,2017-12-31T00:00:00,1.91,1.0,47.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Jose_da_Barra,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Jose_da_Barra,2019-12-31T00:00:00,1.7,1.0,41.18,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Jose_da_Barra,2020-12-31T00:00:00,2.54,2.0,21.26,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Jose_da_Barra,2021-12-31T00:00:00,1.68,1.0,40.48,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2012-12-31T00:00:00,2.28,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2014-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2017-12-31T00:00:00,1.69,2.0,18.34,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2021-12-31T00:00:00,1.48,1.0,32.43,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Roque_de_Minas,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Roque_de_Minas,2013-12-31T00:00:00,1.44,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Roque_de_Minas,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Roque_de_Minas,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Roque_de_Minas,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Roque_de_Minas,2017-12-31T00:00:00,1.46,1.0,31.51,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Roque_de_Minas,2018-12-31T00:00:00,1.62,1.0,38.27,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Roque_de_Minas,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Roque_de_Minas,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Roque_de_Minas,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2013-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2017-12-31T00:00:00,1.78,2.0,12.36,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2018-12-31T00:00:00,1.64,2.0,21.95,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2012-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2013-12-31T00:00:00,1.71,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2014-12-31T00:00:00,1.42,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2016-12-31T00:00:00,1.7,2.0,17.65,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2017-12-31T00:00:00,1.54,2.0,29.87,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2018-12-31T00:00:00,1.89,2.0,5.82,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2019-12-31T00:00:00,1.47,2.0,36.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2020-12-31T00:00:00,1.93,2.0,3.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2021-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2012-12-31T00:00:00,1.75,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2013-12-31T00:00:00,1.05,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2014-12-31T00:00:00,1.35,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2017-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sericita,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sericita,2013-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sericita,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sericita,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sericita,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sericita,2017-12-31T00:00:00,1.12,1.0,10.71,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sericita,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sericita,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sericita,2020-12-31T00:00:00,2.52,2.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sericita,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serra_do_Salitre,2012-12-31T00:00:00,3.1,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serra_do_Salitre,2013-12-31T00:00:00,1.74,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serra_do_Salitre,2014-12-31T00:00:00,2.22,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serra_do_Salitre,2015-12-31T00:00:00,1.29,2.0,55.04,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serra_do_Salitre,2016-12-31T00:00:00,3.0,3.0,0.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serra_do_Salitre,2017-12-31T00:00:00,1.12,2.0,78.57,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serra_do_Salitre,2018-12-31T00:00:00,2.34,3.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serra_do_Salitre,2019-12-31T00:00:00,1.49,1.0,32.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serra_do_Salitre,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serra_do_Salitre,2021-12-31T00:00:00,1.47,1.0,31.97,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serrania,2012-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serrania,2013-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serrania,2014-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serrania,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serrania,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serrania,2017-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serrania,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serrania,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serrania,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serrania,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Simonesia,2012-12-31T00:00:00,1.56,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Simonesia,2013-12-31T00:00:00,1.62,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Simonesia,2014-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Simonesia,2015-12-31T00:00:00,1.44,2.0,38.89,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Simonesia,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Simonesia,2017-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Simonesia,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Simonesia,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Simonesia,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Simonesia,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tiros,2012-12-31T00:00:00,2.28,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tiros,2013-12-31T00:00:00,2.28,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tiros,2014-12-31T00:00:00,1.79,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tiros,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tiros,2016-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tiros,2017-12-31T00:00:00,3.0,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tiros,2018-12-31T00:00:00,1.92,3.0,56.25,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tiros,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tiros,2020-12-31T00:00:00,1.74,3.0,72.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tiros,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Coracoes,2012-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Coracoes,2013-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Coracoes,2014-12-31T00:00:00,0.9,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Coracoes,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Coracoes,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Coracoes,2017-12-31T00:00:00,2.67,2.0,25.09,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Coracoes,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Coracoes,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Coracoes,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Coracoes,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Pontas,2012-12-31T00:00:00,1.38,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Pontas,2013-12-31T00:00:00,1.53,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Pontas,2014-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Pontas,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Pontas,2016-12-31T00:00:00,1.86,2.0,7.53,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Pontas,2017-12-31T00:00:00,1.88,2.0,6.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Pontas,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Pontas,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Pontas,2020-12-31T00:00:00,2.26,2.0,11.5,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Pontas,2021-12-31T00:00:00,1.35,2.0,48.15,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Turvolandia,2012-12-31T00:00:00,1.2,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Turvolandia,2013-12-31T00:00:00,1.08,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Turvolandia,2014-12-31T00:00:00,0.84,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Turvolandia,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Turvolandia,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Turvolandia,2017-12-31T00:00:00,1.8,1.0,44.44,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Turvolandia,2018-12-31T00:00:00,2.29,2.0,12.66,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Turvolandia,2019-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Turvolandia,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Turvolandia,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ubaporanga,2012-12-31T00:00:00,1.02,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ubaporanga,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ubaporanga,2014-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ubaporanga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ubaporanga,2016-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ubaporanga,2017-12-31T00:00:00,1.03,1.0,2.91,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ubaporanga,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ubaporanga,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ubaporanga,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ubaporanga,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Unai,2012-12-31T00:00:00,2.94,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Unai,2013-12-31T00:00:00,3.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Unai,2014-12-31T00:00:00,2.94,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Unai,2015-12-31T00:00:00,2.64,3.0,13.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Unai,2016-12-31T00:00:00,2.7,3.0,11.11,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Unai,2017-12-31T00:00:00,2.6,3.0,15.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Unai,2018-12-31T00:00:00,2.52,3.0,19.05,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Unai,2019-12-31T00:00:00,2.82,3.0,6.38,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Unai,2020-12-31T00:00:00,2.34,3.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Unai,2021-12-31T00:00:00,2.64,3.0,13.64,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varginha,2012-12-31T00:00:00,1.32,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varginha,2013-12-31T00:00:00,1.5,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varginha,2014-12-31T00:00:00,1.03,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varginha,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varginha,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varginha,2017-12-31T00:00:00,2.12,2.0,5.66,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varginha,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varginha,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varginha,2020-12-31T00:00:00,1.9,2.0,5.26,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varginha,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varjao_de_Minas,2012-12-31T00:00:00,2.4,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varjao_de_Minas,2013-12-31T00:00:00,2.4,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varjao_de_Minas,2014-12-31T00:00:00,3.0,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varjao_de_Minas,2015-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varjao_de_Minas,2016-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varjao_de_Minas,2017-12-31T00:00:00,2.4,2.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varjao_de_Minas,2018-12-31T00:00:00,2.4,3.0,25.0,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varjao_de_Minas,2019-12-31T00:00:00,2.49,2.0,19.68,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varjao_de_Minas,2020-12-31T00:00:00,2.52,2.0,20.63,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varjao_de_Minas,2021-12-31T00:00:00,2.39,2.0,16.32,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Vermelho_Novo,2012-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Vermelho_Novo,2013-12-31T00:00:00,0.96,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Vermelho_Novo,2014-12-31T00:00:00,0.8,-,-,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Vermelho_Novo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Vermelho_Novo,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Vermelho_Novo,2017-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Vermelho_Novo,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Vermelho_Novo,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Vermelho_Novo,2020-12-31T00:00:00,1.32,1.0,24.24,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Vermelho_Novo,2021-12-31T00:00:00,0.96,1.0,4.17,treino,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Abre_Campo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alfenas,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alpinopolis,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alterosa,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Caparao,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Jequitiba,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Andradas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Angelandia,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araguari,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araponga,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araxa,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Areado,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bambui,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Boa_Esperanca,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Jesus_da_Penha,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Sucesso,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Botelhos,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bueno_Brandao,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Buritizeiro,2022-12-31T00:00:00,3.0,3,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cabo_Verde,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caiana,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cambuquira,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campanha,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campestre,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_Belo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_do_Meio,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Altos,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Gerais,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Candeias,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caparao,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capelinha,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capetinga,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capitolio,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caputira,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carangola,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caratinga,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_da_Cachoeira,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Paranaiba,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Rio_Claro,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cassia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Chale,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Claraval,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_da_Aparecida,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conselheiro_Pena,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coqueiral,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coromandel,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cristais,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divino,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divisa_Nova,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Durande,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Eloi_Mendes,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ervalia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Espera_Feliz,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Estrela_do_Sul,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Fervedouro,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Formiga,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guape,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaranesia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaxupe,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guimarania,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Heliodora,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibiraci,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibitiura_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ilicinea,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Imbe_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inconfidentes,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Indianopolis,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inhapim,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Itamogi,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacui,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacutinga,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jequeri,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Joao_Pinheiro,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Juruaia,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lajinha,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lambari,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lavras,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Luisburgo,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Machado,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhuacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhumirim,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Martins_Soares,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Matipo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Medeiros,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Miradouro,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monsenhor_Paulo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Belo,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Carmelo,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Santo_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Siao,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Mutum,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Muzambinho,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Natercia,2022-12-31T00:00:00,1.0,0,-100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nazareno,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nepomuceno,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ninheira,2022-12-31T00:00:00,3.0,2,-33.33,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Belem,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Resende,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Oliveira,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Orizania,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ouro_Fino,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paracatu,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paraguacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Passos,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patos_de_Minas,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patrocinio,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedra_Bonita,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedralva,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdizes,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdoes,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piedade_de_Caratinga,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pimenta,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piumhi,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Poco_Fundo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pocos_de_Caldas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pratinha,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Presidente_Olegario,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Raul_Soares,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Reduto,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Rio_Paranaiba,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Romaria,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sacramento,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Barbara_do_Leste,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Margarida,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Itueto,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rosa_da_Serra,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_da_Vargem,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_do_Manhuacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Domingos_das_Dores,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Francisco_de_Paula,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Gotardo,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Jose_da_Barra,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2022-12-31T00:00:00,1.0,2,100.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Roque_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sericita,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serra_do_Salitre,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serrania,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Simonesia,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tiros,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Coracoes,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Pontas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Turvolandia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ubaporanga,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Unai,2022-12-31T00:00:00,3.0,3,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varginha,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varjao_de_Minas,2022-12-31T00:00:00,2.0,2,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Vermelho_Novo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Abre_Campo,2023-12-31T00:00:00,1.8000000000000003,1.393073320388794,-22.61,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alfenas,2023-12-31T00:00:00,1.698430922311519,1.7903928756713867,5.41,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alpinopolis,2023-12-31T00:00:00,1.8000000000000003,2.172529935836792,20.7,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alterosa,2023-12-31T00:00:00,1.2603960396039597,1.4444031715393066,14.6,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Caparao,2023-12-31T00:00:00,1.32,1.5715444087982178,19.06,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Alto_Jequitiba,2023-12-31T00:00:00,1.08,1.2877534627914429,19.24,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Andradas,2023-12-31T00:00:00,1.22,1.7255334854125977,41.44,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Angelandia,2023-12-31T00:00:00,2.095663265306122,1.8722013235092163,-10.66,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araguari,2023-12-31T00:00:00,3.0,1.9246727228164673,-35.84,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araponga,2023-12-31T00:00:00,1.5,1.4247586727142334,-5.02,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Araxa,2023-12-31T00:00:00,1.638401296246287,1.4182342290878296,-13.44,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Areado,2023-12-31T00:00:00,1.590034364261168,1.550062656402588,-2.51,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bambui,2023-12-31T00:00:00,1.5,1.578145980834961,5.21,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Boa_Esperanca,2023-12-31T00:00:00,1.08,1.6300151348114014,50.93,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Jesus_da_Penha,2023-12-31T00:00:00,1.2,1.803708553314209,50.31,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bom_Sucesso,2023-12-31T00:00:00,1.5,1.6939557790756226,12.93,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Botelhos,2023-12-31T00:00:00,1.080052666227781,1.5533740520477295,43.82,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Bueno_Brandao,2023-12-31T00:00:00,1.379901960784314,1.5268285274505615,10.65,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Buritizeiro,2023-12-31T00:00:00,3.0,5.394571304321289,79.82,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cabo_Verde,2023-12-31T00:00:00,1.319976635514019,1.713310956954956,29.8,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caiana,2023-12-31T00:00:00,1.08,1.1962357759475708,10.76,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cambuquira,2023-12-31T00:00:00,1.56,1.559813141822815,-0.01,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campanha,2023-12-31T00:00:00,1.327075098814229,1.4900914430618286,12.28,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campestre,2023-12-31T00:00:00,1.319964428634949,1.6106561422348022,22.02,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_Belo,2023-12-31T00:00:00,1.5,1.6580325365066528,10.54,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campo_do_Meio,2023-12-31T00:00:00,0.8771626297577856,1.8699157238006592,113.18,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Altos,2023-12-31T00:00:00,1.380040526849037,1.7915617227554321,29.82,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Campos_Gerais,2023-12-31T00:00:00,1.560242401107029,2.0831289291381836,33.51,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Candeias,2023-12-31T00:00:00,1.080032206119163,1.5405542850494385,42.64,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caparao,2023-12-31T00:00:00,1.32,1.5308575630187988,15.97,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capelinha,2023-12-31T00:00:00,1.541750580945004,1.514894962310791,-1.74,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capetinga,2023-12-31T00:00:00,1.4484046164290563,1.998918056488037,38.01,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Capitolio,2023-12-31T00:00:00,1.32,1.5639162063598633,18.48,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caputira,2023-12-31T00:00:00,1.380065005417118,1.6672146320343018,20.81,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carangola,2023-12-31T00:00:00,1.140118343195266,1.2309949398040771,7.97,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Caratinga,2023-12-31T00:00:00,1.3799999999999997,1.7956352233886719,30.12,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_da_Cachoeira,2023-12-31T00:00:00,1.32,1.5567336082458496,17.93,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_de_Minas,2023-12-31T00:00:00,1.5,1.4977179765701294,-0.15,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Paranaiba,2023-12-31T00:00:00,2.369801007771799,1.722933292388916,-27.3,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Carmo_do_Rio_Claro,2023-12-31T00:00:00,1.816856256463288,2.130354642868042,17.25,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cassia,2023-12-31T00:00:00,1.6377649325626198,1.7208653688430786,5.07,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Chale,2023-12-31T00:00:00,1.3799999999999997,1.4871149063110352,7.76,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Claraval,2023-12-31T00:00:00,1.8600214362272243,1.7902871370315552,-3.75,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_da_Aparecida,2023-12-31T00:00:00,1.679948420373952,1.9024344682693481,13.24,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2023-12-31T00:00:00,1.44,1.6462712287902832,14.32,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Conselheiro_Pena,2023-12-31T00:00:00,1.320048602673147,1.6737258434295654,26.79,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coqueiral,2023-12-31T00:00:00,1.32,1.7303138971328735,31.08,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Coromandel,2023-12-31T00:00:00,2.0926575541308825,1.567144513130188,-25.11,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Cristais,2023-12-31T00:00:00,1.5631111111111111,1.645735740661621,5.29,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divino,2023-12-31T00:00:00,1.02,1.3501746654510498,32.37,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Divisa_Nova,2023-12-31T00:00:00,1.680092592592593,1.4975132942199707,-10.87,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Durande,2023-12-31T00:00:00,1.3799999999999997,1.8324886560440063,32.79,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Eloi_Mendes,2023-12-31T00:00:00,1.577412806790921,1.6112024784088135,2.14,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ervalia,2023-12-31T00:00:00,1.380044843049327,1.2800012826919556,-7.25,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Espera_Feliz,2023-12-31T00:00:00,1.08,1.478731393814087,36.92,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Estrela_do_Sul,2023-12-31T00:00:00,2.43015873015873,1.8084716796875,-25.58,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Fervedouro,2023-12-31T00:00:00,1.2,1.4304440021514893,19.2,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Formiga,2023-12-31T00:00:00,2.0215439856373436,1.745428204536438,-13.66,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guape,2023-12-31T00:00:00,1.560674157303371,1.5739116668701172,0.85,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaranesia,2023-12-31T00:00:00,1.1961439588688951,1.3557370901107788,13.34,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guaxupe,2023-12-31T00:00:00,1.2,1.4106508493423462,17.55,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Guimarania,2023-12-31T00:00:00,1.7704600484261497,1.665701985359192,-5.92,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Heliodora,2023-12-31T00:00:00,1.439872408293461,1.341760277748108,-6.81,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibia,2023-12-31T00:00:00,1.668,1.4773104190826416,-11.43,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibiraci,2023-12-31T00:00:00,1.8041698841698839,1.927476167678833,6.83,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ibitiura_de_Minas,2023-12-31T00:00:00,1.150344827586207,1.6869161128997803,46.64,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ilicinea,2023-12-31T00:00:00,1.560032362459547,1.728022813796997,10.77,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Imbe_de_Minas,2023-12-31T00:00:00,1.3799999999999997,1.368873953819275,-0.81,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inconfidentes,2023-12-31T00:00:00,1.7401360544217692,2.000168800354004,14.94,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Indianopolis,2023-12-31T00:00:00,2.7000000000000006,1.8412864208221436,-31.8,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Inhapim,2023-12-31T00:00:00,2.7000000000000006,1.6988537311553955,-37.08,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Itamogi,2023-12-31T00:00:00,1.740011254924029,1.887020230293274,8.45,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacui,2023-12-31T00:00:00,1.32,1.3649601936340332,3.41,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jacutinga,2023-12-31T00:00:00,1.160053262316911,1.516369342803955,30.72,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Jequeri,2023-12-31T00:00:00,1.5,1.8892515897750854,25.95,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Joao_Pinheiro,2023-12-31T00:00:00,3.3,2.752236843109131,-16.6,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Juruaia,2023-12-31T00:00:00,1.5,1.6944133043289185,12.96,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lajinha,2023-12-31T00:00:00,1.4399536768963517,1.6091969013214111,11.75,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lambari,2023-12-31T00:00:00,1.56,1.4816123247146606,-5.02,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Lavras,2023-12-31T00:00:00,1.5,1.4871914386749268,-0.85,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Luisburgo,2023-12-31T00:00:00,1.44,1.7017033100128174,18.17,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Machado,2023-12-31T00:00:00,1.500560931145702,1.700246810913086,13.31,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhuacu,2023-12-31T00:00:00,1.2,1.517706036567688,26.48,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Manhumirim,2023-12-31T00:00:00,1.44,1.5045690536499023,4.48,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Martins_Soares,2023-12-31T00:00:00,1.2,1.5379116535186768,28.16,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Matipo,2023-12-31T00:00:00,1.25990675990676,1.445218801498413,14.71,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Medeiros,2023-12-31T00:00:00,1.8599999999999999,1.7164185047149658,-7.72,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Miradouro,2023-12-31T00:00:00,1.140084388185654,1.2181912660598755,6.85,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monsenhor_Paulo,2023-12-31T00:00:00,1.389931972789116,1.724524974822998,24.07,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Belo,2023-12-31T00:00:00,1.3799999999999997,1.5509717464447021,12.39,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Carmelo,2023-12-31T00:00:00,2.4900284900284904,1.8927768468856812,-23.99,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Santo_de_Minas,2023-12-31T00:00:00,1.620050377833753,1.8201111555099487,12.35,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Monte_Siao,2023-12-31T00:00:00,1.5,1.5648293495178223,4.32,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Mutum,2023-12-31T00:00:00,1.019985196150999,1.5265247821807861,49.66,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Muzambinho,2023-12-31T00:00:00,1.68,1.5710704326629639,-6.48,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Natercia,2023-12-31T00:00:00,1.5,1.8184431791305542,21.23,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nazareno,2023-12-31T00:00:00,1.5599056603773582,1.6954600811004639,8.69,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nepomuceno,2023-12-31T00:00:00,1.650045578851413,1.5657005310058594,-5.11,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ninheira,2023-12-31T00:00:00,3.0,2.831658363342285,-5.61,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Belem,2023-12-31T00:00:00,0.9599999999999999,1.0644725561141968,10.88,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Nova_Resende,2023-12-31T00:00:00,1.56,1.9372307062149048,24.18,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Oliveira,2023-12-31T00:00:00,1.8240227434257288,1.7968943119049072,-1.49,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Orizania,2023-12-31T00:00:00,1.2,1.57623291015625,31.35,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ouro_Fino,2023-12-31T00:00:00,1.68,1.5928664207458496,-5.19,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paracatu,2023-12-31T00:00:00,2.4,2.52431058883667,5.18,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Paraguacu,2023-12-31T00:00:00,1.2982914572864321,1.6126590967178345,24.21,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Passos,2023-12-31T00:00:00,1.8298013245033107,1.4390888214111328,-21.35,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patos_de_Minas,2023-12-31T00:00:00,1.782452316076294,1.847313404083252,3.64,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Patrocinio,2023-12-31T00:00:00,2.4370275910039414,1.4113167524337769,-42.09,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedra_Bonita,2023-12-31T00:00:00,1.08,1.7105491161346436,58.38,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pedralva,2023-12-31T00:00:00,1.67998417721519,1.491576910018921,-11.21,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdizes,2023-12-31T00:00:00,2.106451612903226,1.630476951599121,-22.6,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Perdoes,2023-12-31T00:00:00,1.6799401197604789,1.5459418296813965,-7.98,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piedade_de_Caratinga,2023-12-31T00:00:00,2.4,1.5583832263946533,-35.07,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pimenta,2023-12-31T00:00:00,1.504186046511628,1.6152148246765137,7.38,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Piumhi,2023-12-31T00:00:00,1.7399633363886338,1.544837236404419,-11.21,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Poco_Fundo,2023-12-31T00:00:00,1.380160799652325,1.3215738534927368,-4.24,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pocos_de_Caldas,2023-12-31T00:00:00,1.5,1.4826946258544922,-1.15,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Pratinha,2023-12-31T00:00:00,2.13,1.84373939037323,-13.44,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Presidente_Olegario,2023-12-31T00:00:00,2.160074626865672,1.8368139266967773,-14.97,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Raul_Soares,2023-12-31T00:00:00,1.5,1.3384239673614502,-10.77,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Reduto,2023-12-31T00:00:00,0.9,1.5291252136230469,69.9,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Rio_Paranaiba,2023-12-31T00:00:00,1.8000000000000003,1.4834749698638916,-17.58,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Romaria,2023-12-31T00:00:00,2.131578947368421,2.147237777709961,0.73,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sacramento,2023-12-31T00:00:00,1.75,1.690585732460022,-3.4,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Barbara_do_Leste,2023-12-31T00:00:00,1.08,1.336971640586853,23.79,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Margarida,2023-12-31T00:00:00,1.32,1.5944783687591553,20.79,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_de_Minas,2023-12-31T00:00:00,1.019930675909879,1.312074899673462,28.64,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Itueto,2023-12-31T00:00:00,1.2,1.7132041454315186,42.77,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2023-12-31T00:00:00,1.3799999999999997,1.5216984748840332,10.27,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santa_Rosa_da_Serra,2023-12-31T00:00:00,1.68,2.0405616760253906,21.46,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_da_Vargem,2023-12-31T00:00:00,1.5593593593593589,1.7111672163009644,9.74,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santana_do_Manhuacu,2023-12-31T00:00:00,1.32,1.5433557033538818,16.92,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2023-12-31T00:00:00,1.8000000000000003,1.6589492559432983,-7.84,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Domingos_das_Dores,2023-12-31T00:00:00,1.296078431372549,1.3513504266738892,4.26,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Francisco_de_Paula,2023-12-31T00:00:00,1.56,1.5865154266357422,1.7,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2023-12-31T00:00:00,1.2,1.9672448635101318,63.94,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Gotardo,2023-12-31T00:00:00,1.5,1.4202139377593994,-5.32,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2023-12-31T00:00:00,1.2,1.526045560836792,27.17,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Jose_da_Barra,2023-12-31T00:00:00,2.2413698630136984,2.3204891681671143,3.53,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2023-12-31T00:00:00,1.5,1.8665266036987305,24.44,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Roque_de_Minas,2023-12-31T00:00:00,1.8000000000000003,1.526733636856079,-15.18,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2023-12-31T00:00:00,1.7998212689901698,1.7503900527954102,-2.75,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2023-12-31T00:00:00,1.4400177462289259,1.6243183612823486,12.8,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2023-12-31T00:00:00,1.319968346082828,1.764970302581787,33.71,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Sericita,2023-12-31T00:00:00,1.26,2.079585075378418,65.05,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serra_do_Salitre,2023-12-31T00:00:00,1.489636363636364,1.6481852531433105,10.64,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Serrania,2023-12-31T00:00:00,0.9,1.3914527893066406,54.61,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Simonesia,2023-12-31T00:00:00,1.5,1.7741777896881104,18.28,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tiros,2023-12-31T00:00:00,2.382038834951457,1.6178079843521118,-32.08,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Coracoes,2023-12-31T00:00:00,1.44,1.7602568864822388,22.24,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Tres_Pontas,2023-12-31T00:00:00,1.5900000000000003,1.7255029678344727,8.52,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Turvolandia,2023-12-31T00:00:00,1.259748427672956,1.5080082416534424,19.71,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Ubaporanga,2023-12-31T00:00:00,1.5,1.4379894733428955,-4.13,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Unai,2023-12-31T00:00:00,2.520095187731359,2.5256659984588623,0.22,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varginha,2023-12-31T00:00:00,1.679990280646337,1.5373928546905518,-8.49,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Varjao_de_Minas,2023-12-31T00:00:00,2.3892857142857142,2.438256025314331,2.05,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
IBGE V5 - Metodo 2 (2023),Vermelho_Novo,2023-12-31T00:00:00,1.5,1.1383711099624634,-24.11,teste,V27,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V27, onde os cluster entraram como features.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.0001401595795066046
encoder_hidden_size: 128
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 600
",2025-09-04T15:43:19
