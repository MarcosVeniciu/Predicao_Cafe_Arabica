treino_id,unique_id,ds,y,y_pred,diferença_%,flag,dataset,modelo,comentario,data_treino
IBGE V5 Cluster 0 - Metodo 1 (2023),Abre_Campo,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Abre_Campo,2013-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Abre_Campo,2014-12-31T00:00:00,0.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Abre_Campo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Abre_Campo,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Abre_Campo,2017-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Abre_Campo,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Abre_Campo,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Abre_Campo,2020-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Abre_Campo,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Angelandia,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Angelandia,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Angelandia,2014-12-31T00:00:00,1.15,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Angelandia,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Angelandia,2016-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Angelandia,2017-12-31T00:00:00,0.94,1.0,6.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Angelandia,2018-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Angelandia,2019-12-31T00:00:00,1.22,1.0,18.03,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Angelandia,2020-12-31T00:00:00,1.84,2.0,8.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Angelandia,2021-12-31T00:00:00,1.51,2.0,32.45,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Buritizeiro,2012-12-31T00:00:00,3.6,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Buritizeiro,2013-12-31T00:00:00,3.6,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Buritizeiro,2014-12-31T00:00:00,3.6,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Buritizeiro,2015-12-31T00:00:00,1.92,1.0,47.92,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Buritizeiro,2016-12-31T00:00:00,2.1,4.0,90.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Buritizeiro,2017-12-31T00:00:00,1.97,3.0,52.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Buritizeiro,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Buritizeiro,2019-12-31T00:00:00,2.88,2.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Buritizeiro,2020-12-31T00:00:00,3.0,3.0,0.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Buritizeiro,2021-12-31T00:00:00,3.0,3.0,0.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Capelinha,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Capelinha,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Capelinha,2014-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Capelinha,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Capelinha,2016-12-31T00:00:00,1.08,2.0,85.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Capelinha,2017-12-31T00:00:00,1.16,1.0,13.79,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Capelinha,2018-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Capelinha,2019-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Capelinha,2020-12-31T00:00:00,1.43,1.0,30.07,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Capelinha,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Conselheiro_Pena,2012-12-31T00:00:00,1.48,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Conselheiro_Pena,2013-12-31T00:00:00,1.3,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Conselheiro_Pena,2014-12-31T00:00:00,1.3,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Conselheiro_Pena,2015-12-31T00:00:00,1.06,1.0,5.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Conselheiro_Pena,2016-12-31T00:00:00,1.18,1.0,15.25,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Conselheiro_Pena,2017-12-31T00:00:00,1.29,1.0,22.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Conselheiro_Pena,2018-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Conselheiro_Pena,2019-12-31T00:00:00,1.3,1.0,23.08,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Conselheiro_Pena,2020-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Conselheiro_Pena,2021-12-31T00:00:00,1.29,1.0,22.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Fervedouro,2012-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Fervedouro,2013-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Fervedouro,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Fervedouro,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Fervedouro,2016-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Fervedouro,2017-12-31T00:00:00,1.06,1.0,5.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Fervedouro,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Fervedouro,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Fervedouro,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Fervedouro,2021-12-31T00:00:00,1.15,1.0,13.04,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Formiga,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Formiga,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Formiga,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Formiga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Formiga,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Formiga,2017-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Formiga,2018-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Formiga,2019-12-31T00:00:00,1.24,2.0,61.29,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Formiga,2020-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Formiga,2021-12-31T00:00:00,1.35,1.0,25.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Imbe_de_Minas,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Imbe_de_Minas,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Imbe_de_Minas,2014-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Imbe_de_Minas,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Imbe_de_Minas,2016-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Imbe_de_Minas,2017-12-31T00:00:00,1.29,1.0,22.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Imbe_de_Minas,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Imbe_de_Minas,2019-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Imbe_de_Minas,2020-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Imbe_de_Minas,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Inhapim,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Inhapim,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Inhapim,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Inhapim,2015-12-31T00:00:00,1.21,1.0,17.36,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Inhapim,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Inhapim,2017-12-31T00:00:00,1.12,1.0,10.71,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Inhapim,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Inhapim,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Inhapim,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Inhapim,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Jequeri,2012-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Jequeri,2013-12-31T00:00:00,1.86,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Jequeri,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Jequeri,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Jequeri,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Jequeri,2017-12-31T00:00:00,1.28,1.0,21.88,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Jequeri,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Jequeri,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Jequeri,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Jequeri,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Passos,2012-12-31T00:00:00,2.16,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Passos,2013-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Passos,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Passos,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Passos,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Passos,2017-12-31T00:00:00,1.92,1.0,47.92,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Passos,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Passos,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Passos,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Passos,2021-12-31T00:00:00,0.99,2.0,102.02,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pedra_Bonita,2012-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pedra_Bonita,2013-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pedra_Bonita,2014-12-31T00:00:00,0.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pedra_Bonita,2015-12-31T00:00:00,0.6,1.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pedra_Bonita,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pedra_Bonita,2017-12-31T00:00:00,0.86,1.0,16.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pedra_Bonita,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pedra_Bonita,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pedra_Bonita,2020-12-31T00:00:00,1.74,1.0,42.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pedra_Bonita,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Piedade_de_Caratinga,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Piedade_de_Caratinga,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Piedade_de_Caratinga,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Piedade_de_Caratinga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Piedade_de_Caratinga,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Piedade_de_Caratinga,2017-12-31T00:00:00,1.59,1.0,37.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Piedade_de_Caratinga,2018-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Piedade_de_Caratinga,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Piedade_de_Caratinga,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Piedade_de_Caratinga,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pratinha,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pratinha,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pratinha,2014-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pratinha,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pratinha,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pratinha,2017-12-31T00:00:00,1.72,2.0,16.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pratinha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pratinha,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pratinha,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pratinha,2021-12-31T00:00:00,1.35,2.0,48.15,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Santa_Rita_do_Itueto,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Santa_Rita_do_Itueto,2013-12-31T00:00:00,1.17,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Santa_Rita_do_Itueto,2014-12-31T00:00:00,1.11,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Santa_Rita_do_Itueto,2015-12-31T00:00:00,0.92,1.0,8.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Santa_Rita_do_Itueto,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Santa_Rita_do_Itueto,2017-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Santa_Rita_do_Itueto,2018-12-31T00:00:00,1.35,1.0,25.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Santa_Rita_do_Itueto,2019-12-31T00:00:00,1.65,1.0,39.39,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Santa_Rita_do_Itueto,2020-12-31T00:00:00,1.77,2.0,12.99,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Santa_Rita_do_Itueto,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Francisco_de_Paula,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Francisco_de_Paula,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Francisco_de_Paula,2014-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Francisco_de_Paula,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Francisco_de_Paula,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Francisco_de_Paula,2017-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Francisco_de_Paula,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Francisco_de_Paula,2019-12-31T00:00:00,1.64,2.0,21.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Francisco_de_Paula,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Francisco_de_Paula,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Jose_da_Barra,2012-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Jose_da_Barra,2013-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Jose_da_Barra,2014-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Jose_da_Barra,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Jose_da_Barra,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Jose_da_Barra,2017-12-31T00:00:00,1.91,2.0,4.71,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Jose_da_Barra,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Jose_da_Barra,2019-12-31T00:00:00,1.7,2.0,17.65,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Jose_da_Barra,2020-12-31T00:00:00,2.54,2.0,21.26,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Jose_da_Barra,2021-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Roque_de_Minas,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Roque_de_Minas,2013-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Roque_de_Minas,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Roque_de_Minas,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Roque_de_Minas,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Roque_de_Minas,2017-12-31T00:00:00,1.46,1.0,31.51,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Roque_de_Minas,2018-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Roque_de_Minas,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Roque_de_Minas,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Roque_de_Minas,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2013-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2017-12-31T00:00:00,1.78,2.0,12.36,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2018-12-31T00:00:00,1.64,2.0,21.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Simonesia,2012-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Simonesia,2013-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Simonesia,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Simonesia,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Simonesia,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Simonesia,2017-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Simonesia,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Simonesia,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Simonesia,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Simonesia,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Abre_Campo,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Angelandia,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Buritizeiro,2022-12-31T00:00:00,3.0,3,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Capelinha,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Conselheiro_Pena,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Fervedouro,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Formiga,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Imbe_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Inhapim,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Jequeri,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Passos,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pedra_Bonita,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Piedade_de_Caratinga,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pratinha,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Santa_Rita_do_Itueto,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Francisco_de_Paula,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Jose_da_Barra,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Roque_de_Minas,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Simonesia,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Abre_Campo,2023-12-31T00:00:00,1.8000000000000005,1.2824872732162476,-28.75,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Angelandia,2023-12-31T00:00:00,2.095663265306122,1.774924874305725,-15.3,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Buritizeiro,2023-12-31T00:00:00,3.0,3.1721017360687256,5.74,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Capelinha,2023-12-31T00:00:00,1.541750580945004,1.5030184984207153,-2.51,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Conselheiro_Pena,2023-12-31T00:00:00,1.320048602673147,1.5870709419250488,20.23,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Fervedouro,2023-12-31T00:00:00,1.2,1.3822917938232422,15.19,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Formiga,2023-12-31T00:00:00,2.021543985637344,1.7073304653167725,-15.54,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Imbe_de_Minas,2023-12-31T00:00:00,1.3799999999999997,1.367974877357483,-0.87,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Inhapim,2023-12-31T00:00:00,2.7000000000000006,1.6557972431182861,-38.67,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Jequeri,2023-12-31T00:00:00,1.5,1.7997955083847046,19.99,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Passos,2023-12-31T00:00:00,1.829801324503311,2.1091787815093994,15.27,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pedra_Bonita,2023-12-31T00:00:00,1.08,1.5118151903152466,39.98,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Piedade_de_Caratinga,2023-12-31T00:00:00,2.4,1.6139705181121826,-32.75,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Pratinha,2023-12-31T00:00:00,2.13,1.776176929473877,-16.61,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Santa_Rita_do_Itueto,2023-12-31T00:00:00,1.2,1.585745096206665,32.15,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Francisco_de_Paula,2023-12-31T00:00:00,1.56,1.6131696701049805,3.41,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Jose_da_Barra,2023-12-31T00:00:00,2.2413698630136984,2.445218563079834,9.09,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Roque_de_Minas,2023-12-31T00:00:00,1.8000000000000005,1.4287108182907104,-20.63,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Sao_Sebastiao_do_Anta,2023-12-31T00:00:00,1.7998212689901698,1.7431845664978027,-3.15,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 1 (2023),Simonesia,2023-12-31T00:00:00,1.5,1.5824891328811646,5.5,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.00011328430808051102
encoder_hidden_size: 64
decoder_layers: 2
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:49:51
IBGE V5 Cluster 0 - Metodo 2 (2023),Abre_Campo,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Abre_Campo,2013-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Abre_Campo,2014-12-31T00:00:00,0.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Abre_Campo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Abre_Campo,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Abre_Campo,2017-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Abre_Campo,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Abre_Campo,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Abre_Campo,2020-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Abre_Campo,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Angelandia,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Angelandia,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Angelandia,2014-12-31T00:00:00,1.15,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Angelandia,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Angelandia,2016-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Angelandia,2017-12-31T00:00:00,0.94,1.0,6.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Angelandia,2018-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Angelandia,2019-12-31T00:00:00,1.22,1.0,18.03,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Angelandia,2020-12-31T00:00:00,1.84,2.0,8.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Angelandia,2021-12-31T00:00:00,1.51,2.0,32.45,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Buritizeiro,2012-12-31T00:00:00,3.6,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Buritizeiro,2013-12-31T00:00:00,3.6,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Buritizeiro,2014-12-31T00:00:00,3.6,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Buritizeiro,2015-12-31T00:00:00,1.92,-0.0,100.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Buritizeiro,2016-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Buritizeiro,2017-12-31T00:00:00,1.97,3.0,52.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Buritizeiro,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Buritizeiro,2019-12-31T00:00:00,2.88,2.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Buritizeiro,2020-12-31T00:00:00,3.0,3.0,0.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Buritizeiro,2021-12-31T00:00:00,3.0,3.0,0.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Capelinha,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Capelinha,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Capelinha,2014-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Capelinha,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Capelinha,2016-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Capelinha,2017-12-31T00:00:00,1.16,1.0,13.79,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Capelinha,2018-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Capelinha,2019-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Capelinha,2020-12-31T00:00:00,1.43,1.0,30.07,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Capelinha,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Conselheiro_Pena,2012-12-31T00:00:00,1.48,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Conselheiro_Pena,2013-12-31T00:00:00,1.3,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Conselheiro_Pena,2014-12-31T00:00:00,1.3,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Conselheiro_Pena,2015-12-31T00:00:00,1.06,1.0,5.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Conselheiro_Pena,2016-12-31T00:00:00,1.18,1.0,15.25,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Conselheiro_Pena,2017-12-31T00:00:00,1.29,1.0,22.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Conselheiro_Pena,2018-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Conselheiro_Pena,2019-12-31T00:00:00,1.3,1.0,23.08,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Conselheiro_Pena,2020-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Conselheiro_Pena,2021-12-31T00:00:00,1.29,1.0,22.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Fervedouro,2012-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Fervedouro,2013-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Fervedouro,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Fervedouro,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Fervedouro,2016-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Fervedouro,2017-12-31T00:00:00,1.06,1.0,5.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Fervedouro,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Fervedouro,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Fervedouro,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Fervedouro,2021-12-31T00:00:00,1.15,1.0,13.04,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Formiga,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Formiga,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Formiga,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Formiga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Formiga,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Formiga,2017-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Formiga,2018-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Formiga,2019-12-31T00:00:00,1.24,1.0,19.35,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Formiga,2020-12-31T00:00:00,1.44,3.0,108.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Formiga,2021-12-31T00:00:00,1.35,1.0,25.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Imbe_de_Minas,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Imbe_de_Minas,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Imbe_de_Minas,2014-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Imbe_de_Minas,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Imbe_de_Minas,2016-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Imbe_de_Minas,2017-12-31T00:00:00,1.29,1.0,22.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Imbe_de_Minas,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Imbe_de_Minas,2019-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Imbe_de_Minas,2020-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Imbe_de_Minas,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Inhapim,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Inhapim,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Inhapim,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Inhapim,2015-12-31T00:00:00,1.21,1.0,17.36,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Inhapim,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Inhapim,2017-12-31T00:00:00,1.12,2.0,78.57,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Inhapim,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Inhapim,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Inhapim,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Inhapim,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Jequeri,2012-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Jequeri,2013-12-31T00:00:00,1.86,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Jequeri,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Jequeri,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Jequeri,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Jequeri,2017-12-31T00:00:00,1.28,1.0,21.88,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Jequeri,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Jequeri,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Jequeri,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Jequeri,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Passos,2012-12-31T00:00:00,2.16,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Passos,2013-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Passos,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Passos,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Passos,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Passos,2017-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Passos,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Passos,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Passos,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Passos,2021-12-31T00:00:00,0.99,1.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pedra_Bonita,2012-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pedra_Bonita,2013-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pedra_Bonita,2014-12-31T00:00:00,0.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pedra_Bonita,2015-12-31T00:00:00,0.6,1.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pedra_Bonita,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pedra_Bonita,2017-12-31T00:00:00,0.86,1.0,16.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pedra_Bonita,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pedra_Bonita,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pedra_Bonita,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pedra_Bonita,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Piedade_de_Caratinga,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Piedade_de_Caratinga,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Piedade_de_Caratinga,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Piedade_de_Caratinga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Piedade_de_Caratinga,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Piedade_de_Caratinga,2017-12-31T00:00:00,1.59,1.0,37.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Piedade_de_Caratinga,2018-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Piedade_de_Caratinga,2019-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Piedade_de_Caratinga,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Piedade_de_Caratinga,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pratinha,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pratinha,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pratinha,2014-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pratinha,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pratinha,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pratinha,2017-12-31T00:00:00,1.72,2.0,16.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pratinha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pratinha,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pratinha,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pratinha,2021-12-31T00:00:00,1.35,1.0,25.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Santa_Rita_do_Itueto,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Santa_Rita_do_Itueto,2013-12-31T00:00:00,1.17,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Santa_Rita_do_Itueto,2014-12-31T00:00:00,1.11,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Santa_Rita_do_Itueto,2015-12-31T00:00:00,0.92,1.0,8.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Santa_Rita_do_Itueto,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Santa_Rita_do_Itueto,2017-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Santa_Rita_do_Itueto,2018-12-31T00:00:00,1.35,2.0,48.15,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Santa_Rita_do_Itueto,2019-12-31T00:00:00,1.65,1.0,39.39,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Santa_Rita_do_Itueto,2020-12-31T00:00:00,1.77,2.0,12.99,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Santa_Rita_do_Itueto,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Francisco_de_Paula,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Francisco_de_Paula,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Francisco_de_Paula,2014-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Francisco_de_Paula,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Francisco_de_Paula,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Francisco_de_Paula,2017-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Francisco_de_Paula,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Francisco_de_Paula,2019-12-31T00:00:00,1.64,2.0,21.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Francisco_de_Paula,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Francisco_de_Paula,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Jose_da_Barra,2012-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Jose_da_Barra,2013-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Jose_da_Barra,2014-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Jose_da_Barra,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Jose_da_Barra,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Jose_da_Barra,2017-12-31T00:00:00,1.91,2.0,4.71,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Jose_da_Barra,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Jose_da_Barra,2019-12-31T00:00:00,1.7,2.0,17.65,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Jose_da_Barra,2020-12-31T00:00:00,2.54,2.0,21.26,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Jose_da_Barra,2021-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Roque_de_Minas,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Roque_de_Minas,2013-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Roque_de_Minas,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Roque_de_Minas,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Roque_de_Minas,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Roque_de_Minas,2017-12-31T00:00:00,1.46,1.0,31.51,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Roque_de_Minas,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Roque_de_Minas,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Roque_de_Minas,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Roque_de_Minas,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2013-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2017-12-31T00:00:00,1.78,21.0,1079.78,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2018-12-31T00:00:00,1.64,2.0,21.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Simonesia,2012-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Simonesia,2013-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Simonesia,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Simonesia,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Simonesia,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Simonesia,2017-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Simonesia,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Simonesia,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Simonesia,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Simonesia,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Abre_Campo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Angelandia,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Buritizeiro,2022-12-31T00:00:00,3.0,3,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Capelinha,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Conselheiro_Pena,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Fervedouro,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Formiga,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Imbe_de_Minas,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Inhapim,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Jequeri,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Passos,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pedra_Bonita,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Piedade_de_Caratinga,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pratinha,2022-12-31T00:00:00,2.0,3,50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Santa_Rita_do_Itueto,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Francisco_de_Paula,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Jose_da_Barra,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Roque_de_Minas,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Simonesia,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Abre_Campo,2023-12-31T00:00:00,1.8000000000000005,1.3073190450668335,-27.37,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Angelandia,2023-12-31T00:00:00,2.095663265306122,1.8707166910171509,-10.73,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Buritizeiro,2023-12-31T00:00:00,3.0,13.995795249938965,366.53,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Capelinha,2023-12-31T00:00:00,1.541750580945004,1.4997162818908691,-2.73,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Conselheiro_Pena,2023-12-31T00:00:00,1.320048602673147,1.5951964855194092,20.84,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Fervedouro,2023-12-31T00:00:00,1.2,1.4318304061889648,19.32,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Formiga,2023-12-31T00:00:00,2.021543985637344,1.7493914365768433,-13.46,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Imbe_de_Minas,2023-12-31T00:00:00,1.3799999999999997,1.359905481338501,-1.46,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Inhapim,2023-12-31T00:00:00,2.7000000000000006,1.6000380516052246,-40.74,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Jequeri,2023-12-31T00:00:00,1.5,1.79457426071167,19.64,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Passos,2023-12-31T00:00:00,1.829801324503311,1.4987646341323853,-18.09,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pedra_Bonita,2023-12-31T00:00:00,1.08,1.7303764820098877,60.22,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Piedade_de_Caratinga,2023-12-31T00:00:00,2.4,1.5766490697860718,-34.31,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Pratinha,2023-12-31T00:00:00,2.13,1.6554197072982788,-22.28,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Santa_Rita_do_Itueto,2023-12-31T00:00:00,1.2,1.5166504383087158,26.39,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Francisco_de_Paula,2023-12-31T00:00:00,1.56,1.6776108741760254,7.54,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Jose_da_Barra,2023-12-31T00:00:00,2.2413698630136984,2.587493419647217,15.44,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Roque_de_Minas,2023-12-31T00:00:00,1.8000000000000005,1.353291392326355,-24.82,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Sao_Sebastiao_do_Anta,2023-12-31T00:00:00,1.7998212689901698,1.701984167098999,-5.44,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 0 - Metodo 2 (2023),Simonesia,2023-12-31T00:00:00,1.5,1.706839680671692,13.79,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 0.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.0007191018555277156
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:52:14
IBGE V5 Cluster 1 - Metodo 1 (2023),Araguari,2012-12-31T00:00:00,2.4,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araguari,2013-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araguari,2014-12-31T00:00:00,2.4,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araguari,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araguari,2016-12-31T00:00:00,2.58,2.0,22.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araguari,2017-12-31T00:00:00,2.76,2.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araguari,2018-12-31T00:00:00,2.64,2.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araguari,2019-12-31T00:00:00,1.68,3.0,78.57,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araguari,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araguari,2021-12-31T00:00:00,1.86,2.0,7.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araxa,2012-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araxa,2013-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araxa,2014-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araxa,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araxa,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araxa,2017-12-31T00:00:00,1.76,2.0,13.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araxa,2018-12-31T00:00:00,1.94,2.0,3.09,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araxa,2019-12-31T00:00:00,1.39,2.0,43.88,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araxa,2020-12-31T00:00:00,1.79,2.0,11.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araxa,2021-12-31T00:00:00,1.25,1.0,20.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Bambui,2012-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Bambui,2013-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Bambui,2014-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Bambui,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Bambui,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Bambui,2017-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Bambui,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Bambui,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Bambui,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Bambui,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Campos_Altos,2012-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Campos_Altos,2013-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Campos_Altos,2014-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Campos_Altos,2015-12-31T00:00:00,1.71,2.0,16.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Campos_Altos,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Campos_Altos,2017-12-31T00:00:00,1.99,2.0,0.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Campos_Altos,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Campos_Altos,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Campos_Altos,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Campos_Altos,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Carmo_do_Paranaiba,2012-12-31T00:00:00,2.11,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Carmo_do_Paranaiba,2013-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Carmo_do_Paranaiba,2014-12-31T00:00:00,1.93,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Carmo_do_Paranaiba,2015-12-31T00:00:00,1.89,2.0,5.82,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Carmo_do_Paranaiba,2016-12-31T00:00:00,2.37,2.0,15.61,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Carmo_do_Paranaiba,2017-12-31T00:00:00,1.75,2.0,14.29,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Carmo_do_Paranaiba,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Carmo_do_Paranaiba,2019-12-31T00:00:00,1.79,2.0,11.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Carmo_do_Paranaiba,2020-12-31T00:00:00,1.95,2.0,2.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Carmo_do_Paranaiba,2021-12-31T00:00:00,1.54,2.0,29.87,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Claraval,2012-12-31T00:00:00,2.55,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Claraval,2013-12-31T00:00:00,1.42,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Claraval,2014-12-31T00:00:00,2.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Claraval,2015-12-31T00:00:00,1.02,1.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Claraval,2016-12-31T00:00:00,2.34,2.0,14.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Claraval,2017-12-31T00:00:00,1.22,1.0,18.03,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Claraval,2018-12-31T00:00:00,2.55,1.0,60.78,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Claraval,2019-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Claraval,2020-12-31T00:00:00,2.8,2.0,28.57,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Claraval,2021-12-31T00:00:00,1.05,1.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Coromandel,2012-12-31T00:00:00,1.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Coromandel,2013-12-31T00:00:00,2.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Coromandel,2014-12-31T00:00:00,1.91,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Coromandel,2015-12-31T00:00:00,1.91,2.0,4.71,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Coromandel,2016-12-31T00:00:00,2.09,2.0,4.31,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Coromandel,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Coromandel,2018-12-31T00:00:00,2.65,2.0,24.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Coromandel,2019-12-31T00:00:00,1.65,2.0,21.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Coromandel,2020-12-31T00:00:00,1.82,2.0,9.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Coromandel,2021-12-31T00:00:00,1.25,2.0,60.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Estrela_do_Sul,2012-12-31T00:00:00,2.22,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Estrela_do_Sul,2013-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Estrela_do_Sul,2014-12-31T00:00:00,1.98,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Estrela_do_Sul,2015-12-31T00:00:00,1.26,2.0,58.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Estrela_do_Sul,2016-12-31T00:00:00,2.52,2.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Estrela_do_Sul,2017-12-31T00:00:00,2.38,2.0,15.97,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Estrela_do_Sul,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Estrela_do_Sul,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Estrela_do_Sul,2020-12-31T00:00:00,2.06,2.0,2.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Estrela_do_Sul,2021-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibia,2012-12-31T00:00:00,1.74,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibia,2013-12-31T00:00:00,1.74,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibia,2014-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibia,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibia,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibia,2017-12-31T00:00:00,1.59,2.0,25.79,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibia,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibia,2020-12-31T00:00:00,1.75,2.0,14.29,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibia,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibiraci,2012-12-31T00:00:00,2.49,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibiraci,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibiraci,2014-12-31T00:00:00,2.01,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibiraci,2015-12-31T00:00:00,1.08,2.0,85.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibiraci,2016-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibiraci,2017-12-31T00:00:00,1.61,2.0,24.22,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibiraci,2018-12-31T00:00:00,3.0,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibiraci,2019-12-31T00:00:00,1.28,1.0,21.88,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibiraci,2020-12-31T00:00:00,2.63,2.0,23.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibiraci,2021-12-31T00:00:00,1.33,2.0,50.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Indianopolis,2012-12-31T00:00:00,2.4,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Indianopolis,2013-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Indianopolis,2014-12-31T00:00:00,2.45,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Indianopolis,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Indianopolis,2016-12-31T00:00:00,2.7,2.0,25.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Indianopolis,2017-12-31T00:00:00,1.67,2.0,19.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Indianopolis,2018-12-31T00:00:00,2.52,2.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Indianopolis,2019-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Indianopolis,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Indianopolis,2021-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Joao_Pinheiro,2012-12-31T00:00:00,2.82,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Joao_Pinheiro,2013-12-31T00:00:00,3.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Joao_Pinheiro,2014-12-31T00:00:00,2.7,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Joao_Pinheiro,2015-12-31T00:00:00,2.4,3.0,25.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Joao_Pinheiro,2016-12-31T00:00:00,3.6,3.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Joao_Pinheiro,2017-12-31T00:00:00,1.91,3.0,57.07,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Joao_Pinheiro,2018-12-31T00:00:00,2.4,3.0,25.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Joao_Pinheiro,2019-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Joao_Pinheiro,2020-12-31T00:00:00,3.0,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Joao_Pinheiro,2021-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Medeiros,2012-12-31T00:00:00,2.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Medeiros,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Medeiros,2014-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Medeiros,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Medeiros,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Medeiros,2017-12-31T00:00:00,1.79,2.0,11.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Medeiros,2018-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Medeiros,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Medeiros,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Medeiros,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Monte_Carmelo,2012-12-31T00:00:00,2.34,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Monte_Carmelo,2013-12-31T00:00:00,1.89,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Monte_Carmelo,2014-12-31T00:00:00,2.28,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Monte_Carmelo,2015-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Monte_Carmelo,2016-12-31T00:00:00,2.64,2.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Monte_Carmelo,2017-12-31T00:00:00,1.84,2.0,8.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Monte_Carmelo,2018-12-31T00:00:00,2.31,2.0,13.42,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Monte_Carmelo,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Monte_Carmelo,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Monte_Carmelo,2021-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ninheira,2012-12-31T00:00:00,3.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ninheira,2013-12-31T00:00:00,3.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ninheira,2014-12-31T00:00:00,3.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ninheira,2015-12-31T00:00:00,2.76,-1.0,136.23,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ninheira,2016-12-31T00:00:00,3.0,3.0,0.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ninheira,2017-12-31T00:00:00,2.34,3.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ninheira,2018-12-31T00:00:00,3.0,3.0,0.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ninheira,2019-12-31T00:00:00,2.7,3.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ninheira,2020-12-31T00:00:00,3.0,3.0,0.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ninheira,2021-12-31T00:00:00,2.1,3.0,42.86,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Paracatu,2012-12-31T00:00:00,3.45,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Paracatu,2013-12-31T00:00:00,3.45,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Paracatu,2014-12-31T00:00:00,3.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Paracatu,2015-12-31T00:00:00,1.8,3.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Paracatu,2016-12-31T00:00:00,2.4,3.0,25.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Paracatu,2017-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Paracatu,2018-12-31T00:00:00,3.0,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Paracatu,2019-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Paracatu,2020-12-31T00:00:00,2.7,3.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Paracatu,2021-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Patrocinio,2012-12-31T00:00:00,2.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Patrocinio,2013-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Patrocinio,2014-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Patrocinio,2015-12-31T00:00:00,1.25,1.0,20.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Patrocinio,2016-12-31T00:00:00,2.64,2.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Patrocinio,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Patrocinio,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Patrocinio,2019-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Patrocinio,2020-12-31T00:00:00,1.75,2.0,14.29,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Patrocinio,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Perdizes,2012-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Perdizes,2013-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Perdizes,2014-12-31T00:00:00,1.66,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Perdizes,2015-12-31T00:00:00,1.48,2.0,35.14,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Perdizes,2016-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Perdizes,2017-12-31T00:00:00,2.07,2.0,3.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Perdizes,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Perdizes,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Perdizes,2020-12-31T00:00:00,2.05,2.0,2.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Perdizes,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Pimenta,2012-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Pimenta,2013-12-31T00:00:00,1.74,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Pimenta,2014-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Pimenta,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Pimenta,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Pimenta,2017-12-31T00:00:00,1.73,2.0,15.61,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Pimenta,2018-12-31T00:00:00,2.7,2.0,25.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Pimenta,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Pimenta,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Pimenta,2021-12-31T00:00:00,1.35,2.0,48.15,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Piumhi,2012-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Piumhi,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Piumhi,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Piumhi,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Piumhi,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Piumhi,2017-12-31T00:00:00,1.36,1.0,26.47,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Piumhi,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Piumhi,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Piumhi,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Piumhi,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Presidente_Olegario,2012-12-31T00:00:00,2.66,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Presidente_Olegario,2013-12-31T00:00:00,2.65,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Presidente_Olegario,2014-12-31T00:00:00,2.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Presidente_Olegario,2015-12-31T00:00:00,1.88,2.0,6.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Presidente_Olegario,2016-12-31T00:00:00,2.52,2.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Presidente_Olegario,2017-12-31T00:00:00,1.84,2.0,8.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Presidente_Olegario,2018-12-31T00:00:00,2.28,2.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Presidente_Olegario,2019-12-31T00:00:00,1.87,2.0,6.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Presidente_Olegario,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Presidente_Olegario,2021-12-31T00:00:00,1.64,2.0,21.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Rio_Paranaiba,2012-12-31T00:00:00,2.16,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Rio_Paranaiba,2013-12-31T00:00:00,2.07,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Rio_Paranaiba,2014-12-31T00:00:00,1.89,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Rio_Paranaiba,2015-12-31T00:00:00,1.42,2.0,40.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Rio_Paranaiba,2016-12-31T00:00:00,2.08,2.0,3.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Rio_Paranaiba,2017-12-31T00:00:00,1.39,2.0,43.88,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Rio_Paranaiba,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Rio_Paranaiba,2019-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Rio_Paranaiba,2020-12-31T00:00:00,1.82,2.0,9.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Rio_Paranaiba,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Romaria,2012-12-31T00:00:00,2.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Romaria,2013-12-31T00:00:00,2.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Romaria,2014-12-31T00:00:00,2.28,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Romaria,2015-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Romaria,2016-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Romaria,2017-12-31T00:00:00,1.53,2.0,30.72,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Romaria,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Romaria,2019-12-31T00:00:00,2.2,1.0,54.55,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Romaria,2020-12-31T00:00:00,2.33,2.0,14.16,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Romaria,2021-12-31T00:00:00,2.05,2.0,2.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sacramento,2012-12-31T00:00:00,2.22,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sacramento,2013-12-31T00:00:00,1.87,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sacramento,2014-12-31T00:00:00,1.53,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sacramento,2015-12-31T00:00:00,1.34,1.0,25.37,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sacramento,2016-12-31T00:00:00,1.87,2.0,6.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sacramento,2017-12-31T00:00:00,1.31,1.0,23.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sacramento,2018-12-31T00:00:00,1.87,2.0,6.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sacramento,2019-12-31T00:00:00,1.23,1.0,18.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sacramento,2020-12-31T00:00:00,1.83,2.0,9.29,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sacramento,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Santa_Rosa_da_Serra,2012-12-31T00:00:00,3.17,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Santa_Rosa_da_Serra,2013-12-31T00:00:00,3.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Santa_Rosa_da_Serra,2014-12-31T00:00:00,2.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Santa_Rosa_da_Serra,2015-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Santa_Rosa_da_Serra,2016-12-31T00:00:00,1.5,3.0,100.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Santa_Rosa_da_Serra,2017-12-31T00:00:00,1.63,2.0,22.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Santa_Rosa_da_Serra,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Santa_Rosa_da_Serra,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Santa_Rosa_da_Serra,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Santa_Rosa_da_Serra,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sao_Gotardo,2012-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sao_Gotardo,2013-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sao_Gotardo,2014-12-31T00:00:00,1.71,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sao_Gotardo,2015-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sao_Gotardo,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sao_Gotardo,2017-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sao_Gotardo,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sao_Gotardo,2019-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sao_Gotardo,2020-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sao_Gotardo,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Serra_do_Salitre,2012-12-31T00:00:00,3.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Serra_do_Salitre,2013-12-31T00:00:00,1.74,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Serra_do_Salitre,2014-12-31T00:00:00,2.22,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Serra_do_Salitre,2015-12-31T00:00:00,1.29,1.0,22.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Serra_do_Salitre,2016-12-31T00:00:00,3.0,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Serra_do_Salitre,2017-12-31T00:00:00,1.12,2.0,78.57,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Serra_do_Salitre,2018-12-31T00:00:00,2.34,2.0,14.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Serra_do_Salitre,2019-12-31T00:00:00,1.49,1.0,32.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Serra_do_Salitre,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Serra_do_Salitre,2021-12-31T00:00:00,1.47,2.0,36.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Tiros,2012-12-31T00:00:00,2.28,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Tiros,2013-12-31T00:00:00,2.28,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Tiros,2014-12-31T00:00:00,1.79,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Tiros,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Tiros,2016-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Tiros,2017-12-31T00:00:00,3.0,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Tiros,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Tiros,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Tiros,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Tiros,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araguari,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araxa,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Bambui,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Campos_Altos,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Carmo_do_Paranaiba,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Claraval,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Coromandel,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Estrela_do_Sul,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibiraci,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Indianopolis,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Joao_Pinheiro,2022-12-31T00:00:00,2.0,3,50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Medeiros,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Monte_Carmelo,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ninheira,2022-12-31T00:00:00,3.0,3,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Paracatu,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Patrocinio,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Perdizes,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Pimenta,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Piumhi,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Presidente_Olegario,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Rio_Paranaiba,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Romaria,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sacramento,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Santa_Rosa_da_Serra,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sao_Gotardo,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Serra_do_Salitre,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Tiros,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araguari,2023-12-31T00:00:00,3.0,1.9346750974655151,-35.51,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Araxa,2023-12-31T00:00:00,1.638401296246287,1.4538747072219849,-11.26,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Bambui,2023-12-31T00:00:00,1.5,1.5991441011428833,6.61,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Campos_Altos,2023-12-31T00:00:00,1.380040526849037,1.672926664352417,21.22,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Carmo_do_Paranaiba,2023-12-31T00:00:00,2.369801007771799,1.696786880493164,-28.4,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Claraval,2023-12-31T00:00:00,1.8600214362272245,1.8951003551483154,1.89,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Coromandel,2023-12-31T00:00:00,2.0926575541308825,1.5025224685668945,-28.2,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Estrela_do_Sul,2023-12-31T00:00:00,2.43015873015873,1.8358172178268433,-24.46,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibia,2023-12-31T00:00:00,1.668,1.428138017654419,-14.38,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ibiraci,2023-12-31T00:00:00,1.804169884169884,1.9427285194396973,7.68,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Indianopolis,2023-12-31T00:00:00,2.7000000000000006,1.8551874160766602,-31.29,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Joao_Pinheiro,2023-12-31T00:00:00,3.3,2.7026796340942383,-18.1,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Medeiros,2023-12-31T00:00:00,1.86,1.6612298488616943,-10.69,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Monte_Carmelo,2023-12-31T00:00:00,2.4900284900284904,1.863702416419983,-25.15,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Ninheira,2023-12-31T00:00:00,3.0,2.756535768508911,-8.12,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Paracatu,2023-12-31T00:00:00,2.4,2.5663020610809326,6.93,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Patrocinio,2023-12-31T00:00:00,2.4370275910039414,1.510304570198059,-38.03,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Perdizes,2023-12-31T00:00:00,2.106451612903226,1.6796352863311768,-20.26,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Pimenta,2023-12-31T00:00:00,1.504186046511628,1.5539336204528809,3.31,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Piumhi,2023-12-31T00:00:00,1.7399633363886338,1.5313446521759033,-11.99,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Presidente_Olegario,2023-12-31T00:00:00,2.160074626865672,1.8839061260223389,-12.79,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Rio_Paranaiba,2023-12-31T00:00:00,1.8000000000000005,1.4102957248687744,-21.65,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Romaria,2023-12-31T00:00:00,2.131578947368421,2.1560912132263184,1.15,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sacramento,2023-12-31T00:00:00,1.75,1.6602931022644043,-5.13,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Santa_Rosa_da_Serra,2023-12-31T00:00:00,1.68,2.014496326446533,19.91,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Sao_Gotardo,2023-12-31T00:00:00,1.5,1.4142143726348877,-5.72,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Serra_do_Salitre,2023-12-31T00:00:00,1.489636363636364,1.6460192203521729,10.5,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 1 (2023),Tiros,2023-12-31T00:00:00,2.382038834951457,1.638113260269165,-31.23,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001391224982556958
encoder_hidden_size: 64
decoder_layers: 4
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 100
",2025-09-04T09:53:31
IBGE V5 Cluster 1 - Metodo 2 (2023),Araguari,2012-12-31T00:00:00,2.4,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araguari,2013-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araguari,2014-12-31T00:00:00,2.4,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araguari,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araguari,2016-12-31T00:00:00,2.58,2.0,22.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araguari,2017-12-31T00:00:00,2.76,2.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araguari,2018-12-31T00:00:00,2.64,2.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araguari,2019-12-31T00:00:00,1.68,3.0,78.57,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araguari,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araguari,2021-12-31T00:00:00,1.86,2.0,7.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araxa,2012-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araxa,2013-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araxa,2014-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araxa,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araxa,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araxa,2017-12-31T00:00:00,1.76,2.0,13.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araxa,2018-12-31T00:00:00,1.94,2.0,3.09,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araxa,2019-12-31T00:00:00,1.39,2.0,43.88,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araxa,2020-12-31T00:00:00,1.79,2.0,11.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araxa,2021-12-31T00:00:00,1.25,2.0,60.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Bambui,2012-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Bambui,2013-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Bambui,2014-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Bambui,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Bambui,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Bambui,2017-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Bambui,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Bambui,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Bambui,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Bambui,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Campos_Altos,2012-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Campos_Altos,2013-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Campos_Altos,2014-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Campos_Altos,2015-12-31T00:00:00,1.71,2.0,16.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Campos_Altos,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Campos_Altos,2017-12-31T00:00:00,1.99,2.0,0.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Campos_Altos,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Campos_Altos,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Campos_Altos,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Campos_Altos,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Carmo_do_Paranaiba,2012-12-31T00:00:00,2.11,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Carmo_do_Paranaiba,2013-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Carmo_do_Paranaiba,2014-12-31T00:00:00,1.93,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Carmo_do_Paranaiba,2015-12-31T00:00:00,1.89,2.0,5.82,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Carmo_do_Paranaiba,2016-12-31T00:00:00,2.37,2.0,15.61,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Carmo_do_Paranaiba,2017-12-31T00:00:00,1.75,2.0,14.29,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Carmo_do_Paranaiba,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Carmo_do_Paranaiba,2019-12-31T00:00:00,1.79,2.0,11.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Carmo_do_Paranaiba,2020-12-31T00:00:00,1.95,2.0,2.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Carmo_do_Paranaiba,2021-12-31T00:00:00,1.54,2.0,29.87,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Claraval,2012-12-31T00:00:00,2.55,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Claraval,2013-12-31T00:00:00,1.42,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Claraval,2014-12-31T00:00:00,2.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Claraval,2015-12-31T00:00:00,1.02,2.0,96.08,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Claraval,2016-12-31T00:00:00,2.34,2.0,14.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Claraval,2017-12-31T00:00:00,1.22,1.0,18.03,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Claraval,2018-12-31T00:00:00,2.55,2.0,21.57,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Claraval,2019-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Claraval,2020-12-31T00:00:00,2.8,2.0,28.57,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Claraval,2021-12-31T00:00:00,1.05,2.0,90.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Coromandel,2012-12-31T00:00:00,1.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Coromandel,2013-12-31T00:00:00,2.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Coromandel,2014-12-31T00:00:00,1.91,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Coromandel,2015-12-31T00:00:00,1.91,2.0,4.71,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Coromandel,2016-12-31T00:00:00,2.09,2.0,4.31,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Coromandel,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Coromandel,2018-12-31T00:00:00,2.65,2.0,24.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Coromandel,2019-12-31T00:00:00,1.65,2.0,21.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Coromandel,2020-12-31T00:00:00,1.82,2.0,9.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Coromandel,2021-12-31T00:00:00,1.25,2.0,60.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Estrela_do_Sul,2012-12-31T00:00:00,2.22,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Estrela_do_Sul,2013-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Estrela_do_Sul,2014-12-31T00:00:00,1.98,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Estrela_do_Sul,2015-12-31T00:00:00,1.26,2.0,58.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Estrela_do_Sul,2016-12-31T00:00:00,2.52,2.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Estrela_do_Sul,2017-12-31T00:00:00,2.38,2.0,15.97,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Estrela_do_Sul,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Estrela_do_Sul,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Estrela_do_Sul,2020-12-31T00:00:00,2.06,2.0,2.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Estrela_do_Sul,2021-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibia,2012-12-31T00:00:00,1.74,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibia,2013-12-31T00:00:00,1.74,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibia,2014-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibia,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibia,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibia,2017-12-31T00:00:00,1.59,2.0,25.79,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibia,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibia,2020-12-31T00:00:00,1.75,1.0,42.86,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibia,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibiraci,2012-12-31T00:00:00,2.49,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibiraci,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibiraci,2014-12-31T00:00:00,2.01,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibiraci,2015-12-31T00:00:00,1.08,2.0,85.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibiraci,2016-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibiraci,2017-12-31T00:00:00,1.61,1.0,37.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibiraci,2018-12-31T00:00:00,3.0,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibiraci,2019-12-31T00:00:00,1.28,2.0,56.25,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibiraci,2020-12-31T00:00:00,2.63,2.0,23.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibiraci,2021-12-31T00:00:00,1.33,2.0,50.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Indianopolis,2012-12-31T00:00:00,2.4,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Indianopolis,2013-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Indianopolis,2014-12-31T00:00:00,2.45,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Indianopolis,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Indianopolis,2016-12-31T00:00:00,2.7,2.0,25.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Indianopolis,2017-12-31T00:00:00,1.67,2.0,19.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Indianopolis,2018-12-31T00:00:00,2.52,2.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Indianopolis,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Indianopolis,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Indianopolis,2021-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Joao_Pinheiro,2012-12-31T00:00:00,2.82,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Joao_Pinheiro,2013-12-31T00:00:00,3.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Joao_Pinheiro,2014-12-31T00:00:00,2.7,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Joao_Pinheiro,2015-12-31T00:00:00,2.4,3.0,25.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Joao_Pinheiro,2016-12-31T00:00:00,3.6,3.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Joao_Pinheiro,2017-12-31T00:00:00,1.91,3.0,57.07,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Joao_Pinheiro,2018-12-31T00:00:00,2.4,3.0,25.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Joao_Pinheiro,2019-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Joao_Pinheiro,2020-12-31T00:00:00,3.0,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Joao_Pinheiro,2021-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Medeiros,2012-12-31T00:00:00,2.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Medeiros,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Medeiros,2014-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Medeiros,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Medeiros,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Medeiros,2017-12-31T00:00:00,1.79,2.0,11.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Medeiros,2018-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Medeiros,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Medeiros,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Medeiros,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Monte_Carmelo,2012-12-31T00:00:00,2.34,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Monte_Carmelo,2013-12-31T00:00:00,1.89,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Monte_Carmelo,2014-12-31T00:00:00,2.28,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Monte_Carmelo,2015-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Monte_Carmelo,2016-12-31T00:00:00,2.64,2.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Monte_Carmelo,2017-12-31T00:00:00,1.84,2.0,8.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Monte_Carmelo,2018-12-31T00:00:00,2.31,2.0,13.42,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Monte_Carmelo,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Monte_Carmelo,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Monte_Carmelo,2021-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ninheira,2012-12-31T00:00:00,3.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ninheira,2013-12-31T00:00:00,3.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ninheira,2014-12-31T00:00:00,3.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ninheira,2015-12-31T00:00:00,2.76,-0.0,100.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ninheira,2016-12-31T00:00:00,3.0,3.0,0.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ninheira,2017-12-31T00:00:00,2.34,3.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ninheira,2018-12-31T00:00:00,3.0,3.0,0.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ninheira,2019-12-31T00:00:00,2.7,3.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ninheira,2020-12-31T00:00:00,3.0,3.0,0.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ninheira,2021-12-31T00:00:00,2.1,3.0,42.86,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Paracatu,2012-12-31T00:00:00,3.45,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Paracatu,2013-12-31T00:00:00,3.45,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Paracatu,2014-12-31T00:00:00,3.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Paracatu,2015-12-31T00:00:00,1.8,3.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Paracatu,2016-12-31T00:00:00,2.4,3.0,25.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Paracatu,2017-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Paracatu,2018-12-31T00:00:00,3.0,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Paracatu,2019-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Paracatu,2020-12-31T00:00:00,2.7,3.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Paracatu,2021-12-31T00:00:00,2.4,3.0,25.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Patrocinio,2012-12-31T00:00:00,2.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Patrocinio,2013-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Patrocinio,2014-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Patrocinio,2015-12-31T00:00:00,1.25,2.0,60.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Patrocinio,2016-12-31T00:00:00,2.64,2.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Patrocinio,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Patrocinio,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Patrocinio,2019-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Patrocinio,2020-12-31T00:00:00,1.75,1.0,42.86,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Patrocinio,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Perdizes,2012-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Perdizes,2013-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Perdizes,2014-12-31T00:00:00,1.66,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Perdizes,2015-12-31T00:00:00,1.48,2.0,35.14,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Perdizes,2016-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Perdizes,2017-12-31T00:00:00,2.07,2.0,3.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Perdizes,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Perdizes,2019-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Perdizes,2020-12-31T00:00:00,2.05,2.0,2.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Perdizes,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Pimenta,2012-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Pimenta,2013-12-31T00:00:00,1.74,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Pimenta,2014-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Pimenta,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Pimenta,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Pimenta,2017-12-31T00:00:00,1.73,2.0,15.61,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Pimenta,2018-12-31T00:00:00,2.7,2.0,25.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Pimenta,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Pimenta,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Pimenta,2021-12-31T00:00:00,1.35,2.0,48.15,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Piumhi,2012-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Piumhi,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Piumhi,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Piumhi,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Piumhi,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Piumhi,2017-12-31T00:00:00,1.36,1.0,26.47,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Piumhi,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Piumhi,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Piumhi,2020-12-31T00:00:00,1.74,1.0,42.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Piumhi,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Presidente_Olegario,2012-12-31T00:00:00,2.66,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Presidente_Olegario,2013-12-31T00:00:00,2.65,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Presidente_Olegario,2014-12-31T00:00:00,2.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Presidente_Olegario,2015-12-31T00:00:00,1.88,2.0,6.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Presidente_Olegario,2016-12-31T00:00:00,2.52,2.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Presidente_Olegario,2017-12-31T00:00:00,1.84,2.0,8.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Presidente_Olegario,2018-12-31T00:00:00,2.28,2.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Presidente_Olegario,2019-12-31T00:00:00,1.87,2.0,6.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Presidente_Olegario,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Presidente_Olegario,2021-12-31T00:00:00,1.64,2.0,21.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Rio_Paranaiba,2012-12-31T00:00:00,2.16,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Rio_Paranaiba,2013-12-31T00:00:00,2.07,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Rio_Paranaiba,2014-12-31T00:00:00,1.89,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Rio_Paranaiba,2015-12-31T00:00:00,1.42,2.0,40.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Rio_Paranaiba,2016-12-31T00:00:00,2.08,2.0,3.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Rio_Paranaiba,2017-12-31T00:00:00,1.39,2.0,43.88,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Rio_Paranaiba,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Rio_Paranaiba,2019-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Rio_Paranaiba,2020-12-31T00:00:00,1.82,1.0,45.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Rio_Paranaiba,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Romaria,2012-12-31T00:00:00,2.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Romaria,2013-12-31T00:00:00,2.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Romaria,2014-12-31T00:00:00,2.28,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Romaria,2015-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Romaria,2016-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Romaria,2017-12-31T00:00:00,1.53,2.0,30.72,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Romaria,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Romaria,2019-12-31T00:00:00,2.2,2.0,9.09,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Romaria,2020-12-31T00:00:00,2.33,2.0,14.16,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Romaria,2021-12-31T00:00:00,2.05,2.0,2.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sacramento,2012-12-31T00:00:00,2.22,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sacramento,2013-12-31T00:00:00,1.87,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sacramento,2014-12-31T00:00:00,1.53,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sacramento,2015-12-31T00:00:00,1.34,2.0,49.25,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sacramento,2016-12-31T00:00:00,1.87,2.0,6.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sacramento,2017-12-31T00:00:00,1.31,1.0,23.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sacramento,2018-12-31T00:00:00,1.87,2.0,6.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sacramento,2019-12-31T00:00:00,1.23,1.0,18.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sacramento,2020-12-31T00:00:00,1.83,1.0,45.36,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sacramento,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Santa_Rosa_da_Serra,2012-12-31T00:00:00,3.17,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Santa_Rosa_da_Serra,2013-12-31T00:00:00,3.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Santa_Rosa_da_Serra,2014-12-31T00:00:00,2.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Santa_Rosa_da_Serra,2015-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Santa_Rosa_da_Serra,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Santa_Rosa_da_Serra,2017-12-31T00:00:00,1.63,2.0,22.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Santa_Rosa_da_Serra,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Santa_Rosa_da_Serra,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Santa_Rosa_da_Serra,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Santa_Rosa_da_Serra,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sao_Gotardo,2012-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sao_Gotardo,2013-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sao_Gotardo,2014-12-31T00:00:00,1.71,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sao_Gotardo,2015-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sao_Gotardo,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sao_Gotardo,2017-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sao_Gotardo,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sao_Gotardo,2019-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sao_Gotardo,2020-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sao_Gotardo,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Serra_do_Salitre,2012-12-31T00:00:00,3.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Serra_do_Salitre,2013-12-31T00:00:00,1.74,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Serra_do_Salitre,2014-12-31T00:00:00,2.22,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Serra_do_Salitre,2015-12-31T00:00:00,1.29,2.0,55.04,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Serra_do_Salitre,2016-12-31T00:00:00,3.0,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Serra_do_Salitre,2017-12-31T00:00:00,1.12,2.0,78.57,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Serra_do_Salitre,2018-12-31T00:00:00,2.34,2.0,14.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Serra_do_Salitre,2019-12-31T00:00:00,1.49,0.0,100.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Serra_do_Salitre,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Serra_do_Salitre,2021-12-31T00:00:00,1.47,2.0,36.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Tiros,2012-12-31T00:00:00,2.28,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Tiros,2013-12-31T00:00:00,2.28,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Tiros,2014-12-31T00:00:00,1.79,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Tiros,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Tiros,2016-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Tiros,2017-12-31T00:00:00,3.0,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Tiros,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Tiros,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Tiros,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Tiros,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araguari,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araxa,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Bambui,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Campos_Altos,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Carmo_do_Paranaiba,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Claraval,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Coromandel,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Estrela_do_Sul,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibiraci,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Indianopolis,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Joao_Pinheiro,2022-12-31T00:00:00,2.0,3,50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Medeiros,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Monte_Carmelo,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ninheira,2022-12-31T00:00:00,3.0,3,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Paracatu,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Patrocinio,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Perdizes,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Pimenta,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Piumhi,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Presidente_Olegario,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Rio_Paranaiba,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Romaria,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sacramento,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Santa_Rosa_da_Serra,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sao_Gotardo,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Serra_do_Salitre,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Tiros,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araguari,2023-12-31T00:00:00,3.0,1.9476608037948608,-35.08,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Araxa,2023-12-31T00:00:00,1.638401296246287,1.4287636280059814,-12.8,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Bambui,2023-12-31T00:00:00,1.5,1.6734631061553955,11.56,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Campos_Altos,2023-12-31T00:00:00,1.380040526849037,1.7915141582489014,29.82,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Carmo_do_Paranaiba,2023-12-31T00:00:00,2.369801007771799,1.7330936193466187,-26.87,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Claraval,2023-12-31T00:00:00,1.8600214362272245,1.5802462100982666,-15.04,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Coromandel,2023-12-31T00:00:00,2.0926575541308825,1.5464379787445068,-26.1,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Estrela_do_Sul,2023-12-31T00:00:00,2.43015873015873,1.796090841293335,-26.09,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibia,2023-12-31T00:00:00,1.668,1.4222307205200195,-14.73,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ibiraci,2023-12-31T00:00:00,1.804169884169884,1.666304588317871,-7.64,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Indianopolis,2023-12-31T00:00:00,2.7000000000000006,1.8522274494171143,-31.4,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Joao_Pinheiro,2023-12-31T00:00:00,3.3,2.72167706489563,-17.52,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Medeiros,2023-12-31T00:00:00,1.86,1.7369062900543213,-6.62,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Monte_Carmelo,2023-12-31T00:00:00,2.4900284900284904,1.877274513244629,-24.61,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Ninheira,2023-12-31T00:00:00,3.0,2.785369873046875,-7.15,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Paracatu,2023-12-31T00:00:00,2.4,2.5665571689605713,6.94,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Patrocinio,2023-12-31T00:00:00,2.4370275910039414,1.5962402820587158,-34.5,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Perdizes,2023-12-31T00:00:00,2.106451612903226,1.6684110164642334,-20.8,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Pimenta,2023-12-31T00:00:00,1.504186046511628,1.5910345315933228,5.77,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Piumhi,2023-12-31T00:00:00,1.7399633363886338,1.5972024202346802,-8.2,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Presidente_Olegario,2023-12-31T00:00:00,2.160074626865672,1.9132746458053589,-11.43,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Rio_Paranaiba,2023-12-31T00:00:00,1.8000000000000005,1.4715137481689453,-18.25,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Romaria,2023-12-31T00:00:00,2.131578947368421,2.1532955169677734,1.02,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sacramento,2023-12-31T00:00:00,1.75,1.6186814308166504,-7.5,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Santa_Rosa_da_Serra,2023-12-31T00:00:00,1.68,2.0891242027282715,24.35,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Sao_Gotardo,2023-12-31T00:00:00,1.5,1.4239403009414673,-5.07,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Serra_do_Salitre,2023-12-31T00:00:00,1.489636363636364,1.6820087432861328,12.91,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 1 - Metodo 2 (2023),Tiros,2023-12-31T00:00:00,2.382038834951457,1.7187840938568115,-27.84,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 1.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 7
learning_rate: 0.00010365891412087827
encoder_hidden_size: 192
decoder_layers: 4
decoder_hidden_size: 192
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:54:50
IBGE V5 Cluster 2 - Metodo 1 (2023),Alfenas,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alfenas,2013-12-31T00:00:00,1.98,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alfenas,2014-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alfenas,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alfenas,2016-12-31T00:00:00,2.25,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alfenas,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alfenas,2018-12-31T00:00:00,2.34,2.0,14.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alfenas,2019-12-31T00:00:00,1.76,2.0,13.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alfenas,2020-12-31T00:00:00,2.36,2.0,15.25,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alfenas,2021-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alterosa,2012-12-31T00:00:00,1.59,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alterosa,2013-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alterosa,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alterosa,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alterosa,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alterosa,2017-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alterosa,2018-12-31T00:00:00,1.96,2.0,2.04,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alterosa,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alterosa,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alterosa,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Andradas,2012-12-31T00:00:00,1.98,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Andradas,2013-12-31T00:00:00,1.59,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Andradas,2014-12-31T00:00:00,1.21,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Andradas,2015-12-31T00:00:00,1.89,2.0,5.82,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Andradas,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Andradas,2017-12-31T00:00:00,1.34,1.0,25.37,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Andradas,2018-12-31T00:00:00,1.73,2.0,15.61,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Andradas,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Andradas,2020-12-31T00:00:00,2.57,3.0,16.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Andradas,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Boa_Esperanca,2012-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Boa_Esperanca,2013-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Boa_Esperanca,2014-12-31T00:00:00,1.15,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Boa_Esperanca,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Boa_Esperanca,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Boa_Esperanca,2017-12-31T00:00:00,2.53,2.0,20.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Boa_Esperanca,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Boa_Esperanca,2019-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Boa_Esperanca,2020-12-31T00:00:00,2.06,2.0,2.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Boa_Esperanca,2021-12-31T00:00:00,1.57,2.0,27.39,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bom_Sucesso,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bom_Sucesso,2013-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bom_Sucesso,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bom_Sucesso,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bom_Sucesso,2016-12-31T00:00:00,1.92,1.0,47.92,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bom_Sucesso,2017-12-31T00:00:00,1.48,1.0,32.43,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bom_Sucesso,2018-12-31T00:00:00,1.7,2.0,17.65,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bom_Sucesso,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bom_Sucesso,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bom_Sucesso,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bueno_Brandao,2012-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bueno_Brandao,2013-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bueno_Brandao,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bueno_Brandao,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bueno_Brandao,2016-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bueno_Brandao,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bueno_Brandao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bueno_Brandao,2019-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bueno_Brandao,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bueno_Brandao,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cambuquira,2012-12-31T00:00:00,1.15,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cambuquira,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cambuquira,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cambuquira,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cambuquira,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cambuquira,2017-12-31T00:00:00,1.82,2.0,9.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cambuquira,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cambuquira,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cambuquira,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cambuquira,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campanha,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campanha,2013-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campanha,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campanha,2015-12-31T00:00:00,1.02,1.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campanha,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campanha,2017-12-31T00:00:00,1.88,2.0,6.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campanha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campanha,2019-12-31T00:00:00,1.51,1.0,33.77,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campanha,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campanha,2021-12-31T00:00:00,1.33,1.0,24.81,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_Belo,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_Belo,2013-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_Belo,2014-12-31T00:00:00,0.63,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_Belo,2015-12-31T00:00:00,0.84,1.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_Belo,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_Belo,2017-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_Belo,2018-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_Belo,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_Belo,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_Belo,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_do_Meio,2012-12-31T00:00:00,1.17,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_do_Meio,2013-12-31T00:00:00,1.17,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_do_Meio,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_do_Meio,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_do_Meio,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_do_Meio,2017-12-31T00:00:00,2.08,2.0,3.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_do_Meio,2018-12-31T00:00:00,1.82,2.0,9.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_do_Meio,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_do_Meio,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_do_Meio,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campos_Gerais,2012-12-31T00:00:00,1.65,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campos_Gerais,2013-12-31T00:00:00,1.22,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campos_Gerais,2014-12-31T00:00:00,1.15,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campos_Gerais,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campos_Gerais,2016-12-31T00:00:00,2.5,2.0,20.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campos_Gerais,2017-12-31T00:00:00,1.81,3.0,65.75,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campos_Gerais,2018-12-31T00:00:00,2.21,2.0,9.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campos_Gerais,2019-12-31T00:00:00,1.59,2.0,25.79,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campos_Gerais,2020-12-31T00:00:00,2.69,3.0,11.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campos_Gerais,2021-12-31T00:00:00,1.59,1.0,37.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Candeias,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Candeias,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Candeias,2014-12-31T00:00:00,0.99,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Candeias,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Candeias,2016-12-31T00:00:00,2.28,2.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Candeias,2017-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Candeias,2018-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Candeias,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Candeias,2020-12-31T00:00:00,1.86,2.0,7.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Candeias,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capetinga,2012-12-31T00:00:00,1.81,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capetinga,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capetinga,2014-12-31T00:00:00,1.59,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capetinga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capetinga,2016-12-31T00:00:00,1.93,2.0,3.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capetinga,2017-12-31T00:00:00,2.09,2.0,4.31,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capetinga,2018-12-31T00:00:00,2.7,3.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capetinga,2019-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capetinga,2020-12-31T00:00:00,2.21,2.0,9.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capetinga,2021-12-31T00:00:00,1.64,2.0,21.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capitolio,2012-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capitolio,2013-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capitolio,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capitolio,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capitolio,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capitolio,2017-12-31T00:00:00,1.47,1.0,31.97,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capitolio,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capitolio,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capitolio,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capitolio,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_da_Cachoeira,2012-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_da_Cachoeira,2013-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_da_Cachoeira,2014-12-31T00:00:00,1.15,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_da_Cachoeira,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_da_Cachoeira,2016-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_da_Cachoeira,2017-12-31T00:00:00,1.73,2.0,15.61,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_da_Cachoeira,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_da_Cachoeira,2019-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_da_Cachoeira,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_da_Cachoeira,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_de_Minas,2012-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_de_Minas,2013-12-31T00:00:00,1.74,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_de_Minas,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_de_Minas,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_de_Minas,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_de_Minas,2017-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_de_Minas,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_de_Minas,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_de_Minas,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_de_Minas,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_do_Rio_Claro,2012-12-31T00:00:00,1.66,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_do_Rio_Claro,2013-12-31T00:00:00,1.94,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_do_Rio_Claro,2014-12-31T00:00:00,1.65,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_do_Rio_Claro,2015-12-31T00:00:00,1.53,2.0,30.72,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_do_Rio_Claro,2016-12-31T00:00:00,2.37,2.0,15.61,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_do_Rio_Claro,2017-12-31T00:00:00,1.82,2.0,9.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_do_Rio_Claro,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_do_Rio_Claro,2019-12-31T00:00:00,1.94,2.0,3.09,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_do_Rio_Claro,2020-12-31T00:00:00,2.78,3.0,7.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_do_Rio_Claro,2021-12-31T00:00:00,1.64,2.0,21.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cassia,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cassia,2013-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cassia,2014-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cassia,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cassia,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cassia,2017-12-31T00:00:00,1.6,2.0,25.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cassia,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cassia,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cassia,2020-12-31T00:00:00,2.17,2.0,7.83,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cassia,2021-12-31T00:00:00,1.31,1.0,23.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2015-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2017-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Coqueiral,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Coqueiral,2013-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Coqueiral,2014-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Coqueiral,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Coqueiral,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Coqueiral,2017-12-31T00:00:00,1.75,2.0,14.29,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Coqueiral,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Coqueiral,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Coqueiral,2020-12-31T00:00:00,2.16,2.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Coqueiral,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cristais,2012-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cristais,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cristais,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cristais,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cristais,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cristais,2017-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cristais,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cristais,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cristais,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cristais,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Divisa_Nova,2012-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Divisa_Nova,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Divisa_Nova,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Divisa_Nova,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Divisa_Nova,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Divisa_Nova,2017-12-31T00:00:00,2.28,2.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Divisa_Nova,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Divisa_Nova,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Divisa_Nova,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Divisa_Nova,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Eloi_Mendes,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Eloi_Mendes,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Eloi_Mendes,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Eloi_Mendes,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Eloi_Mendes,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Eloi_Mendes,2017-12-31T00:00:00,1.83,2.0,9.29,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Eloi_Mendes,2018-12-31T00:00:00,1.72,2.0,16.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Eloi_Mendes,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Eloi_Mendes,2020-12-31T00:00:00,2.11,2.0,5.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Eloi_Mendes,2021-12-31T00:00:00,1.04,1.0,3.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Guape,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Guape,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Guape,2014-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Guape,2015-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Guape,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Guape,2017-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Guape,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Guape,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Guape,2020-12-31T00:00:00,1.85,2.0,8.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Guape,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Heliodora,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Heliodora,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Heliodora,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Heliodora,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Heliodora,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Heliodora,2017-12-31T00:00:00,1.57,1.0,36.31,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Heliodora,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Heliodora,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Heliodora,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Heliodora,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ilicinea,2012-12-31T00:00:00,1.69,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ilicinea,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ilicinea,2014-12-31T00:00:00,1.35,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ilicinea,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ilicinea,2016-12-31T00:00:00,2.46,2.0,18.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ilicinea,2017-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ilicinea,2018-12-31T00:00:00,2.46,2.0,18.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ilicinea,2019-12-31T00:00:00,1.86,2.0,7.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ilicinea,2020-12-31T00:00:00,2.1,3.0,42.86,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ilicinea,2021-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Inconfidentes,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Inconfidentes,2013-12-31T00:00:00,1.4,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Inconfidentes,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Inconfidentes,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Inconfidentes,2016-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Inconfidentes,2017-12-31T00:00:00,1.3,1.0,23.08,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Inconfidentes,2018-12-31T00:00:00,2.88,3.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Inconfidentes,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Inconfidentes,2020-12-31T00:00:00,2.46,2.0,18.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Inconfidentes,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Itamogi,2012-12-31T00:00:00,1.95,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Itamogi,2013-12-31T00:00:00,1.17,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Itamogi,2014-12-31T00:00:00,1.6,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Itamogi,2015-12-31T00:00:00,1.58,1.0,36.71,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Itamogi,2016-12-31T00:00:00,1.95,2.0,2.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Itamogi,2017-12-31T00:00:00,1.42,1.0,29.58,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Itamogi,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Itamogi,2019-12-31T00:00:00,1.85,2.0,8.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Itamogi,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Itamogi,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Jacutinga,2012-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Jacutinga,2013-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Jacutinga,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Jacutinga,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Jacutinga,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Jacutinga,2017-12-31T00:00:00,1.37,1.0,27.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Jacutinga,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Jacutinga,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Jacutinga,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Jacutinga,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lambari,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lambari,2013-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lambari,2014-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lambari,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lambari,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lambari,2017-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lambari,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lambari,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lambari,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lambari,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lavras,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lavras,2013-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lavras,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lavras,2015-12-31T00:00:00,0.84,1.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lavras,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lavras,2017-12-31T00:00:00,2.25,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lavras,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lavras,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lavras,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lavras,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Machado,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Machado,2013-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Machado,2014-12-31T00:00:00,1.15,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Machado,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Machado,2016-12-31T00:00:00,1.86,2.0,7.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Machado,2017-12-31T00:00:00,1.51,1.0,33.77,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Machado,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Machado,2019-12-31T00:00:00,1.53,2.0,30.72,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Machado,2020-12-31T00:00:00,2.21,2.0,9.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Machado,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monsenhor_Paulo,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monsenhor_Paulo,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monsenhor_Paulo,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monsenhor_Paulo,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monsenhor_Paulo,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monsenhor_Paulo,2017-12-31T00:00:00,1.72,2.0,16.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monsenhor_Paulo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monsenhor_Paulo,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monsenhor_Paulo,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monsenhor_Paulo,2021-12-31T00:00:00,1.41,1.0,29.08,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Belo,2012-12-31T00:00:00,1.63,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Belo,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Belo,2014-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Belo,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Belo,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Belo,2017-12-31T00:00:00,2.43,2.0,17.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Belo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Belo,2019-12-31T00:00:00,1.64,2.0,21.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Belo,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Belo,2021-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Santo_de_Minas,2012-12-31T00:00:00,1.89,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Santo_de_Minas,2013-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Santo_de_Minas,2014-12-31T00:00:00,1.6,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Santo_de_Minas,2015-12-31T00:00:00,1.35,1.0,25.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Santo_de_Minas,2016-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Santo_de_Minas,2017-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Santo_de_Minas,2018-12-31T00:00:00,1.88,2.0,6.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Santo_de_Minas,2019-12-31T00:00:00,1.59,2.0,25.79,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Santo_de_Minas,2020-12-31T00:00:00,2.12,2.0,5.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Santo_de_Minas,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Siao,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Siao,2013-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Siao,2014-12-31T00:00:00,0.66,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Siao,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Siao,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Siao,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Siao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Siao,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Siao,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Siao,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Muzambinho,2012-12-31T00:00:00,1.74,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Muzambinho,2013-12-31T00:00:00,1.35,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Muzambinho,2014-12-31T00:00:00,1.35,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Muzambinho,2015-12-31T00:00:00,1.48,1.0,32.43,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Muzambinho,2016-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Muzambinho,2017-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Muzambinho,2018-12-31T00:00:00,1.3,2.0,53.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Muzambinho,2019-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Muzambinho,2020-12-31T00:00:00,1.76,2.0,13.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Muzambinho,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Natercia,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Natercia,2013-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Natercia,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Natercia,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Natercia,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Natercia,2017-12-31T00:00:00,1.6,2.0,25.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Natercia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Natercia,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Natercia,2020-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Natercia,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nazareno,2012-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nazareno,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nazareno,2014-12-31T00:00:00,1.3,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nazareno,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nazareno,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nazareno,2017-12-31T00:00:00,1.59,2.0,25.79,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nazareno,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nazareno,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nazareno,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nazareno,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nepomuceno,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nepomuceno,2013-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nepomuceno,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nepomuceno,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nepomuceno,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nepomuceno,2017-12-31T00:00:00,1.69,2.0,18.34,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nepomuceno,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nepomuceno,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nepomuceno,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nepomuceno,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Oliveira,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Oliveira,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Oliveira,2014-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Oliveira,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Oliveira,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Oliveira,2017-12-31T00:00:00,1.4,1.0,28.57,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Oliveira,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Oliveira,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Oliveira,2020-12-31T00:00:00,1.95,2.0,2.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Oliveira,2021-12-31T00:00:00,1.54,2.0,29.87,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ouro_Fino,2012-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ouro_Fino,2013-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ouro_Fino,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ouro_Fino,2015-12-31T00:00:00,1.29,1.0,22.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ouro_Fino,2016-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ouro_Fino,2017-12-31T00:00:00,1.42,1.0,29.58,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ouro_Fino,2018-12-31T00:00:00,1.69,2.0,18.34,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ouro_Fino,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ouro_Fino,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ouro_Fino,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Paraguacu,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Paraguacu,2013-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Paraguacu,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Paraguacu,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Paraguacu,2016-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Paraguacu,2017-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Paraguacu,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Paraguacu,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Paraguacu,2020-12-31T00:00:00,2.34,2.0,14.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Paraguacu,2021-12-31T00:00:00,0.97,1.0,3.09,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pedralva,2012-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pedralva,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pedralva,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pedralva,2015-12-31T00:00:00,0.96,2.0,108.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pedralva,2016-12-31T00:00:00,2.28,2.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pedralva,2017-12-31T00:00:00,1.43,1.0,30.07,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pedralva,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pedralva,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pedralva,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pedralva,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Perdoes,2012-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Perdoes,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Perdoes,2014-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Perdoes,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Perdoes,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Perdoes,2017-12-31T00:00:00,1.84,2.0,8.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Perdoes,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Perdoes,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Perdoes,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Perdoes,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Poco_Fundo,2012-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Poco_Fundo,2013-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Poco_Fundo,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Poco_Fundo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Poco_Fundo,2016-12-31T00:00:00,1.41,1.0,29.08,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Poco_Fundo,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Poco_Fundo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Poco_Fundo,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Poco_Fundo,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Poco_Fundo,2021-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pocos_de_Caldas,2012-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pocos_de_Caldas,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pocos_de_Caldas,2014-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pocos_de_Caldas,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pocos_de_Caldas,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pocos_de_Caldas,2017-12-31T00:00:00,1.77,2.0,12.99,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pocos_de_Caldas,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pocos_de_Caldas,2019-12-31T00:00:00,1.08,2.0,85.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pocos_de_Caldas,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pocos_de_Caldas,2021-12-31T00:00:00,1.42,1.0,29.58,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2017-12-31T00:00:00,1.79,2.0,11.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santana_da_Vargem,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santana_da_Vargem,2013-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santana_da_Vargem,2014-12-31T00:00:00,1.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santana_da_Vargem,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santana_da_Vargem,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santana_da_Vargem,2017-12-31T00:00:00,2.09,2.0,4.31,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santana_da_Vargem,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santana_da_Vargem,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santana_da_Vargem,2020-12-31T00:00:00,2.22,2.0,9.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santana_da_Vargem,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2012-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2013-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2014-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2017-12-31T00:00:00,1.34,1.0,25.37,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2018-12-31T00:00:00,2.16,2.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2019-12-31T00:00:00,1.64,2.0,21.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2012-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2013-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2017-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2021-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2012-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2013-12-31T00:00:00,1.71,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2014-12-31T00:00:00,1.42,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2016-12-31T00:00:00,1.7,2.0,17.65,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2017-12-31T00:00:00,1.54,1.0,35.06,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2018-12-31T00:00:00,1.89,2.0,5.82,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2019-12-31T00:00:00,1.47,2.0,36.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2020-12-31T00:00:00,1.93,2.0,3.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2021-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2012-12-31T00:00:00,1.75,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2013-12-31T00:00:00,1.05,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2014-12-31T00:00:00,1.35,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2017-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Serrania,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Serrania,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Serrania,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Serrania,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Serrania,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Serrania,2017-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Serrania,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Serrania,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Serrania,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Serrania,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Coracoes,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Coracoes,2013-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Coracoes,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Coracoes,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Coracoes,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Coracoes,2017-12-31T00:00:00,2.67,3.0,12.36,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Coracoes,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Coracoes,2019-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Coracoes,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Coracoes,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Pontas,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Pontas,2013-12-31T00:00:00,1.53,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Pontas,2014-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Pontas,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Pontas,2016-12-31T00:00:00,1.86,2.0,7.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Pontas,2017-12-31T00:00:00,1.88,2.0,6.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Pontas,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Pontas,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Pontas,2020-12-31T00:00:00,2.26,2.0,11.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Pontas,2021-12-31T00:00:00,1.35,1.0,25.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Turvolandia,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Turvolandia,2013-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Turvolandia,2014-12-31T00:00:00,0.84,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Turvolandia,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Turvolandia,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Turvolandia,2017-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Turvolandia,2018-12-31T00:00:00,2.29,2.0,12.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Turvolandia,2019-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Turvolandia,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Turvolandia,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Varginha,2012-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Varginha,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Varginha,2014-12-31T00:00:00,1.03,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Varginha,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Varginha,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Varginha,2017-12-31T00:00:00,2.12,2.0,5.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Varginha,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Varginha,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Varginha,2020-12-31T00:00:00,1.9,2.0,5.26,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Varginha,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alfenas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alterosa,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Andradas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Boa_Esperanca,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bom_Sucesso,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bueno_Brandao,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cambuquira,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campanha,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_Belo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_do_Meio,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campos_Gerais,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Candeias,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capetinga,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capitolio,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_da_Cachoeira,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_do_Rio_Claro,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cassia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Coqueiral,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cristais,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Divisa_Nova,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Eloi_Mendes,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Guape,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Heliodora,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ilicinea,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Inconfidentes,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Itamogi,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Jacutinga,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lambari,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lavras,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Machado,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monsenhor_Paulo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Belo,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Santo_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Siao,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Muzambinho,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Natercia,2022-12-31T00:00:00,1.0,0,-100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nazareno,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nepomuceno,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Oliveira,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ouro_Fino,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Paraguacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pedralva,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Perdoes,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Poco_Fundo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pocos_de_Caldas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santana_da_Vargem,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Serrania,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Coracoes,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Pontas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Turvolandia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Varginha,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alfenas,2023-12-31T00:00:00,1.698430922311519,1.7865428924560547,5.19,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Alterosa,2023-12-31T00:00:00,1.2603960396039595,1.3356701135635376,5.97,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Andradas,2023-12-31T00:00:00,1.22,1.4710538387298584,20.58,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Boa_Esperanca,2023-12-31T00:00:00,1.08,1.6280593872070312,50.75,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bom_Sucesso,2023-12-31T00:00:00,1.5,1.4412705898284912,-3.92,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Bueno_Brandao,2023-12-31T00:00:00,1.379901960784314,1.4703283309936523,6.55,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cambuquira,2023-12-31T00:00:00,1.56,1.6135197877883911,3.43,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campanha,2023-12-31T00:00:00,1.327075098814229,1.6092973947525024,21.27,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_Belo,2023-12-31T00:00:00,1.5,1.5113494396209717,0.76,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campo_do_Meio,2023-12-31T00:00:00,0.8771626297577856,2.0563645362854004,134.43,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Campos_Gerais,2023-12-31T00:00:00,1.560242401107029,2.374901533126831,52.21,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Candeias,2023-12-31T00:00:00,1.080032206119163,1.483684778213501,37.37,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capetinga,2023-12-31T00:00:00,1.4484046164290565,1.7391078472137451,20.07,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Capitolio,2023-12-31T00:00:00,1.32,1.431642770767212,8.46,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_da_Cachoeira,2023-12-31T00:00:00,1.32,1.656582236289978,25.5,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_de_Minas,2023-12-31T00:00:00,1.5,1.41524338722229,-5.65,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Carmo_do_Rio_Claro,2023-12-31T00:00:00,1.816856256463288,2.0397913455963135,12.27,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cassia,2023-12-31T00:00:00,1.6377649325626198,1.520608901977539,-7.15,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Conceicao_do_Rio_Verde,2023-12-31T00:00:00,1.44,1.625859022140503,12.91,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Coqueiral,2023-12-31T00:00:00,1.32,1.736084222793579,31.52,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Cristais,2023-12-31T00:00:00,1.5631111111111111,1.4550435543060303,-6.91,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Divisa_Nova,2023-12-31T00:00:00,1.680092592592593,1.444644570350647,-14.01,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Eloi_Mendes,2023-12-31T00:00:00,1.577412806790921,1.8678045272827148,18.41,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Guape,2023-12-31T00:00:00,1.560674157303371,1.5706000328063965,0.64,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Heliodora,2023-12-31T00:00:00,1.439872408293461,1.4052340984344482,-2.41,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ilicinea,2023-12-31T00:00:00,1.560032362459547,1.4410030841827393,-7.63,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Inconfidentes,2023-12-31T00:00:00,1.7401360544217692,1.805589199066162,3.76,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Itamogi,2023-12-31T00:00:00,1.740011254924029,1.6569056510925293,-4.78,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Jacutinga,2023-12-31T00:00:00,1.160053262316911,1.305199146270752,12.51,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lambari,2023-12-31T00:00:00,1.56,1.6338602304458618,4.73,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Lavras,2023-12-31T00:00:00,1.5,1.4688509702682495,-2.08,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Machado,2023-12-31T00:00:00,1.500560931145702,1.7132806777954102,14.18,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monsenhor_Paulo,2023-12-31T00:00:00,1.389931972789116,1.7754082679748535,27.73,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Belo,2023-12-31T00:00:00,1.3799999999999997,1.5488654375076294,12.24,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Santo_de_Minas,2023-12-31T00:00:00,1.620050377833753,1.6327574253082275,0.78,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Monte_Siao,2023-12-31T00:00:00,1.5,1.1354913711547852,-24.3,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Muzambinho,2023-12-31T00:00:00,1.68,1.7278820276260376,2.85,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Natercia,2023-12-31T00:00:00,1.5,2.6642189025878906,77.61,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nazareno,2023-12-31T00:00:00,1.5599056603773582,1.7315311431884766,11.0,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Nepomuceno,2023-12-31T00:00:00,1.650045578851413,1.6100764274597168,-2.42,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Oliveira,2023-12-31T00:00:00,1.8240227434257288,1.419582486152649,-22.17,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Ouro_Fino,2023-12-31T00:00:00,1.68,1.4097590446472168,-16.09,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Paraguacu,2023-12-31T00:00:00,1.298291457286432,1.7496589422225952,34.77,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pedralva,2023-12-31T00:00:00,1.67998417721519,1.5635719299316406,-6.93,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Perdoes,2023-12-31T00:00:00,1.6799401197604789,1.3871039152145386,-17.43,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Poco_Fundo,2023-12-31T00:00:00,1.380160799652325,1.1749587059020996,-14.87,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Pocos_de_Caldas,2023-12-31T00:00:00,1.5,1.593778133392334,6.25,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santa_Rita_do_Sapucai,2023-12-31T00:00:00,1.3799999999999997,1.3903700113296509,0.75,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santana_da_Vargem,2023-12-31T00:00:00,1.5593593593593589,1.8237627744674683,16.96,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Santo_Antonio_do_Amparo,2023-12-31T00:00:00,1.8000000000000005,1.6693971157073975,-7.26,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Goncalo_do_Sapucai,2023-12-31T00:00:00,1.2,2.0290918350219727,69.09,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Sebastiao_do_Paraiso,2023-12-31T00:00:00,1.440017746228926,1.3626142740249634,-5.38,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Sao_Tomas_de_Aquino,2023-12-31T00:00:00,1.319968346082828,1.442032814025879,9.25,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Serrania,2023-12-31T00:00:00,0.9,1.4114515781402588,56.83,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Coracoes,2023-12-31T00:00:00,1.44,1.9010202884674072,32.02,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Tres_Pontas,2023-12-31T00:00:00,1.5900000000000003,1.865662932395935,17.34,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Turvolandia,2023-12-31T00:00:00,1.259748427672956,1.530818223953247,21.52,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 1 (2023),Varginha,2023-12-31T00:00:00,1.679990280646337,1.5791276693344116,-6.0,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.000153775722501296
encoder_hidden_size: 128
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 700
",2025-09-04T09:55:55
IBGE V5 Cluster 2 - Metodo 2 (2023),Alfenas,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alfenas,2013-12-31T00:00:00,1.98,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alfenas,2014-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alfenas,2015-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alfenas,2016-12-31T00:00:00,2.25,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alfenas,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alfenas,2018-12-31T00:00:00,2.34,2.0,14.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alfenas,2019-12-31T00:00:00,1.76,2.0,13.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alfenas,2020-12-31T00:00:00,2.36,2.0,15.25,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alfenas,2021-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alterosa,2012-12-31T00:00:00,1.59,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alterosa,2013-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alterosa,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alterosa,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alterosa,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alterosa,2017-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alterosa,2018-12-31T00:00:00,1.96,2.0,2.04,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alterosa,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alterosa,2020-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alterosa,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Andradas,2012-12-31T00:00:00,1.98,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Andradas,2013-12-31T00:00:00,1.59,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Andradas,2014-12-31T00:00:00,1.21,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Andradas,2015-12-31T00:00:00,1.89,1.0,47.09,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Andradas,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Andradas,2017-12-31T00:00:00,1.34,2.0,49.25,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Andradas,2018-12-31T00:00:00,1.73,2.0,15.61,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Andradas,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Andradas,2020-12-31T00:00:00,2.57,2.0,22.18,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Andradas,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Boa_Esperanca,2012-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Boa_Esperanca,2013-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Boa_Esperanca,2014-12-31T00:00:00,1.15,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Boa_Esperanca,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Boa_Esperanca,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Boa_Esperanca,2017-12-31T00:00:00,2.53,2.0,20.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Boa_Esperanca,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Boa_Esperanca,2019-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Boa_Esperanca,2020-12-31T00:00:00,2.06,2.0,2.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Boa_Esperanca,2021-12-31T00:00:00,1.57,2.0,27.39,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bom_Sucesso,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bom_Sucesso,2013-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bom_Sucesso,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bom_Sucesso,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bom_Sucesso,2016-12-31T00:00:00,1.92,1.0,47.92,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bom_Sucesso,2017-12-31T00:00:00,1.48,2.0,35.14,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bom_Sucesso,2018-12-31T00:00:00,1.7,2.0,17.65,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bom_Sucesso,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bom_Sucesso,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bom_Sucesso,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bueno_Brandao,2012-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bueno_Brandao,2013-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bueno_Brandao,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bueno_Brandao,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bueno_Brandao,2016-12-31T00:00:00,2.1,1.0,52.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bueno_Brandao,2017-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bueno_Brandao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bueno_Brandao,2019-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bueno_Brandao,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bueno_Brandao,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cambuquira,2012-12-31T00:00:00,1.15,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cambuquira,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cambuquira,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cambuquira,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cambuquira,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cambuquira,2017-12-31T00:00:00,1.82,2.0,9.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cambuquira,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cambuquira,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cambuquira,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cambuquira,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campanha,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campanha,2013-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campanha,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campanha,2015-12-31T00:00:00,1.02,1.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campanha,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campanha,2017-12-31T00:00:00,1.88,1.0,46.81,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campanha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campanha,2019-12-31T00:00:00,1.51,2.0,32.45,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campanha,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campanha,2021-12-31T00:00:00,1.33,2.0,50.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_Belo,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_Belo,2013-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_Belo,2014-12-31T00:00:00,0.63,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_Belo,2015-12-31T00:00:00,0.84,1.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_Belo,2016-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_Belo,2017-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_Belo,2018-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_Belo,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_Belo,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_Belo,2021-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_do_Meio,2012-12-31T00:00:00,1.17,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_do_Meio,2013-12-31T00:00:00,1.17,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_do_Meio,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_do_Meio,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_do_Meio,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_do_Meio,2017-12-31T00:00:00,2.08,2.0,3.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_do_Meio,2018-12-31T00:00:00,1.82,2.0,9.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_do_Meio,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_do_Meio,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_do_Meio,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campos_Gerais,2012-12-31T00:00:00,1.65,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campos_Gerais,2013-12-31T00:00:00,1.22,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campos_Gerais,2014-12-31T00:00:00,1.15,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campos_Gerais,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campos_Gerais,2016-12-31T00:00:00,2.5,1.0,60.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campos_Gerais,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campos_Gerais,2018-12-31T00:00:00,2.21,3.0,35.75,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campos_Gerais,2019-12-31T00:00:00,1.59,2.0,25.79,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campos_Gerais,2020-12-31T00:00:00,2.69,2.0,25.65,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campos_Gerais,2021-12-31T00:00:00,1.59,2.0,25.79,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Candeias,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Candeias,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Candeias,2014-12-31T00:00:00,0.99,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Candeias,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Candeias,2016-12-31T00:00:00,2.28,2.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Candeias,2017-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Candeias,2018-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Candeias,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Candeias,2020-12-31T00:00:00,1.86,2.0,7.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Candeias,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capetinga,2012-12-31T00:00:00,1.81,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capetinga,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capetinga,2014-12-31T00:00:00,1.59,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capetinga,2015-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capetinga,2016-12-31T00:00:00,1.93,2.0,3.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capetinga,2017-12-31T00:00:00,2.09,2.0,4.31,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capetinga,2018-12-31T00:00:00,2.7,2.0,25.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capetinga,2019-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capetinga,2020-12-31T00:00:00,2.21,3.0,35.75,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capetinga,2021-12-31T00:00:00,1.64,2.0,21.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capitolio,2012-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capitolio,2013-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capitolio,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capitolio,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capitolio,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capitolio,2017-12-31T00:00:00,1.47,1.0,31.97,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capitolio,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capitolio,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capitolio,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capitolio,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_da_Cachoeira,2012-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_da_Cachoeira,2013-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_da_Cachoeira,2014-12-31T00:00:00,1.15,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_da_Cachoeira,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_da_Cachoeira,2016-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_da_Cachoeira,2017-12-31T00:00:00,1.73,1.0,42.2,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_da_Cachoeira,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_da_Cachoeira,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_da_Cachoeira,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_da_Cachoeira,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_de_Minas,2012-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_de_Minas,2013-12-31T00:00:00,1.74,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_de_Minas,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_de_Minas,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_de_Minas,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_de_Minas,2017-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_de_Minas,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_de_Minas,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_de_Minas,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_de_Minas,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_do_Rio_Claro,2012-12-31T00:00:00,1.66,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_do_Rio_Claro,2013-12-31T00:00:00,1.94,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_do_Rio_Claro,2014-12-31T00:00:00,1.65,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_do_Rio_Claro,2015-12-31T00:00:00,1.53,2.0,30.72,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_do_Rio_Claro,2016-12-31T00:00:00,2.37,2.0,15.61,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_do_Rio_Claro,2017-12-31T00:00:00,1.82,2.0,9.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_do_Rio_Claro,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_do_Rio_Claro,2019-12-31T00:00:00,1.94,2.0,3.09,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_do_Rio_Claro,2020-12-31T00:00:00,2.78,2.0,28.06,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_do_Rio_Claro,2021-12-31T00:00:00,1.64,2.0,21.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cassia,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cassia,2013-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cassia,2014-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cassia,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cassia,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cassia,2017-12-31T00:00:00,1.6,1.0,37.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cassia,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cassia,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cassia,2020-12-31T00:00:00,2.17,2.0,7.83,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cassia,2021-12-31T00:00:00,1.31,2.0,52.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2015-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2017-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Coqueiral,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Coqueiral,2013-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Coqueiral,2014-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Coqueiral,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Coqueiral,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Coqueiral,2017-12-31T00:00:00,1.75,2.0,14.29,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Coqueiral,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Coqueiral,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Coqueiral,2020-12-31T00:00:00,2.16,2.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Coqueiral,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cristais,2012-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cristais,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cristais,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cristais,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cristais,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cristais,2017-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cristais,2018-12-31T00:00:00,1.92,3.0,56.25,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cristais,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cristais,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cristais,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Divisa_Nova,2012-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Divisa_Nova,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Divisa_Nova,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Divisa_Nova,2015-12-31T00:00:00,1.08,2.0,85.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Divisa_Nova,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Divisa_Nova,2017-12-31T00:00:00,2.28,1.0,56.14,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Divisa_Nova,2018-12-31T00:00:00,1.8,3.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Divisa_Nova,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Divisa_Nova,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Divisa_Nova,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Eloi_Mendes,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Eloi_Mendes,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Eloi_Mendes,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Eloi_Mendes,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Eloi_Mendes,2016-12-31T00:00:00,1.74,1.0,42.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Eloi_Mendes,2017-12-31T00:00:00,1.83,2.0,9.29,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Eloi_Mendes,2018-12-31T00:00:00,1.72,2.0,16.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Eloi_Mendes,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Eloi_Mendes,2020-12-31T00:00:00,2.11,2.0,5.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Eloi_Mendes,2021-12-31T00:00:00,1.04,2.0,92.31,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Guape,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Guape,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Guape,2014-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Guape,2015-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Guape,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Guape,2017-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Guape,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Guape,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Guape,2020-12-31T00:00:00,1.85,2.0,8.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Guape,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Heliodora,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Heliodora,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Heliodora,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Heliodora,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Heliodora,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Heliodora,2017-12-31T00:00:00,1.57,1.0,36.31,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Heliodora,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Heliodora,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Heliodora,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Heliodora,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ilicinea,2012-12-31T00:00:00,1.69,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ilicinea,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ilicinea,2014-12-31T00:00:00,1.35,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ilicinea,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ilicinea,2016-12-31T00:00:00,2.46,2.0,18.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ilicinea,2017-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ilicinea,2018-12-31T00:00:00,2.46,3.0,21.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ilicinea,2019-12-31T00:00:00,1.86,2.0,7.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ilicinea,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ilicinea,2021-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Inconfidentes,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Inconfidentes,2013-12-31T00:00:00,1.4,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Inconfidentes,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Inconfidentes,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Inconfidentes,2016-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Inconfidentes,2017-12-31T00:00:00,1.3,2.0,53.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Inconfidentes,2018-12-31T00:00:00,2.88,2.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Inconfidentes,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Inconfidentes,2020-12-31T00:00:00,2.46,2.0,18.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Inconfidentes,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Itamogi,2012-12-31T00:00:00,1.95,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Itamogi,2013-12-31T00:00:00,1.17,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Itamogi,2014-12-31T00:00:00,1.6,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Itamogi,2015-12-31T00:00:00,1.58,1.0,36.71,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Itamogi,2016-12-31T00:00:00,1.95,2.0,2.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Itamogi,2017-12-31T00:00:00,1.42,2.0,40.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Itamogi,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Itamogi,2019-12-31T00:00:00,1.85,2.0,8.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Itamogi,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Itamogi,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Jacutinga,2012-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Jacutinga,2013-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Jacutinga,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Jacutinga,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Jacutinga,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Jacutinga,2017-12-31T00:00:00,1.37,1.0,27.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Jacutinga,2018-12-31T00:00:00,1.74,1.0,42.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Jacutinga,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Jacutinga,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Jacutinga,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lambari,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lambari,2013-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lambari,2014-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lambari,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lambari,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lambari,2017-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lambari,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lambari,2019-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lambari,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lambari,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lavras,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lavras,2013-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lavras,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lavras,2015-12-31T00:00:00,0.84,1.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lavras,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lavras,2017-12-31T00:00:00,2.25,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lavras,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lavras,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lavras,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lavras,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Machado,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Machado,2013-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Machado,2014-12-31T00:00:00,1.15,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Machado,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Machado,2016-12-31T00:00:00,1.86,2.0,7.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Machado,2017-12-31T00:00:00,1.51,2.0,32.45,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Machado,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Machado,2019-12-31T00:00:00,1.53,2.0,30.72,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Machado,2020-12-31T00:00:00,2.21,2.0,9.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Machado,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monsenhor_Paulo,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monsenhor_Paulo,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monsenhor_Paulo,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monsenhor_Paulo,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monsenhor_Paulo,2016-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monsenhor_Paulo,2017-12-31T00:00:00,1.72,2.0,16.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monsenhor_Paulo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monsenhor_Paulo,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monsenhor_Paulo,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monsenhor_Paulo,2021-12-31T00:00:00,1.41,2.0,41.84,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Belo,2012-12-31T00:00:00,1.63,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Belo,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Belo,2014-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Belo,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Belo,2016-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Belo,2017-12-31T00:00:00,2.43,2.0,17.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Belo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Belo,2019-12-31T00:00:00,1.64,2.0,21.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Belo,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Belo,2021-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Santo_de_Minas,2012-12-31T00:00:00,1.89,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Santo_de_Minas,2013-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Santo_de_Minas,2014-12-31T00:00:00,1.6,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Santo_de_Minas,2015-12-31T00:00:00,1.35,2.0,48.15,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Santo_de_Minas,2016-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Santo_de_Minas,2017-12-31T00:00:00,1.08,2.0,85.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Santo_de_Minas,2018-12-31T00:00:00,1.88,2.0,6.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Santo_de_Minas,2019-12-31T00:00:00,1.59,2.0,25.79,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Santo_de_Minas,2020-12-31T00:00:00,2.12,2.0,5.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Santo_de_Minas,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Siao,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Siao,2013-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Siao,2014-12-31T00:00:00,0.66,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Siao,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Siao,2016-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Siao,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Siao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Siao,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Siao,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Siao,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Muzambinho,2012-12-31T00:00:00,1.74,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Muzambinho,2013-12-31T00:00:00,1.35,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Muzambinho,2014-12-31T00:00:00,1.35,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Muzambinho,2015-12-31T00:00:00,1.48,1.0,32.43,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Muzambinho,2016-12-31T00:00:00,1.98,1.0,49.49,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Muzambinho,2017-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Muzambinho,2018-12-31T00:00:00,1.3,2.0,53.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Muzambinho,2019-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Muzambinho,2020-12-31T00:00:00,1.76,2.0,13.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Muzambinho,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Natercia,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Natercia,2013-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Natercia,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Natercia,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Natercia,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Natercia,2017-12-31T00:00:00,1.6,2.0,25.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Natercia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Natercia,2019-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Natercia,2020-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Natercia,2021-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nazareno,2012-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nazareno,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nazareno,2014-12-31T00:00:00,1.3,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nazareno,2015-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nazareno,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nazareno,2017-12-31T00:00:00,1.59,2.0,25.79,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nazareno,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nazareno,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nazareno,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nazareno,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nepomuceno,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nepomuceno,2013-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nepomuceno,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nepomuceno,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nepomuceno,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nepomuceno,2017-12-31T00:00:00,1.69,2.0,18.34,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nepomuceno,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nepomuceno,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nepomuceno,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nepomuceno,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Oliveira,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Oliveira,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Oliveira,2014-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Oliveira,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Oliveira,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Oliveira,2017-12-31T00:00:00,1.4,2.0,42.86,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Oliveira,2018-12-31T00:00:00,2.1,1.0,52.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Oliveira,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Oliveira,2020-12-31T00:00:00,1.95,2.0,2.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Oliveira,2021-12-31T00:00:00,1.54,2.0,29.87,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ouro_Fino,2012-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ouro_Fino,2013-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ouro_Fino,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ouro_Fino,2015-12-31T00:00:00,1.29,1.0,22.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ouro_Fino,2016-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ouro_Fino,2017-12-31T00:00:00,1.42,1.0,29.58,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ouro_Fino,2018-12-31T00:00:00,1.69,1.0,40.83,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ouro_Fino,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ouro_Fino,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ouro_Fino,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Paraguacu,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Paraguacu,2013-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Paraguacu,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Paraguacu,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Paraguacu,2016-12-31T00:00:00,1.98,1.0,49.49,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Paraguacu,2017-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Paraguacu,2018-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Paraguacu,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Paraguacu,2020-12-31T00:00:00,2.34,2.0,14.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Paraguacu,2021-12-31T00:00:00,0.97,2.0,106.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pedralva,2012-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pedralva,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pedralva,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pedralva,2015-12-31T00:00:00,0.96,2.0,108.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pedralva,2016-12-31T00:00:00,2.28,2.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pedralva,2017-12-31T00:00:00,1.43,2.0,39.86,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pedralva,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pedralva,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pedralva,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pedralva,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Perdoes,2012-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Perdoes,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Perdoes,2014-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Perdoes,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Perdoes,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Perdoes,2017-12-31T00:00:00,1.84,2.0,8.7,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Perdoes,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Perdoes,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Perdoes,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Perdoes,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Poco_Fundo,2012-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Poco_Fundo,2013-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Poco_Fundo,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Poco_Fundo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Poco_Fundo,2016-12-31T00:00:00,1.41,1.0,29.08,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Poco_Fundo,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Poco_Fundo,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Poco_Fundo,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Poco_Fundo,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Poco_Fundo,2021-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pocos_de_Caldas,2012-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pocos_de_Caldas,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pocos_de_Caldas,2014-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pocos_de_Caldas,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pocos_de_Caldas,2016-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pocos_de_Caldas,2017-12-31T00:00:00,1.77,2.0,12.99,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pocos_de_Caldas,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pocos_de_Caldas,2019-12-31T00:00:00,1.08,2.0,85.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pocos_de_Caldas,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pocos_de_Caldas,2021-12-31T00:00:00,1.42,1.0,29.58,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2017-12-31T00:00:00,1.79,2.0,11.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santana_da_Vargem,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santana_da_Vargem,2013-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santana_da_Vargem,2014-12-31T00:00:00,1.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santana_da_Vargem,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santana_da_Vargem,2016-12-31T00:00:00,1.74,1.0,42.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santana_da_Vargem,2017-12-31T00:00:00,2.09,2.0,4.31,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santana_da_Vargem,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santana_da_Vargem,2019-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santana_da_Vargem,2020-12-31T00:00:00,2.22,2.0,9.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santana_da_Vargem,2021-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2012-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2013-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2014-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2017-12-31T00:00:00,1.34,2.0,49.25,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2018-12-31T00:00:00,2.16,2.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2019-12-31T00:00:00,1.64,1.0,39.02,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2012-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2013-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2017-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2019-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2021-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2012-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2013-12-31T00:00:00,1.71,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2014-12-31T00:00:00,1.42,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2015-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2016-12-31T00:00:00,1.7,2.0,17.65,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2017-12-31T00:00:00,1.54,2.0,29.87,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2018-12-31T00:00:00,1.89,2.0,5.82,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2019-12-31T00:00:00,1.47,2.0,36.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2020-12-31T00:00:00,1.93,2.0,3.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2021-12-31T00:00:00,1.14,2.0,75.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2012-12-31T00:00:00,1.75,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2013-12-31T00:00:00,1.05,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2014-12-31T00:00:00,1.35,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2017-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Serrania,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Serrania,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Serrania,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Serrania,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Serrania,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Serrania,2017-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Serrania,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Serrania,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Serrania,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Serrania,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Coracoes,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Coracoes,2013-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Coracoes,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Coracoes,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Coracoes,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Coracoes,2017-12-31T00:00:00,2.67,2.0,25.09,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Coracoes,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Coracoes,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Coracoes,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Coracoes,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Pontas,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Pontas,2013-12-31T00:00:00,1.53,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Pontas,2014-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Pontas,2015-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Pontas,2016-12-31T00:00:00,1.86,2.0,7.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Pontas,2017-12-31T00:00:00,1.88,2.0,6.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Pontas,2018-12-31T00:00:00,2.22,2.0,9.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Pontas,2019-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Pontas,2020-12-31T00:00:00,2.26,2.0,11.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Pontas,2021-12-31T00:00:00,1.35,2.0,48.15,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Turvolandia,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Turvolandia,2013-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Turvolandia,2014-12-31T00:00:00,0.84,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Turvolandia,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Turvolandia,2016-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Turvolandia,2017-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Turvolandia,2018-12-31T00:00:00,2.29,2.0,12.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Turvolandia,2019-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Turvolandia,2020-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Turvolandia,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Varginha,2012-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Varginha,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Varginha,2014-12-31T00:00:00,1.03,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Varginha,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Varginha,2016-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Varginha,2017-12-31T00:00:00,2.12,2.0,5.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Varginha,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Varginha,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Varginha,2020-12-31T00:00:00,1.9,2.0,5.26,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Varginha,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alfenas,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alterosa,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Andradas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Boa_Esperanca,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bom_Sucesso,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bueno_Brandao,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cambuquira,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campanha,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_Belo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_do_Meio,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campos_Gerais,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Candeias,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capetinga,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capitolio,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_da_Cachoeira,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_do_Rio_Claro,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cassia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Coqueiral,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cristais,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Divisa_Nova,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Eloi_Mendes,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Guape,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Heliodora,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ilicinea,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Inconfidentes,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Itamogi,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Jacutinga,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lambari,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lavras,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Machado,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monsenhor_Paulo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Belo,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Santo_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Siao,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Muzambinho,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Natercia,2022-12-31T00:00:00,1.0,0,-100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nazareno,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nepomuceno,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Oliveira,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ouro_Fino,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Paraguacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pedralva,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Perdoes,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Poco_Fundo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pocos_de_Caldas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santana_da_Vargem,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Serrania,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Coracoes,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Pontas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Turvolandia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Varginha,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alfenas,2023-12-31T00:00:00,1.698430922311519,1.3950411081314087,-17.86,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Alterosa,2023-12-31T00:00:00,1.2603960396039595,1.180938720703125,-6.3,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Andradas,2023-12-31T00:00:00,1.22,1.4141794443130493,15.92,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Boa_Esperanca,2023-12-31T00:00:00,1.08,1.3886303901672363,28.58,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bom_Sucesso,2023-12-31T00:00:00,1.5,1.5489778518676758,3.27,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Bueno_Brandao,2023-12-31T00:00:00,1.379901960784314,1.3224663734436035,-4.16,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cambuquira,2023-12-31T00:00:00,1.56,1.4205451011657715,-8.94,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campanha,2023-12-31T00:00:00,1.327075098814229,1.427518367767334,7.57,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_Belo,2023-12-31T00:00:00,1.5,1.3915117979049683,-7.23,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campo_do_Meio,2023-12-31T00:00:00,0.8771626297577856,1.6230413913726807,85.03,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Campos_Gerais,2023-12-31T00:00:00,1.560242401107029,1.80078125,15.42,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Candeias,2023-12-31T00:00:00,1.080032206119163,1.2622889280319214,16.88,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capetinga,2023-12-31T00:00:00,1.4484046164290565,1.616837501525879,11.63,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Capitolio,2023-12-31T00:00:00,1.32,1.330466628074646,0.79,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_da_Cachoeira,2023-12-31T00:00:00,1.32,1.4143478870391846,7.15,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_de_Minas,2023-12-31T00:00:00,1.5,1.3542869091033936,-9.71,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Carmo_do_Rio_Claro,2023-12-31T00:00:00,1.816856256463288,1.638615369796753,-9.81,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cassia,2023-12-31T00:00:00,1.6377649325626198,1.4944285154342651,-8.75,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Conceicao_do_Rio_Verde,2023-12-31T00:00:00,1.44,1.5910723209381104,10.49,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Coqueiral,2023-12-31T00:00:00,1.32,1.5144217014312744,14.73,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Cristais,2023-12-31T00:00:00,1.5631111111111111,1.3550609350204468,-13.31,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Divisa_Nova,2023-12-31T00:00:00,1.680092592592593,1.363952875137329,-18.82,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Eloi_Mendes,2023-12-31T00:00:00,1.577412806790921,1.2588202953338623,-20.2,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Guape,2023-12-31T00:00:00,1.560674157303371,1.3576897382736206,-13.01,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Heliodora,2023-12-31T00:00:00,1.439872408293461,1.193719744682312,-17.1,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ilicinea,2023-12-31T00:00:00,1.560032362459547,1.3739347457885742,-11.93,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Inconfidentes,2023-12-31T00:00:00,1.7401360544217692,1.7317802906036377,-0.48,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Itamogi,2023-12-31T00:00:00,1.740011254924029,1.386978268623352,-20.29,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Jacutinga,2023-12-31T00:00:00,1.160053262316911,1.3231263160705566,14.06,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lambari,2023-12-31T00:00:00,1.56,1.4653856754302979,-6.07,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Lavras,2023-12-31T00:00:00,1.5,1.3476293087005615,-10.16,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Machado,2023-12-31T00:00:00,1.500560931145702,1.2379708290100098,-17.5,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monsenhor_Paulo,2023-12-31T00:00:00,1.389931972789116,1.5019292831420898,8.06,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Belo,2023-12-31T00:00:00,1.3799999999999997,1.367974877357483,-0.87,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Santo_de_Minas,2023-12-31T00:00:00,1.620050377833753,1.328096866607666,-18.02,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Monte_Siao,2023-12-31T00:00:00,1.5,1.369636058807373,-8.69,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Muzambinho,2023-12-31T00:00:00,1.68,1.3840968608856201,-17.61,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Natercia,2023-12-31T00:00:00,1.5,0.647415041923523,-56.84,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nazareno,2023-12-31T00:00:00,1.5599056603773582,1.5211516618728638,-2.48,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Nepomuceno,2023-12-31T00:00:00,1.650045578851413,1.2097947597503662,-26.68,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Oliveira,2023-12-31T00:00:00,1.8240227434257288,1.5475523471832275,-15.16,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Ouro_Fino,2023-12-31T00:00:00,1.68,1.2232640981674194,-27.19,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Paraguacu,2023-12-31T00:00:00,1.298291457286432,1.2824996709823608,-1.22,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pedralva,2023-12-31T00:00:00,1.67998417721519,1.3991459608078003,-16.72,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Perdoes,2023-12-31T00:00:00,1.6799401197604789,1.449873924255371,-13.69,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Poco_Fundo,2023-12-31T00:00:00,1.380160799652325,1.1165852546691895,-19.1,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Pocos_de_Caldas,2023-12-31T00:00:00,1.5,1.4385536909103394,-4.1,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santa_Rita_do_Sapucai,2023-12-31T00:00:00,1.3799999999999997,1.3765621185302734,-0.25,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santana_da_Vargem,2023-12-31T00:00:00,1.5593593593593589,1.5042407512664795,-3.53,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Santo_Antonio_do_Amparo,2023-12-31T00:00:00,1.8000000000000005,1.6014961004257202,-11.03,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Goncalo_do_Sapucai,2023-12-31T00:00:00,1.2,1.7255498170852661,43.8,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Sebastiao_do_Paraiso,2023-12-31T00:00:00,1.440017746228926,1.2391737699508667,-13.95,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Sao_Tomas_de_Aquino,2023-12-31T00:00:00,1.319968346082828,1.2986546754837036,-1.61,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Serrania,2023-12-31T00:00:00,0.9,1.224283218383789,36.03,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Coracoes,2023-12-31T00:00:00,1.44,1.4607722759246826,1.44,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Tres_Pontas,2023-12-31T00:00:00,1.5900000000000003,1.4532911777496338,-8.6,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Turvolandia,2023-12-31T00:00:00,1.259748427672956,1.4496300220489502,15.07,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 2 - Metodo 2 (2023),Varginha,2023-12-31T00:00:00,1.679990280646337,1.2981586456298828,-22.73,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 2.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 5
learning_rate: 0.0002501545850929119
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:56:58
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Caparao,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Caparao,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Caparao,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Caparao,2015-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Caparao,2016-12-31T00:00:00,2.1,1.0,52.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Caparao,2017-12-31T00:00:00,1.37,2.0,45.99,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Caparao,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Caparao,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Caparao,2020-12-31T00:00:00,1.92,1.0,47.92,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Caparao,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Jequitiba,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Jequitiba,2013-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Jequitiba,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Jequitiba,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Jequitiba,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Jequitiba,2017-12-31T00:00:00,1.53,1.0,34.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Jequitiba,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Jequitiba,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Jequitiba,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Jequitiba,2021-12-31T00:00:00,0.9,2.0,122.22,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Araponga,2012-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Araponga,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Araponga,2014-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Araponga,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Araponga,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Araponga,2017-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Araponga,2018-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Araponga,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Araponga,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Araponga,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caiana,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caiana,2013-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caiana,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caiana,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caiana,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caiana,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caiana,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caiana,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caiana,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caiana,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caparao,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caparao,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caparao,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caparao,2015-12-31T00:00:00,1.02,1.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caparao,2016-12-31T00:00:00,1.92,1.0,47.92,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caparao,2017-12-31T00:00:00,1.45,1.0,31.03,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caparao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caparao,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caparao,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caparao,2021-12-31T00:00:00,0.9,2.0,122.22,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caputira,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caputira,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caputira,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caputira,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caputira,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caputira,2017-12-31T00:00:00,0.77,1.0,29.87,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caputira,2018-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caputira,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caputira,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caputira,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Carangola,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Carangola,2013-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Carangola,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Carangola,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Carangola,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Carangola,2017-12-31T00:00:00,1.1,1.0,9.09,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Carangola,2018-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Carangola,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Carangola,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Carangola,2021-12-31T00:00:00,1.02,1.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caratinga,2012-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caratinga,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caratinga,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caratinga,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caratinga,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caratinga,2017-12-31T00:00:00,1.46,1.0,31.51,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caratinga,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caratinga,2019-12-31T00:00:00,1.0,2.0,100.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caratinga,2020-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caratinga,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Chale,2012-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Chale,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Chale,2014-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Chale,2015-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Chale,2016-12-31T00:00:00,1.42,2.0,40.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Chale,2017-12-31T00:00:00,0.93,1.0,7.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Chale,2018-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Chale,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Chale,2020-12-31T00:00:00,1.71,1.0,41.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Chale,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Divino,2012-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Divino,2013-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Divino,2014-12-31T00:00:00,0.89,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Divino,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Divino,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Divino,2017-12-31T00:00:00,1.19,1.0,15.97,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Divino,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Divino,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Divino,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Divino,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Durande,2012-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Durande,2013-12-31T00:00:00,2.22,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Durande,2014-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Durande,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Durande,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Durande,2017-12-31T00:00:00,1.33,2.0,50.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Durande,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Durande,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Durande,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Durande,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ervalia,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ervalia,2013-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ervalia,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ervalia,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ervalia,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ervalia,2017-12-31T00:00:00,1.36,1.0,26.47,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ervalia,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ervalia,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ervalia,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ervalia,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Espera_Feliz,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Espera_Feliz,2013-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Espera_Feliz,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Espera_Feliz,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Espera_Feliz,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Espera_Feliz,2017-12-31T00:00:00,1.21,1.0,17.36,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Espera_Feliz,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Espera_Feliz,2019-12-31T00:00:00,0.9,2.0,122.22,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Espera_Feliz,2020-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Espera_Feliz,2021-12-31T00:00:00,0.78,1.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Guimarania,2012-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Guimarania,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Guimarania,2014-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Guimarania,2015-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Guimarania,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Guimarania,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Guimarania,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Guimarania,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Guimarania,2020-12-31T00:00:00,2.15,2.0,6.98,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Guimarania,2021-12-31T00:00:00,1.54,2.0,29.87,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Lajinha,2012-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Lajinha,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Lajinha,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Lajinha,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Lajinha,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Lajinha,2017-12-31T00:00:00,1.04,1.0,3.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Lajinha,2018-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Lajinha,2019-12-31T00:00:00,1.06,1.0,5.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Lajinha,2020-12-31T00:00:00,1.92,1.0,47.92,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Lajinha,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Luisburgo,2012-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Luisburgo,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Luisburgo,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Luisburgo,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Luisburgo,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Luisburgo,2017-12-31T00:00:00,1.53,1.0,34.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Luisburgo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Luisburgo,2019-12-31T00:00:00,1.64,2.0,21.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Luisburgo,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Luisburgo,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhuacu,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhuacu,2013-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhuacu,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhuacu,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhuacu,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhuacu,2017-12-31T00:00:00,1.43,1.0,30.07,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhuacu,2018-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhuacu,2019-12-31T00:00:00,1.02,1.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhuacu,2020-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhuacu,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhumirim,2012-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhumirim,2013-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhumirim,2014-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhumirim,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhumirim,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhumirim,2017-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhumirim,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhumirim,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhumirim,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhumirim,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Martins_Soares,2012-12-31T00:00:00,2.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Martins_Soares,2013-12-31T00:00:00,1.98,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Martins_Soares,2014-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Martins_Soares,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Martins_Soares,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Martins_Soares,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Martins_Soares,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Martins_Soares,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Martins_Soares,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Martins_Soares,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Matipo,2012-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Matipo,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Matipo,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Matipo,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Matipo,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Matipo,2017-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Matipo,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Matipo,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Matipo,2020-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Matipo,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Miradouro,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Miradouro,2013-12-31T00:00:00,2.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Miradouro,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Miradouro,2015-12-31T00:00:00,1.59,2.0,25.79,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Miradouro,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Miradouro,2017-12-31T00:00:00,1.16,1.0,13.79,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Miradouro,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Miradouro,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Miradouro,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Miradouro,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Mutum,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Mutum,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Mutum,2014-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Mutum,2015-12-31T00:00:00,1.15,1.0,13.04,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Mutum,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Mutum,2017-12-31T00:00:00,1.45,1.0,31.03,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Mutum,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Mutum,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Mutum,2020-12-31T00:00:00,1.65,1.0,39.39,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Mutum,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Nova_Belem,2012-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Nova_Belem,2013-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Nova_Belem,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Nova_Belem,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Nova_Belem,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Nova_Belem,2017-12-31T00:00:00,0.48,1.0,108.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Nova_Belem,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Nova_Belem,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Nova_Belem,2020-12-31T00:00:00,1.1,1.0,9.09,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Nova_Belem,2021-12-31T00:00:00,1.0,1.0,0.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Orizania,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Orizania,2013-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Orizania,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Orizania,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Orizania,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Orizania,2017-12-31T00:00:00,1.47,1.0,31.97,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Orizania,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Orizania,2019-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Orizania,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Orizania,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Patos_de_Minas,2012-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Patos_de_Minas,2013-12-31T00:00:00,2.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Patos_de_Minas,2014-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Patos_de_Minas,2015-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Patos_de_Minas,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Patos_de_Minas,2017-12-31T00:00:00,1.7,2.0,17.65,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Patos_de_Minas,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Patos_de_Minas,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Patos_de_Minas,2020-12-31T00:00:00,1.94,2.0,3.09,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Patos_de_Minas,2021-12-31T00:00:00,1.72,2.0,16.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Raul_Soares,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Raul_Soares,2013-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Raul_Soares,2014-12-31T00:00:00,0.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Raul_Soares,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Raul_Soares,2016-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Raul_Soares,2017-12-31T00:00:00,0.86,1.0,16.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Raul_Soares,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Raul_Soares,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Raul_Soares,2020-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Raul_Soares,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Reduto,2012-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Reduto,2013-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Reduto,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Reduto,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Reduto,2016-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Reduto,2017-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Reduto,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Reduto,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Reduto,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Reduto,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Barbara_do_Leste,2012-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Barbara_do_Leste,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Barbara_do_Leste,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Barbara_do_Leste,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Barbara_do_Leste,2016-12-31T00:00:00,1.02,1.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Barbara_do_Leste,2017-12-31T00:00:00,0.91,1.0,9.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Barbara_do_Leste,2018-12-31T00:00:00,1.74,1.0,42.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Barbara_do_Leste,2019-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Barbara_do_Leste,2020-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Barbara_do_Leste,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Margarida,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Margarida,2013-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Margarida,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Margarida,2015-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Margarida,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Margarida,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Margarida,2018-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Margarida,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Margarida,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Margarida,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Rita_de_Minas,2012-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Rita_de_Minas,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Rita_de_Minas,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Rita_de_Minas,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Rita_de_Minas,2016-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Rita_de_Minas,2017-12-31T00:00:00,1.25,1.0,20.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Rita_de_Minas,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Rita_de_Minas,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Rita_de_Minas,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Rita_de_Minas,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santana_do_Manhuacu,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santana_do_Manhuacu,2013-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santana_do_Manhuacu,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santana_do_Manhuacu,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santana_do_Manhuacu,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santana_do_Manhuacu,2017-12-31T00:00:00,1.28,1.0,21.88,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santana_do_Manhuacu,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santana_do_Manhuacu,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santana_do_Manhuacu,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santana_do_Manhuacu,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Domingos_das_Dores,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Domingos_das_Dores,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Domingos_das_Dores,2014-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Domingos_das_Dores,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Domingos_das_Dores,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Domingos_das_Dores,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Domingos_das_Dores,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Domingos_das_Dores,2019-12-31T00:00:00,1.3,2.0,53.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Domingos_das_Dores,2020-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Domingos_das_Dores,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2012-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2013-12-31T00:00:00,1.74,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2017-12-31T00:00:00,1.7,1.0,41.18,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2021-12-31T00:00:00,0.9,2.0,122.22,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sericita,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sericita,2013-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sericita,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sericita,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sericita,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sericita,2017-12-31T00:00:00,1.12,1.0,10.71,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sericita,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sericita,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sericita,2020-12-31T00:00:00,2.52,1.0,60.32,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sericita,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ubaporanga,2012-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ubaporanga,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ubaporanga,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ubaporanga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ubaporanga,2016-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ubaporanga,2017-12-31T00:00:00,1.03,1.0,2.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ubaporanga,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ubaporanga,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ubaporanga,2020-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ubaporanga,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Vermelho_Novo,2012-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Vermelho_Novo,2013-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Vermelho_Novo,2014-12-31T00:00:00,0.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Vermelho_Novo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Vermelho_Novo,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Vermelho_Novo,2017-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Vermelho_Novo,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Vermelho_Novo,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Vermelho_Novo,2020-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Vermelho_Novo,2021-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Caparao,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Jequitiba,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Araponga,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caiana,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caparao,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caputira,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Carangola,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caratinga,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Chale,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Divino,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Durande,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ervalia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Espera_Feliz,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Guimarania,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Lajinha,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Luisburgo,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhuacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhumirim,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Martins_Soares,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Matipo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Miradouro,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Mutum,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Nova_Belem,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Orizania,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Patos_de_Minas,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Raul_Soares,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Reduto,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Barbara_do_Leste,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Margarida,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Rita_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santana_do_Manhuacu,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Domingos_das_Dores,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sericita,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ubaporanga,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Vermelho_Novo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Caparao,2023-12-31T00:00:00,1.32,1.517947793006897,15.0,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Alto_Jequitiba,2023-12-31T00:00:00,1.08,1.2147648334503174,12.48,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Araponga,2023-12-31T00:00:00,1.5,1.4165644645690918,-5.56,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caiana,2023-12-31T00:00:00,1.08,1.0913761854171753,1.05,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caparao,2023-12-31T00:00:00,1.32,1.379457712173462,4.5,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caputira,2023-12-31T00:00:00,1.380065005417118,1.5349041223526,11.22,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Carangola,2023-12-31T00:00:00,1.140118343195266,1.2235426902770996,7.32,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Caratinga,2023-12-31T00:00:00,1.3799999999999997,1.7585244178771973,27.43,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Chale,2023-12-31T00:00:00,1.3799999999999997,1.4741429090499878,6.82,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Divino,2023-12-31T00:00:00,1.02,1.3361209630966187,30.99,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Durande,2023-12-31T00:00:00,1.3799999999999997,1.73490571975708,25.72,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ervalia,2023-12-31T00:00:00,1.380044843049327,1.3924789428710938,0.9,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Espera_Feliz,2023-12-31T00:00:00,1.08,1.3219616413116455,22.4,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Guimarania,2023-12-31T00:00:00,1.7704600484261497,1.712711215019226,-3.26,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Lajinha,2023-12-31T00:00:00,1.4399536768963517,1.5281808376312256,6.13,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Luisburgo,2023-12-31T00:00:00,1.44,1.667196273803711,15.78,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhuacu,2023-12-31T00:00:00,1.2,1.3283578157424927,10.7,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Manhumirim,2023-12-31T00:00:00,1.44,1.4174976348876953,-1.56,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Martins_Soares,2023-12-31T00:00:00,1.2,1.4417390823364258,20.14,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Matipo,2023-12-31T00:00:00,1.25990675990676,1.291188359260559,2.48,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Miradouro,2023-12-31T00:00:00,1.140084388185654,1.2401801347732544,8.78,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Mutum,2023-12-31T00:00:00,1.019985196150999,1.4121630191802979,38.45,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Nova_Belem,2023-12-31T00:00:00,0.96,1.0215744972229004,6.41,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Orizania,2023-12-31T00:00:00,1.2,1.4205944538116455,18.38,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Patos_de_Minas,2023-12-31T00:00:00,1.782452316076294,1.8564136028289795,4.15,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Raul_Soares,2023-12-31T00:00:00,1.5,1.181520938873291,-21.23,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Reduto,2023-12-31T00:00:00,0.9,1.4036489725112915,55.96,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Barbara_do_Leste,2023-12-31T00:00:00,1.08,1.2901989221572876,19.46,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Margarida,2023-12-31T00:00:00,1.32,1.3466638326644897,2.02,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santa_Rita_de_Minas,2023-12-31T00:00:00,1.019930675909879,1.2348605394363403,21.07,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Santana_do_Manhuacu,2023-12-31T00:00:00,1.32,1.45022451877594,9.87,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Domingos_das_Dores,2023-12-31T00:00:00,1.296078431372549,1.3337640762329102,2.91,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sao_Joao_do_Manhuacu,2023-12-31T00:00:00,1.2,1.305994987487793,8.83,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Sericita,2023-12-31T00:00:00,1.26,1.523885726928711,20.94,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Ubaporanga,2023-12-31T00:00:00,1.5,1.3830629587173462,-7.8,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 1 (2023),Vermelho_Novo,2023-12-31T00:00:00,1.5,1.0775713920593262,-28.16,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 8
learning_rate: 0.0001190081996218905
encoder_hidden_size: 192
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.01
steps: 100
",2025-09-04T09:58:13
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Caparao,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Caparao,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Caparao,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Caparao,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Caparao,2016-12-31T00:00:00,2.1,1.0,52.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Caparao,2017-12-31T00:00:00,1.37,1.0,27.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Caparao,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Caparao,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Caparao,2020-12-31T00:00:00,1.92,1.0,47.92,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Caparao,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Jequitiba,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Jequitiba,2013-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Jequitiba,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Jequitiba,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Jequitiba,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Jequitiba,2017-12-31T00:00:00,1.53,1.0,34.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Jequitiba,2018-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Jequitiba,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Jequitiba,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Jequitiba,2021-12-31T00:00:00,0.9,2.0,122.22,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Araponga,2012-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Araponga,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Araponga,2014-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Araponga,2015-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Araponga,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Araponga,2017-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Araponga,2018-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Araponga,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Araponga,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Araponga,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caiana,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caiana,2013-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caiana,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caiana,2015-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caiana,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caiana,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caiana,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caiana,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caiana,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caiana,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caparao,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caparao,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caparao,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caparao,2015-12-31T00:00:00,1.02,1.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caparao,2016-12-31T00:00:00,1.92,1.0,47.92,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caparao,2017-12-31T00:00:00,1.45,1.0,31.03,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caparao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caparao,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caparao,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caparao,2021-12-31T00:00:00,0.9,2.0,122.22,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caputira,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caputira,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caputira,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caputira,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caputira,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caputira,2017-12-31T00:00:00,0.77,1.0,29.87,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caputira,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caputira,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caputira,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caputira,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Carangola,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Carangola,2013-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Carangola,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Carangola,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Carangola,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Carangola,2017-12-31T00:00:00,1.1,1.0,9.09,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Carangola,2018-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Carangola,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Carangola,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Carangola,2021-12-31T00:00:00,1.02,1.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caratinga,2012-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caratinga,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caratinga,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caratinga,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caratinga,2016-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caratinga,2017-12-31T00:00:00,1.46,1.0,31.51,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caratinga,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caratinga,2019-12-31T00:00:00,1.0,2.0,100.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caratinga,2020-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caratinga,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Chale,2012-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Chale,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Chale,2014-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Chale,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Chale,2016-12-31T00:00:00,1.42,2.0,40.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Chale,2017-12-31T00:00:00,0.93,1.0,7.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Chale,2018-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Chale,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Chale,2020-12-31T00:00:00,1.71,1.0,41.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Chale,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Divino,2012-12-31T00:00:00,1.32,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Divino,2013-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Divino,2014-12-31T00:00:00,0.89,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Divino,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Divino,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Divino,2017-12-31T00:00:00,1.19,1.0,15.97,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Divino,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Divino,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Divino,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Divino,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Durande,2012-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Durande,2013-12-31T00:00:00,2.22,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Durande,2014-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Durande,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Durande,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Durande,2017-12-31T00:00:00,1.33,2.0,50.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Durande,2018-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Durande,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Durande,2020-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Durande,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ervalia,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ervalia,2013-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ervalia,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ervalia,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ervalia,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ervalia,2017-12-31T00:00:00,1.36,1.0,26.47,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ervalia,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ervalia,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ervalia,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ervalia,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Espera_Feliz,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Espera_Feliz,2013-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Espera_Feliz,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Espera_Feliz,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Espera_Feliz,2016-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Espera_Feliz,2017-12-31T00:00:00,1.21,1.0,17.36,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Espera_Feliz,2018-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Espera_Feliz,2019-12-31T00:00:00,0.9,2.0,122.22,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Espera_Feliz,2020-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Espera_Feliz,2021-12-31T00:00:00,0.78,1.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Guimarania,2012-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Guimarania,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Guimarania,2014-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Guimarania,2015-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Guimarania,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Guimarania,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Guimarania,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Guimarania,2019-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Guimarania,2020-12-31T00:00:00,2.15,2.0,6.98,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Guimarania,2021-12-31T00:00:00,1.54,2.0,29.87,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Lajinha,2012-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Lajinha,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Lajinha,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Lajinha,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Lajinha,2016-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Lajinha,2017-12-31T00:00:00,1.04,1.0,3.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Lajinha,2018-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Lajinha,2019-12-31T00:00:00,1.06,1.0,5.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Lajinha,2020-12-31T00:00:00,1.92,1.0,47.92,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Lajinha,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Luisburgo,2012-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Luisburgo,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Luisburgo,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Luisburgo,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Luisburgo,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Luisburgo,2017-12-31T00:00:00,1.53,1.0,34.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Luisburgo,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Luisburgo,2019-12-31T00:00:00,1.64,2.0,21.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Luisburgo,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Luisburgo,2021-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhuacu,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhuacu,2013-12-31T00:00:00,1.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhuacu,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhuacu,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhuacu,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhuacu,2017-12-31T00:00:00,1.43,1.0,30.07,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhuacu,2018-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhuacu,2019-12-31T00:00:00,1.02,1.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhuacu,2020-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhuacu,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhumirim,2012-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhumirim,2013-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhumirim,2014-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhumirim,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhumirim,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhumirim,2017-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhumirim,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhumirim,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhumirim,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhumirim,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Martins_Soares,2012-12-31T00:00:00,2.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Martins_Soares,2013-12-31T00:00:00,1.98,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Martins_Soares,2014-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Martins_Soares,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Martins_Soares,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Martins_Soares,2017-12-31T00:00:00,1.45,2.0,37.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Martins_Soares,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Martins_Soares,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Martins_Soares,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Martins_Soares,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Matipo,2012-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Matipo,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Matipo,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Matipo,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Matipo,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Matipo,2017-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Matipo,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Matipo,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Matipo,2020-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Matipo,2021-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Miradouro,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Miradouro,2013-12-31T00:00:00,2.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Miradouro,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Miradouro,2015-12-31T00:00:00,1.59,1.0,37.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Miradouro,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Miradouro,2017-12-31T00:00:00,1.16,1.0,13.79,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Miradouro,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Miradouro,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Miradouro,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Miradouro,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Mutum,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Mutum,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Mutum,2014-12-31T00:00:00,1.14,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Mutum,2015-12-31T00:00:00,1.15,1.0,13.04,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Mutum,2016-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Mutum,2017-12-31T00:00:00,1.45,1.0,31.03,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Mutum,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Mutum,2019-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Mutum,2020-12-31T00:00:00,1.65,1.0,39.39,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Mutum,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Nova_Belem,2012-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Nova_Belem,2013-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Nova_Belem,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Nova_Belem,2015-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Nova_Belem,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Nova_Belem,2017-12-31T00:00:00,0.48,1.0,108.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Nova_Belem,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Nova_Belem,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Nova_Belem,2020-12-31T00:00:00,1.1,1.0,9.09,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Nova_Belem,2021-12-31T00:00:00,1.0,1.0,0.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Orizania,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Orizania,2013-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Orizania,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Orizania,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Orizania,2016-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Orizania,2017-12-31T00:00:00,1.47,1.0,31.97,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Orizania,2018-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Orizania,2019-12-31T00:00:00,1.38,2.0,44.93,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Orizania,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Orizania,2021-12-31T00:00:00,1.08,2.0,85.19,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Patos_de_Minas,2012-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Patos_de_Minas,2013-12-31T00:00:00,2.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Patos_de_Minas,2014-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Patos_de_Minas,2015-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Patos_de_Minas,2016-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Patos_de_Minas,2017-12-31T00:00:00,1.7,2.0,17.65,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Patos_de_Minas,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Patos_de_Minas,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Patos_de_Minas,2020-12-31T00:00:00,1.94,2.0,3.09,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Patos_de_Minas,2021-12-31T00:00:00,1.72,2.0,16.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Raul_Soares,2012-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Raul_Soares,2013-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Raul_Soares,2014-12-31T00:00:00,0.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Raul_Soares,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Raul_Soares,2016-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Raul_Soares,2017-12-31T00:00:00,0.86,1.0,16.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Raul_Soares,2018-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Raul_Soares,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Raul_Soares,2020-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Raul_Soares,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Reduto,2012-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Reduto,2013-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Reduto,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Reduto,2015-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Reduto,2016-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Reduto,2017-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Reduto,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Reduto,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Reduto,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Reduto,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Barbara_do_Leste,2012-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Barbara_do_Leste,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Barbara_do_Leste,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Barbara_do_Leste,2015-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Barbara_do_Leste,2016-12-31T00:00:00,1.02,1.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Barbara_do_Leste,2017-12-31T00:00:00,0.91,1.0,9.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Barbara_do_Leste,2018-12-31T00:00:00,1.74,1.0,42.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Barbara_do_Leste,2019-12-31T00:00:00,1.14,1.0,12.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Barbara_do_Leste,2020-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Barbara_do_Leste,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Margarida,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Margarida,2013-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Margarida,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Margarida,2015-12-31T00:00:00,1.56,1.0,35.9,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Margarida,2016-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Margarida,2017-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Margarida,2018-12-31T00:00:00,1.68,1.0,40.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Margarida,2019-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Margarida,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Margarida,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Rita_de_Minas,2012-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Rita_de_Minas,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Rita_de_Minas,2014-12-31T00:00:00,0.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Rita_de_Minas,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Rita_de_Minas,2016-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Rita_de_Minas,2017-12-31T00:00:00,1.25,1.0,20.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Rita_de_Minas,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Rita_de_Minas,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Rita_de_Minas,2020-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Rita_de_Minas,2021-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santana_do_Manhuacu,2012-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santana_do_Manhuacu,2013-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santana_do_Manhuacu,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santana_do_Manhuacu,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santana_do_Manhuacu,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santana_do_Manhuacu,2017-12-31T00:00:00,1.28,1.0,21.88,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santana_do_Manhuacu,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santana_do_Manhuacu,2019-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santana_do_Manhuacu,2020-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santana_do_Manhuacu,2021-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Domingos_das_Dores,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Domingos_das_Dores,2013-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Domingos_das_Dores,2014-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Domingos_das_Dores,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Domingos_das_Dores,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Domingos_das_Dores,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Domingos_das_Dores,2018-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Domingos_das_Dores,2019-12-31T00:00:00,1.3,2.0,53.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Domingos_das_Dores,2020-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Domingos_das_Dores,2021-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2012-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2013-12-31T00:00:00,1.74,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2015-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2016-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2017-12-31T00:00:00,1.7,1.0,41.18,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2020-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sericita,2012-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sericita,2013-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sericita,2014-12-31T00:00:00,1.08,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sericita,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sericita,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sericita,2017-12-31T00:00:00,1.12,1.0,10.71,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sericita,2018-12-31T00:00:00,1.8,1.0,44.44,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sericita,2019-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sericita,2020-12-31T00:00:00,2.52,1.0,60.32,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sericita,2021-12-31T00:00:00,0.9,1.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ubaporanga,2012-12-31T00:00:00,1.02,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ubaporanga,2013-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ubaporanga,2014-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ubaporanga,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ubaporanga,2016-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ubaporanga,2017-12-31T00:00:00,1.03,1.0,2.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ubaporanga,2018-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ubaporanga,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ubaporanga,2020-12-31T00:00:00,1.62,1.0,38.27,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ubaporanga,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Vermelho_Novo,2012-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Vermelho_Novo,2013-12-31T00:00:00,0.96,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Vermelho_Novo,2014-12-31T00:00:00,0.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Vermelho_Novo,2015-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Vermelho_Novo,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Vermelho_Novo,2017-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Vermelho_Novo,2018-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Vermelho_Novo,2019-12-31T00:00:00,1.08,1.0,7.41,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Vermelho_Novo,2020-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Vermelho_Novo,2021-12-31T00:00:00,0.96,1.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Caparao,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Jequitiba,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Araponga,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caiana,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caparao,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caputira,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Carangola,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caratinga,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Chale,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Divino,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Durande,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ervalia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Espera_Feliz,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Guimarania,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Lajinha,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Luisburgo,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhuacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhumirim,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Martins_Soares,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Matipo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Miradouro,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Mutum,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Nova_Belem,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Orizania,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Patos_de_Minas,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Raul_Soares,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Reduto,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Barbara_do_Leste,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Margarida,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Rita_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santana_do_Manhuacu,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Domingos_das_Dores,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sericita,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ubaporanga,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Vermelho_Novo,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Caparao,2023-12-31T00:00:00,1.32,1.4629929065704346,10.83,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Alto_Jequitiba,2023-12-31T00:00:00,1.08,1.1456366777420044,6.08,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Araponga,2023-12-31T00:00:00,1.5,1.395848035812378,-6.94,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caiana,2023-12-31T00:00:00,1.08,1.068810224533081,-1.04,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caparao,2023-12-31T00:00:00,1.32,1.319036841392517,-0.07,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caputira,2023-12-31T00:00:00,1.380065005417118,1.519201636314392,10.08,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Carangola,2023-12-31T00:00:00,1.140118343195266,1.1597156524658203,1.72,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Caratinga,2023-12-31T00:00:00,1.3799999999999997,1.5937559604644775,15.49,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Chale,2023-12-31T00:00:00,1.3799999999999997,1.4467886686325073,4.84,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Divino,2023-12-31T00:00:00,1.02,1.2838695049285889,25.87,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Durande,2023-12-31T00:00:00,1.3799999999999997,1.673082709312439,21.24,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ervalia,2023-12-31T00:00:00,1.380044843049327,1.3624200820922852,-1.28,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Espera_Feliz,2023-12-31T00:00:00,1.08,1.2635252475738525,16.99,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Guimarania,2023-12-31T00:00:00,1.7704600484261497,1.6792131662368774,-5.15,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Lajinha,2023-12-31T00:00:00,1.4399536768963517,1.4962732791900635,3.91,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Luisburgo,2023-12-31T00:00:00,1.44,1.626757264137268,12.97,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhuacu,2023-12-31T00:00:00,1.2,1.286231517791748,7.19,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Manhumirim,2023-12-31T00:00:00,1.44,1.395111322402954,-3.12,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Martins_Soares,2023-12-31T00:00:00,1.2,1.427844524383545,18.99,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Matipo,2023-12-31T00:00:00,1.25990675990676,1.2757618427276611,1.26,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Miradouro,2023-12-31T00:00:00,1.140084388185654,1.2217546701431274,7.16,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Mutum,2023-12-31T00:00:00,1.019985196150999,1.4299430847167969,40.19,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Nova_Belem,2023-12-31T00:00:00,0.96,1.016021490097046,5.84,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Orizania,2023-12-31T00:00:00,1.2,1.386589527130127,15.55,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Patos_de_Minas,2023-12-31T00:00:00,1.782452316076294,1.8652229309082031,4.64,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Raul_Soares,2023-12-31T00:00:00,1.5,1.1303139925003052,-24.65,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Reduto,2023-12-31T00:00:00,0.9,1.3830530643463135,53.67,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Barbara_do_Leste,2023-12-31T00:00:00,1.08,1.2499808073043823,15.74,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Margarida,2023-12-31T00:00:00,1.32,1.3003957271575928,-1.49,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santa_Rita_de_Minas,2023-12-31T00:00:00,1.019930675909879,1.213915467262268,19.02,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Santana_do_Manhuacu,2023-12-31T00:00:00,1.32,1.426035761833191,8.03,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Domingos_das_Dores,2023-12-31T00:00:00,1.296078431372549,1.2998418807983398,0.29,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sao_Joao_do_Manhuacu,2023-12-31T00:00:00,1.2,1.2415872812271118,3.47,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Sericita,2023-12-31T00:00:00,1.26,1.461600422859192,16.0,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Ubaporanga,2023-12-31T00:00:00,1.5,1.3595870733261108,-9.36,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 3 - Metodo 2 (2023),Vermelho_Novo,2023-12-31T00:00:00,1.5,1.0576939582824707,-29.49,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 3.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 6
learning_rate: 0.000124144500468605
encoder_hidden_size: 128
decoder_layers: 3
decoder_hidden_size: 64
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T09:59:12
IBGE V5 Cluster 4 - Metodo 1 (2023),Alpinopolis,2012-12-31T00:00:00,2.7,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Alpinopolis,2013-12-31T00:00:00,2.71,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Alpinopolis,2014-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Alpinopolis,2015-12-31T00:00:00,1.82,2.0,9.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Alpinopolis,2016-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Alpinopolis,2017-12-31T00:00:00,2.06,2.0,2.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Alpinopolis,2018-12-31T00:00:00,2.23,2.0,10.31,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Alpinopolis,2019-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Alpinopolis,2020-12-31T00:00:00,2.77,2.0,27.8,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Alpinopolis,2021-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Areado,2012-12-31T00:00:00,1.95,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Areado,2013-12-31T00:00:00,1.86,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Areado,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Areado,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Areado,2016-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Areado,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Areado,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Areado,2019-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Areado,2020-12-31T00:00:00,2.08,2.0,3.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Areado,2021-12-31T00:00:00,1.52,1.0,34.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Bom_Jesus_da_Penha,2012-12-31T00:00:00,2.58,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Bom_Jesus_da_Penha,2013-12-31T00:00:00,2.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Bom_Jesus_da_Penha,2014-12-31T00:00:00,2.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Bom_Jesus_da_Penha,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Bom_Jesus_da_Penha,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Bom_Jesus_da_Penha,2017-12-31T00:00:00,2.22,2.0,9.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Bom_Jesus_da_Penha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Bom_Jesus_da_Penha,2019-12-31T00:00:00,2.22,2.0,9.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Bom_Jesus_da_Penha,2020-12-31T00:00:00,1.99,2.0,0.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Bom_Jesus_da_Penha,2021-12-31T00:00:00,1.52,1.0,34.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Botelhos,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Botelhos,2013-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Botelhos,2014-12-31T00:00:00,1.74,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Botelhos,2015-12-31T00:00:00,1.44,2.0,38.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Botelhos,2016-12-31T00:00:00,1.2,2.0,66.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Botelhos,2017-12-31T00:00:00,1.88,1.0,46.81,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Botelhos,2018-12-31T00:00:00,2.07,1.0,51.69,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Botelhos,2019-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Botelhos,2020-12-31T00:00:00,1.72,2.0,16.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Botelhos,2021-12-31T00:00:00,1.26,2.0,58.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Cabo_Verde,2012-12-31T00:00:00,2.07,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Cabo_Verde,2013-12-31T00:00:00,2.04,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Cabo_Verde,2014-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Cabo_Verde,2015-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Cabo_Verde,2016-12-31T00:00:00,1.65,2.0,21.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Cabo_Verde,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Cabo_Verde,2018-12-31T00:00:00,1.61,2.0,24.22,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Cabo_Verde,2019-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Cabo_Verde,2020-12-31T00:00:00,2.25,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Cabo_Verde,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Campestre,2012-12-31T00:00:00,1.35,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Campestre,2013-12-31T00:00:00,1.31,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Campestre,2014-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Campestre,2015-12-31T00:00:00,1.91,1.0,47.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Campestre,2016-12-31T00:00:00,1.85,2.0,8.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Campestre,2017-12-31T00:00:00,2.11,2.0,5.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Campestre,2018-12-31T00:00:00,1.54,2.0,29.87,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Campestre,2019-12-31T00:00:00,1.55,2.0,29.03,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Campestre,2020-12-31T00:00:00,2.14,2.0,6.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Campestre,2021-12-31T00:00:00,1.29,1.0,22.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Conceicao_da_Aparecida,2012-12-31T00:00:00,1.95,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Conceicao_da_Aparecida,2013-12-31T00:00:00,1.7,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Conceicao_da_Aparecida,2014-12-31T00:00:00,1.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Conceicao_da_Aparecida,2015-12-31T00:00:00,2.52,2.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Conceicao_da_Aparecida,2016-12-31T00:00:00,1.99,2.0,0.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Conceicao_da_Aparecida,2017-12-31T00:00:00,1.85,2.0,8.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Conceicao_da_Aparecida,2018-12-31T00:00:00,2.03,2.0,1.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Conceicao_da_Aparecida,2019-12-31T00:00:00,1.97,2.0,1.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Conceicao_da_Aparecida,2020-12-31T00:00:00,2.24,2.0,10.71,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Conceicao_da_Aparecida,2021-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaranesia,2012-12-31T00:00:00,1.98,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaranesia,2013-12-31T00:00:00,1.52,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaranesia,2014-12-31T00:00:00,1.86,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaranesia,2015-12-31T00:00:00,1.65,2.0,21.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaranesia,2016-12-31T00:00:00,2.63,2.0,23.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaranesia,2017-12-31T00:00:00,1.36,2.0,47.06,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaranesia,2018-12-31T00:00:00,1.7,2.0,17.65,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaranesia,2019-12-31T00:00:00,1.26,2.0,58.73,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaranesia,2020-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaranesia,2021-12-31T00:00:00,1.03,1.0,2.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaxupe,2012-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaxupe,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaxupe,2014-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaxupe,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaxupe,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaxupe,2017-12-31T00:00:00,1.51,2.0,32.45,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaxupe,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaxupe,2019-12-31T00:00:00,1.32,2.0,51.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaxupe,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaxupe,2021-12-31T00:00:00,1.02,1.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Ibitiura_de_Minas,2012-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Ibitiura_de_Minas,2013-12-31T00:00:00,2.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Ibitiura_de_Minas,2014-12-31T00:00:00,2.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Ibitiura_de_Minas,2015-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Ibitiura_de_Minas,2016-12-31T00:00:00,1.76,2.0,13.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Ibitiura_de_Minas,2017-12-31T00:00:00,1.3,2.0,53.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Ibitiura_de_Minas,2018-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Ibitiura_de_Minas,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Ibitiura_de_Minas,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Ibitiura_de_Minas,2021-12-31T00:00:00,1.92,1.0,47.92,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Jacui,2012-12-31T00:00:00,1.65,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Jacui,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Jacui,2014-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Jacui,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Jacui,2016-12-31T00:00:00,1.7,1.0,41.18,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Jacui,2017-12-31T00:00:00,1.73,2.0,15.61,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Jacui,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Jacui,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Jacui,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Jacui,2021-12-31T00:00:00,0.96,2.0,108.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Juruaia,2012-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Juruaia,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Juruaia,2014-12-31T00:00:00,1.72,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Juruaia,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Juruaia,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Juruaia,2017-12-31T00:00:00,1.69,2.0,18.34,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Juruaia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Juruaia,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Juruaia,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Juruaia,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Nova_Resende,2012-12-31T00:00:00,2.88,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Nova_Resende,2013-12-31T00:00:00,2.18,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Nova_Resende,2014-12-31T00:00:00,1.7,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Nova_Resende,2015-12-31T00:00:00,2.2,2.0,9.09,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Nova_Resende,2016-12-31T00:00:00,2.08,2.0,3.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Nova_Resende,2017-12-31T00:00:00,1.76,2.0,13.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Nova_Resende,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Nova_Resende,2019-12-31T00:00:00,2.12,2.0,5.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Nova_Resende,2020-12-31T00:00:00,2.35,2.0,14.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Nova_Resende,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2012-12-31T00:00:00,2.28,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2014-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2017-12-31T00:00:00,1.69,2.0,18.34,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2021-12-31T00:00:00,1.48,1.0,32.43,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Unai,2012-12-31T00:00:00,2.94,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Unai,2013-12-31T00:00:00,3.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Unai,2014-12-31T00:00:00,2.94,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Unai,2015-12-31T00:00:00,2.64,3.0,13.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Unai,2016-12-31T00:00:00,2.7,3.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Unai,2017-12-31T00:00:00,2.6,3.0,15.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Unai,2018-12-31T00:00:00,2.52,3.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Unai,2019-12-31T00:00:00,2.82,3.0,6.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Unai,2020-12-31T00:00:00,2.34,3.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Unai,2021-12-31T00:00:00,2.64,3.0,13.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Varjao_de_Minas,2012-12-31T00:00:00,2.4,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Varjao_de_Minas,2013-12-31T00:00:00,2.4,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Varjao_de_Minas,2014-12-31T00:00:00,3.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Varjao_de_Minas,2015-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Varjao_de_Minas,2016-12-31T00:00:00,2.4,3.0,25.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Varjao_de_Minas,2017-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Varjao_de_Minas,2018-12-31T00:00:00,2.4,3.0,25.0,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Varjao_de_Minas,2019-12-31T00:00:00,2.49,2.0,19.68,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Varjao_de_Minas,2020-12-31T00:00:00,2.52,3.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Varjao_de_Minas,2021-12-31T00:00:00,2.39,2.0,16.32,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Alpinopolis,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Areado,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Bom_Jesus_da_Penha,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Botelhos,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Cabo_Verde,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Campestre,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Conceicao_da_Aparecida,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaranesia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaxupe,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Ibitiura_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Jacui,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Juruaia,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Nova_Resende,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Unai,2022-12-31T00:00:00,3.0,2,-33.33,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Varjao_de_Minas,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Alpinopolis,2023-12-31T00:00:00,1.8000000000000005,1.6329524517059326,-9.28,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Areado,2023-12-31T00:00:00,1.590034364261168,1.3055634498596191,-17.89,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Bom_Jesus_da_Penha,2023-12-31T00:00:00,1.2,1.6150012016296387,34.58,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Botelhos,2023-12-31T00:00:00,1.080052666227781,1.1342333555221558,5.02,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Cabo_Verde,2023-12-31T00:00:00,1.319976635514019,1.4302003383636475,8.35,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Campestre,2023-12-31T00:00:00,1.319964428634949,1.2859498262405396,-2.58,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Conceicao_da_Aparecida,2023-12-31T00:00:00,1.679948420373952,1.631314754486084,-2.89,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaranesia,2023-12-31T00:00:00,1.1961439588688951,1.148538589477539,-3.98,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Guaxupe,2023-12-31T00:00:00,1.2,1.158155083656311,-3.49,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Ibitiura_de_Minas,2023-12-31T00:00:00,1.150344827586207,1.6034080982208252,39.38,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Jacui,2023-12-31T00:00:00,1.32,1.1046618223190308,-16.31,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Juruaia,2023-12-31T00:00:00,1.5,1.5136113166809082,0.91,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Nova_Resende,2023-12-31T00:00:00,1.56,1.4573824405670166,-6.58,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Sao_Pedro_da_Uniao,2023-12-31T00:00:00,1.5,1.535715937614441,2.38,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Unai,2023-12-31T00:00:00,2.520095187731359,2.4522087574005127,-2.69,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 1 (2023),Varjao_de_Minas,2023-12-31T00:00:00,2.3892857142857142,2.3932318687438965,0.17,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 4
learning_rate: 0.0002247275796032591
encoder_hidden_size: 192
decoder_layers: 2
decoder_hidden_size: 128
batch_size: 32
dropout: 0.4
weight_decay: 1e-05
steps: 200
",2025-09-04T10:00:36
IBGE V5 Cluster 4 - Metodo 2 (2023),Alpinopolis,2012-12-31T00:00:00,2.7,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Alpinopolis,2013-12-31T00:00:00,2.71,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Alpinopolis,2014-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Alpinopolis,2015-12-31T00:00:00,1.82,2.0,9.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Alpinopolis,2016-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Alpinopolis,2017-12-31T00:00:00,2.06,2.0,2.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Alpinopolis,2018-12-31T00:00:00,2.23,2.0,10.31,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Alpinopolis,2019-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Alpinopolis,2020-12-31T00:00:00,2.77,3.0,8.3,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Alpinopolis,2021-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Areado,2012-12-31T00:00:00,1.95,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Areado,2013-12-31T00:00:00,1.86,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Areado,2014-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Areado,2015-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Areado,2016-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Areado,2017-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Areado,2018-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Areado,2019-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Areado,2020-12-31T00:00:00,2.08,2.0,3.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Areado,2021-12-31T00:00:00,1.52,2.0,31.58,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Bom_Jesus_da_Penha,2012-12-31T00:00:00,2.58,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Bom_Jesus_da_Penha,2013-12-31T00:00:00,2.44,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Bom_Jesus_da_Penha,2014-12-31T00:00:00,2.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Bom_Jesus_da_Penha,2015-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Bom_Jesus_da_Penha,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Bom_Jesus_da_Penha,2017-12-31T00:00:00,2.22,2.0,9.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Bom_Jesus_da_Penha,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Bom_Jesus_da_Penha,2019-12-31T00:00:00,2.22,2.0,9.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Bom_Jesus_da_Penha,2020-12-31T00:00:00,1.99,2.0,0.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Bom_Jesus_da_Penha,2021-12-31T00:00:00,1.52,2.0,31.58,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Botelhos,2012-12-31T00:00:00,1.38,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Botelhos,2013-12-31T00:00:00,1.56,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Botelhos,2014-12-31T00:00:00,1.74,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Botelhos,2015-12-31T00:00:00,1.44,1.0,30.56,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Botelhos,2016-12-31T00:00:00,1.2,1.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Botelhos,2017-12-31T00:00:00,1.88,2.0,6.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Botelhos,2018-12-31T00:00:00,2.07,1.0,51.69,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Botelhos,2019-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Botelhos,2020-12-31T00:00:00,1.72,2.0,16.28,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Botelhos,2021-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Cabo_Verde,2012-12-31T00:00:00,2.07,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Cabo_Verde,2013-12-31T00:00:00,2.04,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Cabo_Verde,2014-12-31T00:00:00,1.68,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Cabo_Verde,2015-12-31T00:00:00,1.98,2.0,1.01,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Cabo_Verde,2016-12-31T00:00:00,1.65,2.0,21.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Cabo_Verde,2017-12-31T00:00:00,1.81,2.0,10.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Cabo_Verde,2018-12-31T00:00:00,1.61,2.0,24.22,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Cabo_Verde,2019-12-31T00:00:00,2.04,2.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Cabo_Verde,2020-12-31T00:00:00,2.25,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Cabo_Verde,2021-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Campestre,2012-12-31T00:00:00,1.35,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Campestre,2013-12-31T00:00:00,1.31,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Campestre,2014-12-31T00:00:00,1.26,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Campestre,2015-12-31T00:00:00,1.91,2.0,4.71,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Campestre,2016-12-31T00:00:00,1.85,1.0,45.95,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Campestre,2017-12-31T00:00:00,2.11,2.0,5.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Campestre,2018-12-31T00:00:00,1.54,2.0,29.87,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Campestre,2019-12-31T00:00:00,1.55,2.0,29.03,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Campestre,2020-12-31T00:00:00,2.14,2.0,6.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Campestre,2021-12-31T00:00:00,1.29,1.0,22.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Conceicao_da_Aparecida,2012-12-31T00:00:00,1.95,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Conceicao_da_Aparecida,2013-12-31T00:00:00,1.7,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Conceicao_da_Aparecida,2014-12-31T00:00:00,1.9,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Conceicao_da_Aparecida,2015-12-31T00:00:00,2.52,3.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Conceicao_da_Aparecida,2016-12-31T00:00:00,1.99,2.0,0.5,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Conceicao_da_Aparecida,2017-12-31T00:00:00,1.85,2.0,8.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Conceicao_da_Aparecida,2018-12-31T00:00:00,2.03,2.0,1.48,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Conceicao_da_Aparecida,2019-12-31T00:00:00,1.97,2.0,1.52,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Conceicao_da_Aparecida,2020-12-31T00:00:00,2.24,2.0,10.71,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Conceicao_da_Aparecida,2021-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaranesia,2012-12-31T00:00:00,1.98,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaranesia,2013-12-31T00:00:00,1.52,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaranesia,2014-12-31T00:00:00,1.86,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaranesia,2015-12-31T00:00:00,1.65,2.0,21.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaranesia,2016-12-31T00:00:00,2.63,3.0,14.07,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaranesia,2017-12-31T00:00:00,1.36,2.0,47.06,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaranesia,2018-12-31T00:00:00,1.7,2.0,17.65,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaranesia,2019-12-31T00:00:00,1.26,1.0,20.63,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaranesia,2020-12-31T00:00:00,1.56,2.0,28.21,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaranesia,2021-12-31T00:00:00,1.03,1.0,2.91,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaxupe,2012-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaxupe,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaxupe,2014-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaxupe,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaxupe,2016-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaxupe,2017-12-31T00:00:00,1.51,1.0,33.77,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaxupe,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaxupe,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaxupe,2020-12-31T00:00:00,1.74,2.0,14.94,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaxupe,2021-12-31T00:00:00,1.02,1.0,1.96,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Ibitiura_de_Minas,2012-12-31T00:00:00,1.8,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Ibitiura_de_Minas,2013-12-31T00:00:00,2.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Ibitiura_de_Minas,2014-12-31T00:00:00,2.1,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Ibitiura_de_Minas,2015-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Ibitiura_de_Minas,2016-12-31T00:00:00,1.76,2.0,13.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Ibitiura_de_Minas,2017-12-31T00:00:00,1.3,1.0,23.08,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Ibitiura_de_Minas,2018-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Ibitiura_de_Minas,2019-12-31T00:00:00,1.32,1.0,24.24,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Ibitiura_de_Minas,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Ibitiura_de_Minas,2021-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Jacui,2012-12-31T00:00:00,1.65,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Jacui,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Jacui,2014-12-31T00:00:00,1.5,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Jacui,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Jacui,2016-12-31T00:00:00,1.7,2.0,17.65,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Jacui,2017-12-31T00:00:00,1.73,2.0,15.61,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Jacui,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Jacui,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Jacui,2020-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Jacui,2021-12-31T00:00:00,0.96,2.0,108.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Juruaia,2012-12-31T00:00:00,1.92,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Juruaia,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Juruaia,2014-12-31T00:00:00,1.72,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Juruaia,2015-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Juruaia,2016-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Juruaia,2017-12-31T00:00:00,1.69,2.0,18.34,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Juruaia,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Juruaia,2019-12-31T00:00:00,1.68,2.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Juruaia,2020-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Juruaia,2021-12-31T00:00:00,1.5,2.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Nova_Resende,2012-12-31T00:00:00,2.88,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Nova_Resende,2013-12-31T00:00:00,2.18,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Nova_Resende,2014-12-31T00:00:00,1.7,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Nova_Resende,2015-12-31T00:00:00,2.2,2.0,9.09,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Nova_Resende,2016-12-31T00:00:00,2.08,2.0,3.85,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Nova_Resende,2017-12-31T00:00:00,1.76,2.0,13.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Nova_Resende,2018-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Nova_Resende,2019-12-31T00:00:00,2.12,2.0,5.66,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Nova_Resende,2020-12-31T00:00:00,2.35,2.0,14.89,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Nova_Resende,2021-12-31T00:00:00,1.38,1.0,27.54,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2012-12-31T00:00:00,2.28,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2013-12-31T00:00:00,1.2,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2014-12-31T00:00:00,1.62,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2015-12-31T00:00:00,1.5,1.0,33.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2016-12-31T00:00:00,1.92,2.0,4.17,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2017-12-31T00:00:00,1.69,2.0,18.34,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2018-12-31T00:00:00,1.8,2.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2019-12-31T00:00:00,1.62,2.0,23.46,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2020-12-31T00:00:00,2.1,2.0,4.76,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2021-12-31T00:00:00,1.48,1.0,32.43,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Unai,2012-12-31T00:00:00,2.94,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Unai,2013-12-31T00:00:00,3.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Unai,2014-12-31T00:00:00,2.94,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Unai,2015-12-31T00:00:00,2.64,3.0,13.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Unai,2016-12-31T00:00:00,2.7,3.0,11.11,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Unai,2017-12-31T00:00:00,2.6,3.0,15.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Unai,2018-12-31T00:00:00,2.52,3.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Unai,2019-12-31T00:00:00,2.82,3.0,6.38,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Unai,2020-12-31T00:00:00,2.34,2.0,14.53,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Unai,2021-12-31T00:00:00,2.64,3.0,13.64,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Varjao_de_Minas,2012-12-31T00:00:00,2.4,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Varjao_de_Minas,2013-12-31T00:00:00,2.4,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Varjao_de_Minas,2014-12-31T00:00:00,3.0,-,-,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Varjao_de_Minas,2015-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Varjao_de_Minas,2016-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Varjao_de_Minas,2017-12-31T00:00:00,2.4,2.0,16.67,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Varjao_de_Minas,2018-12-31T00:00:00,2.4,1.0,58.33,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Varjao_de_Minas,2019-12-31T00:00:00,2.49,2.0,19.68,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Varjao_de_Minas,2020-12-31T00:00:00,2.52,3.0,19.05,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Varjao_de_Minas,2021-12-31T00:00:00,2.39,2.0,16.32,treino,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Alpinopolis,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Areado,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Bom_Jesus_da_Penha,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Botelhos,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Cabo_Verde,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Campestre,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Conceicao_da_Aparecida,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaranesia,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaxupe,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Ibitiura_de_Minas,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Jacui,2022-12-31T00:00:00,1.0,1,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Juruaia,2022-12-31T00:00:00,2.0,1,-50.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Nova_Resende,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2022-12-31T00:00:00,1.0,2,100.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Unai,2022-12-31T00:00:00,3.0,3,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Varjao_de_Minas,2022-12-31T00:00:00,2.0,2,0.0,validacao,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Alpinopolis,2023-12-31T00:00:00,1.8000000000000003,1.4585845470428467,-18.97,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Areado,2023-12-31T00:00:00,1.590034364261168,0.9988711476325989,-37.18,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Bom_Jesus_da_Penha,2023-12-31T00:00:00,1.2,1.9685810804367065,64.05,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Botelhos,2023-12-31T00:00:00,1.080052666227781,0.8322046995162964,-22.95,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Cabo_Verde,2023-12-31T00:00:00,1.319976635514019,1.0356115102767944,-21.54,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Campestre,2023-12-31T00:00:00,1.319964428634949,1.0866343975067139,-17.68,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Conceicao_da_Aparecida,2023-12-31T00:00:00,1.679948420373952,1.408568263053894,-16.15,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaranesia,2023-12-31T00:00:00,1.1961439588688951,0.6235271692276001,-47.87,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Guaxupe,2023-12-31T00:00:00,1.2,0.7382118701934814,-38.48,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Ibitiura_de_Minas,2023-12-31T00:00:00,1.150344827586207,1.9558186531066895,70.02,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Jacui,2023-12-31T00:00:00,1.32,0.7730380296707153,-41.44,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Juruaia,2023-12-31T00:00:00,1.5,1.059593915939331,-29.36,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Nova_Resende,2023-12-31T00:00:00,1.56,1.069454312324524,-31.45,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Sao_Pedro_da_Uniao,2023-12-31T00:00:00,1.5,1.1514770984649658,-23.23,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Unai,2023-12-31T00:00:00,2.520095187731359,2.6141464710235596,3.73,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
IBGE V5 Cluster 4 - Metodo 2 (2023),Varjao_de_Minas,2023-12-31T00:00:00,2.3892857142857142,2.442753314971924,2.24,teste,V25 - Cluster V5,LSTM,"Modelo LSTM treinado com dados de 2012 a 2021, validado com os dados de 2022 e testado com os dados de 2023.
Para esse treinamento foi utilizado o dataset V25, que possui 7 clusters, sendo esse modelo referente ao Cluster 4.
O modelo foi treinado com as seguintes configurações:
input_size: 3

encoder_n_layers = 3
learning_rate: 0.009367460210401406
encoder_hidden_size: 256
decoder_layers: 3
decoder_hidden_size: 256
batch_size: 32
dropout: 0.4
weight_decay: 0.0001
steps: 100
",2025-09-04T10:01:24
