{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb787a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pip install --no-cache-dir \\\n",
    "    ipywidgets==8.1.8 \\\n",
    "    lightning==2.6.0 \\\n",
    "    lightning-utilities==0.15.2 \\\n",
    "    nbformat==5.10.4 \\\n",
    "    neuralforecast==3.1.2 \\\n",
    "    numpy==2.2.6 \\\n",
    "    openpyxl==3.1.5 \\\n",
    "    optuna==4.6.0 \\\n",
    "    plotly==6.5.0 \\\n",
    "    pytorch-lightning==2.6.0 \\\n",
    "    scikit-learn==1.7.2 \\\n",
    "    statsforecast==2.0.3 \\\n",
    "    torch==2.9.1 \\\n",
    "    torchmetrics==1.8.2\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c298a4",
   "metadata": {},
   "source": [
    "# Fun√ß√µes auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce15b9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralforecast.losses.pytorch import HuberLoss\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from neuralforecast import NeuralForecast\n",
    "from IPython.display import clear_output\n",
    "from torch.utils.data import DataLoader\n",
    "from neuralforecast.models import LSTM\n",
    "from typing import Dict, Any, Optional\n",
    "from datetime import datetime\n",
    "from torch.optim import AdamW\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import yaml\n",
    "import sys\n",
    "import os\n",
    "\n",
    "global_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4dadee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define as Seed Globais para Reprodutibilidade\n",
    "\n",
    "# =============================================================================\n",
    "# 1. FUN√á√ïES AUXILIARES (IGUAIS AO ANTERIOR)\n",
    "# =============================================================================\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def set_reproducibility(seed=42):\n",
    "    print(f\"üîí Aplicando configura√ß√µes de reprodutibilidade total (Seed: {seed})...\")\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8' \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.set_float32_matmul_precision('medium') # 'high' usa TF32 que pode variar. 'medium' (default) ou 'highest' s√£o mais est√°veis.\n",
    "    \n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "# =============================================================================\n",
    "# 2. O PATCH CIR√öRGICO (INJE√á√ÉO DE M√âTODO)\n",
    "# =============================================================================\n",
    "# Aqui est√° a diferen√ßa: n√£o criamos uma classe nova.\n",
    "# N√≥s entramos dentro da classe DataLoader do PyTorch e trocamos o c√©rebro dela.\n",
    "\n",
    "# Verifica se j√° n√£o aplicamos o patch para evitar recurs√£o infinita se rodar a c√©lula 2x\n",
    "if not hasattr(DataLoader, '_original_init'):\n",
    "    print(\"üîß Injetando c√≥digo determin√≠stico no DataLoader original...\")\n",
    "    \n",
    "    # Salva o __init__ original numa gaveta segura\n",
    "    DataLoader._original_init = DataLoader.__init__\n",
    "\n",
    "    # Define a nova fun√ß√£o que vai substituir o construtor\n",
    "    def deterministic_init(self, *args, **kwargs):\n",
    "        # 1. Inje√ß√£o do Generator\n",
    "        if 'generator' not in kwargs:\n",
    "            g = torch.Generator()\n",
    "            # Usa o seed atual para criar o generator\n",
    "            g.manual_seed(torch.initial_seed()) \n",
    "            kwargs['generator'] = g\n",
    "        \n",
    "        # 2. Inje√ß√£o do Worker Init\n",
    "        if kwargs.get('num_workers', 0) > 0 and 'worker_init_fn' not in kwargs:\n",
    "            kwargs['worker_init_fn'] = seed_worker\n",
    "            \n",
    "        # 3. Chama o __init__ original que salvamos\n",
    "        # Isso garante que a NeuralForecast continue funcionando como esperado\n",
    "        DataLoader._original_init(self, *args, **kwargs)\n",
    "\n",
    "    # Substitui o m√©todo na classe original\n",
    "    DataLoader.__init__ = deterministic_init\n",
    "    print(\"‚úÖ DataLoader 'hackeado' com sucesso (M√©todo In-Place).\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Patch j√° estava aplicado. Nenhuma a√ß√£o necess√°ria.\")\n",
    "\n",
    "# =============================================================================\n",
    "# 3. EXECU√á√ÉO\n",
    "# =============================================================================\n",
    "set_reproducibility(global_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d025e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classe para facilitar o uso de cores no terminal\n",
    "class CoresTerminal:\n",
    "    \"\"\"Cont√©m c√≥digos ANSI para colorir o output no terminal.\"\"\"\n",
    "    VERMELHO = '\\033[91m'\n",
    "    VERDE = '\\033[92m'\n",
    "    FIM = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f129e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ßao para avaliar previs√µes\n",
    "def evaluate_simple_forecast(\n",
    "    model,\n",
    "    train_df: pd.DataFrame,\n",
    "    test_df: pd.DataFrame,\n",
    "    model_name: str = 'LSTM',  \n",
    "    split: str =\"\",\n",
    "    inteiro: bool = False,\n",
    "    log_transformed: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Executa avalia√ß√£o de previs√£o considerando o nome do modelo din√¢mico.\n",
    "    Faz a previs√£o sem o uso de janela deslizande (rolling forecast).\n",
    "    \"\"\"\n",
    "    print(f\"\\n{split} Iniciando a previs√£o com o modelo: {model_name}...\")\n",
    "\n",
    "    # --- 1. Prepara√ß√£o dos Dados ---\n",
    "    history_df = train_df.copy()\n",
    "    future_data_to_evaluate = test_df.copy()\n",
    "\n",
    "    # Convers√£o de datas\n",
    "    history_df['ds'] = pd.to_datetime(history_df['ds'])\n",
    "    future_data_to_evaluate['ds'] = pd.to_datetime(future_data_to_evaluate['ds'])\n",
    "\n",
    "    # Filtra hist√≥rico para evitar vazamento de dados\n",
    "    cutoff = future_data_to_evaluate['ds'].min()\n",
    "    history_df = history_df[history_df['ds'] < cutoff]\n",
    "  \n",
    "    # Prepara df futuro para predict (sem y)\n",
    "    futr_df = future_data_to_evaluate.drop(columns=[\"y\"], errors='ignore')\n",
    "\n",
    "    # --- 2. Gera√ß√£o da Previs√£o ---\n",
    "    forecasts_df = model.predict(\n",
    "        df=history_df,\n",
    "        futr_df=futr_df\n",
    "    )\n",
    "    \n",
    "    # Verifica√ß√£o de seguran√ßa: A coluna do modelo existe?\n",
    "    if model_name not in forecasts_df.columns:\n",
    "        # Tenta fallback inteligente ou erro\n",
    "        cols_disponiveis = [c for c in forecasts_df.columns if c not in ['unique_id', 'ds']]\n",
    "        raise ValueError(f\"A coluna '{model_name}' n√£o foi encontrada na previs√£o. Colunas dispon√≠veis: {cols_disponiveis}\")\n",
    "\n",
    "    print(\"Previs√£o conclu√≠da. Combinando com dados reais...\")\n",
    "\n",
    "    # --- 3. P√≥s-processamento e Avalia√ß√£o ---\n",
    "    evaluation_df = forecasts_df.merge(\n",
    "        future_data_to_evaluate[[\"unique_id\", \"ds\", \"y\"]],\n",
    "        on=[\"unique_id\", \"ds\"],\n",
    "        how=\"inner\"\n",
    "    )\n",
    "\n",
    "    # Tratamento de Log (Revers√£o)\n",
    "    if log_transformed:\n",
    "        evaluation_df['y'] = np.expm1(evaluation_df['y'])\n",
    "        evaluation_df[model_name] = np.expm1(evaluation_df[model_name]) # Usa model_name\n",
    "\n",
    "    # Arredondamento\n",
    "    if inteiro:\n",
    "        evaluation_df[model_name] = evaluation_df[model_name].round().astype(int)\n",
    "        evaluation_df['y'] = evaluation_df['y'].round().astype(int)\n",
    "\n",
    "    # C√°lculo da Diferen√ßa Percentual (passando o nome da coluna)\n",
    "    evaluation_df = calcular_diferenca_percentual(evaluation_df, col_pred=model_name)\n",
    "\n",
    "    print(\"Avalia√ß√£o finalizada.\")\n",
    "    return evaluation_df\n",
    "\n",
    "\n",
    "\n",
    "def calcular_diferenca_percentual(df: pd.DataFrame, col_pred: str = 'LSTM', col_real: str = 'y') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calcula a diferen√ßa percentual absoluta entre previs√£o e valor real.\n",
    "    Retorna NaN para casos de divis√£o por zero ou dados ausentes (per√≠odo de treino).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 1. Garante que as colunas sejam num√©ricas\n",
    "    # 'errors=\"coerce\"' √© o segredo: ele transforma \"-\" e textos em NaN automaticamente\n",
    "    pred_numeric = pd.to_numeric(df[col_pred], errors='coerce')\n",
    "    real_numeric = pd.to_numeric(df[col_real], errors='coerce')\n",
    "    \n",
    "    # 2. C√°lculo Vetorizado: |(Pred - Real) / Real| * 100\n",
    "    # O Pandas/Numpy lida com NaNs automaticamente (NaN em qualquer opera√ß√£o resulta em NaN)\n",
    "    diff_pct = ((pred_numeric - real_numeric) / real_numeric).abs() * 100\n",
    "    \n",
    "    # 3. Tratamento de Divis√£o por Zero\n",
    "    # O c√°lculo acima gera 'inf' (infinito) se o real for 0.\n",
    "    # Aqui substitu√≠mos 'inf' por NaN para manter a consist√™ncia.\n",
    "    df['diferen√ßa_%'] = diff_pct.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # 4. Arredondamento (opcional, para limpeza visual)\n",
    "    df['diferen√ßa_%'] = df['diferen√ßa_%'].round(2)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18821b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√£o que testa o modelo salvo e salva resultados em CSV\n",
    "def teste_modelo(\n",
    "    local,              # Path onde o modelo treinado est√° salvo\n",
    "    csv_dir,            # Path do arquivo CSV onde os resultados ser√£o acumulados\n",
    "    treino_id,          # ID √∫nico do experimento/treino\n",
    "    dataset,            # DataFrame completo (Treino + Valida√ß√£o + Teste)\n",
    "    train_ds,           # DataFrame apenas de Treino (usado para metadados)\n",
    "    val_ds,             # DataFrame de Valida√ß√£o (pode ser None)\n",
    "    test_ds,            # DataFrame de Teste\n",
    "    comentario,         # String com observa√ß√µes do analista\n",
    "    nome_dataset,       # Nome do dataset (ex: 'V43')\n",
    "    hiperparametros,    # Dicion√°rio com configs do modelo\n",
    "    incluir_treino=False # Flag pesada: se True, faz previs√£o no passado (In-Sample)\n",
    "):\n",
    "    \"\"\"\n",
    "    Carrega um modelo NeuralForecast salvo, gera previs√µes para Teste, Valida√ß√£o \n",
    "    e (opcionalmente) Treino, calcula m√©tricas de erro e salva tudo em um CSV hist√≥rico.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\n>>> Iniciando Teste do Modelo carregado de: {local}\")\n",
    "    \n",
    "    # Carrega o modelo pr√©-treinado\n",
    "    model = NeuralForecast.load(path=f\"{local}\")\n",
    "    \n",
    "    # Define colunas padr√£o para garantir consist√™ncia\n",
    "    colunas_finais = ['treino_id', 'unique_id', 'ds', 'y', 'y_pred', 'diferen√ßa_%', 'flag', 'dataset', 'modelo', 'comentario', 'data_treino']\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # FUN√á√ÉO AUXILIAR INTERNA\n",
    "    # -------------------------------------------------------------------------\n",
    "    def processar_dataframe(df_raw, flag_name):\n",
    "        \"\"\"\n",
    "        Padroniza o dataframe de previs√µes: renomeia colunas, calcula erro e ajusta datas.\n",
    "        \"\"\"\n",
    "        if df_raw is None or df_raw.empty:\n",
    "            return None\n",
    "        \n",
    "        df_proc = df_raw.copy()\n",
    "        \n",
    "        # Identifica dinamicamente a coluna de previs√£o (que n√£o seja unique_id, ds ou y)\n",
    "        # O NeuralForecast costuma nomear a coluna com o nome do modelo (ex: LSTM, NHITS)\n",
    "        cols_reservadas = ['unique_id', 'ds', 'y', 'cutoff']\n",
    "        candidatos_pred = [c for c in df_proc.columns if c not in cols_reservadas]\n",
    "        \n",
    "        # Se houver colunas de previs√£o, pega a primeira (assume modelo √∫nico aqui)\n",
    "        col_pred_name = candidatos_pred[0] if candidatos_pred else 'LSTM'\n",
    "        \n",
    "        # Padroniza nomes\n",
    "        df_proc = df_proc.rename(columns={col_pred_name: 'y_pred'})\n",
    "        \n",
    "        # Garante apenas as colunas essenciais\n",
    "        df_proc = df_proc[['unique_id', 'ds', 'y', 'y_pred']].copy()\n",
    "        \n",
    "        # Calcula Erro da diferen√ßa percentual\n",
    "        df_proc = calcular_diferenca_percentual(df_proc, col_pred='y_pred', col_real='y')\n",
    "        \n",
    "        # Metadados\n",
    "        df_proc['flag'] = flag_name\n",
    "        \n",
    "        # Padroniza√ß√£o de Data para String (ISO format) para salvar no CSV sem problemas\n",
    "        df_proc['ds'] = (pd.to_datetime(df_proc['ds'], errors='coerce')\n",
    "                         .fillna(pd.Timestamp.now())\n",
    "                         .dt.strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "        \n",
    "        return df_proc\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. PREVIS√ïES NO CONJUNTO DE TESTE (Obrigat√≥rio)\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"... Gerando previs√µes de Teste\")\n",
    "    preds_test = evaluate_simple_forecast(\n",
    "        model=model,\n",
    "        train_df=dataset,\n",
    "        test_df=test_ds,\n",
    "        split=\"Teste\"\n",
    "    )\n",
    "    dados_teste = processar_dataframe(preds_test, 'teste')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. PREVIS√ïES NO CONJUNTO DE VALIDA√á√ÉO (Se val_ds for diferente de None)\n",
    "    # -------------------------------------------------------------------------\n",
    "    dados_validacao = None\n",
    "    if val_ds is not None:\n",
    "        print(\"... Gerando previs√µes de Valida√ß√£o\")\n",
    "        preds_val = evaluate_simple_forecast(\n",
    "            model=model,\n",
    "            train_df=dataset,\n",
    "            test_df=val_ds,\n",
    "            split=\"Valida√ß√£o\"\n",
    "        )\n",
    "        dados_validacao = processar_dataframe(preds_val, 'validacao')\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 3. PREVIS√ïES NO CONJUNTO DE TREINO (In-Sample)\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Prepara o dataframe base de treino (revertendo log)\n",
    "    dados_treino = dataset[['unique_id', 'ds', 'y']].copy()\n",
    "    dados_treino['y'] = np.expm1(dados_treino['y']) # Revers√£o Log1p\n",
    "    \n",
    "    # Define data de corte para separar o que √© treino\n",
    "    if val_ds is not None:\n",
    "        data_inicio_corte = val_ds['ds'].min() # Se tiver dados de valida√ß√£o o treino √© anterior a ele.\n",
    "    else:\n",
    "        data_inicio_corte = dados_teste['ds'].min() # Se n√£o houver valida√ß√£o, usa o periodo anterio ao teste.\n",
    "        \n",
    "    # Filtra apenas datas anteriores ao corte (valida√ß√£o ou teste)\n",
    "    dados_treino = dados_treino[pd.to_datetime(dados_treino['ds']) < pd.to_datetime(data_inicio_corte)].copy()\n",
    "    \n",
    "    if incluir_treino:\n",
    "        print(\"... Iniciando 'predict_insample' (Isso pode demorar!)\")\n",
    "        # Gera previs√µes dentro da amostra de treino\n",
    "        insample_df = model.predict_insample(step_size=hiperparametros['h'])\n",
    "        \n",
    "        # Revers√£o da transforma√ß√£o logar√≠tmica nas previs√µes e valores reais retornados\n",
    "        insample_df['y'] = np.expm1(insample_df['y'])\n",
    "        \n",
    "        # Encontra colunas de previs√£o no insample (ex: LSTM, Autoformer)\n",
    "        cols_pred_insample = [c for c in insample_df.columns if c not in ['unique_id', 'ds', 'cutoff', 'y']]\n",
    "        col_model = cols_pred_insample[0] if cols_pred_insample else 'LSTM'\n",
    "        insample_df[col_model] = np.expm1(insample_df[col_model])\n",
    "\n",
    "        # Remove o per√≠odo de \"aquecimento\" (context window) onde as previs√µes s√£o ruins/inexistentes\n",
    "        contexto = hiperparametros.get('input_size', 0)\n",
    "        min_cutoff_valido = pd.to_datetime(insample_df['cutoff'].min()) + pd.DateOffset(years=contexto)\n",
    "        insample_df = insample_df[insample_df['cutoff'] >= min_cutoff_valido].copy()\n",
    "\n",
    "        # Merge para trazer a previs√£o para o dataframe de treino original\n",
    "        insample_preds = insample_df[['unique_id', 'ds', col_model]].rename(columns={col_model: 'y_pred'})\n",
    "        \n",
    "        # Garante tipos de dados compat√≠veis para o merge\n",
    "        dados_treino['ds'] = pd.to_datetime(dados_treino['ds'])\n",
    "        insample_preds['ds'] = pd.to_datetime(insample_preds['ds'])\n",
    "        \n",
    "        dados_treino = dados_treino.merge(insample_preds, on=['unique_id', 'ds'], how='left')\n",
    "        \n",
    "        # Processa erro e formata√ß√£o\n",
    "        dados_treino = calcular_diferenca_percentual(dados_treino, col_pred='y_pred', col_real='y')\n",
    "        dados_treino['flag'] = 'treino'\n",
    "        dados_treino['ds'] = dados_treino['ds'].dt.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "        \n",
    "    else:\n",
    "        # Se n√£o incluir treino, cria DF vazio estruturado para n√£o quebrar o concat\n",
    "        dados_treino = pd.DataFrame(columns=['unique_id', 'ds', 'y', 'y_pred', 'diferen√ßa_%', 'flag'])\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 4. CONSOLIDA√á√ÉO E METADADOS\n",
    "    # -------------------------------------------------------------------------\n",
    "    print(\"... Consolidando dados\")\n",
    "    \n",
    "    # Lista de dataframes v√°lidos (ignora None)\n",
    "    dfs_para_concatenar = [df for df in [dados_treino, dados_validacao, dados_teste] if df is not None and not df.empty]\n",
    "    dados_completos = pd.concat(dfs_para_concatenar, ignore_index=True)\n",
    "\n",
    "    # Adiciona Metadados Globais\n",
    "    dados_completos['treino_id'] = treino_id\n",
    "    dados_completos['dataset'] = nome_dataset\n",
    "    dados_completos['modelo'] = \"LSTM\"\n",
    "    dados_completos['data_treino'] = datetime.now().strftime('%Y-%m-%dT%H:%M:%S')\n",
    "\n",
    "    # Gera o texto do coment√°rio detalhado\n",
    "    def get_period_text(df_periodo):\n",
    "        if df_periodo is None or df_periodo.empty: return \"sem dados\"\n",
    "        min_year = df_periodo['ds'].dt.year.min()\n",
    "        max_year = df_periodo['ds'].dt.year.max()\n",
    "        return f\"{min_year}\" if min_year == max_year else f\"{min_year} a {max_year}\"\n",
    "\n",
    "    txt_val = get_period_text(val_ds)\n",
    "    txt_test = get_period_text(test_ds)\n",
    "    txt_train = get_period_text(train_ds)\n",
    "\n",
    "    full_comment = (\n",
    "        f\"{local}\\n\"\n",
    "        f\"Modelo LSTM. Treino: {txt_train}. Valida√ß√£o: {txt_val}. Teste: {txt_test}.\\n\"\n",
    "        f\"Dataset: {nome_dataset}.\\n\"\n",
    "        f\"Obs: {comentario}\\n\\n\"\n",
    "        f\"Hyperparams:\\n\"\n",
    "        f\"input_size: {hiperparametros.get('input_size')}\"\n",
    "        f\"h: {hiperparametros.get('h')}\"\n",
    "        f\"lr: {hiperparametros.get('learning_rate')}\"\n",
    "        f\"batch: {hiperparametros.get('batch_size')}\"\n",
    "        f\"encoder n layers: {hiperparametros.get('encoder_n_layers')}\"\n",
    "        f\"decoder layers: {hiperparametros.get('decoder_layers')}\"\n",
    "    )\n",
    "    dados_completos['comentario'] = full_comment\n",
    "\n",
    "    # Ordena√ß√£o final das colunas\n",
    "    # Garante que todas as colunas existem, se n√£o, cria com NaN\n",
    "    for col in colunas_finais:\n",
    "        if col not in dados_completos.columns:\n",
    "            dados_completos[col] = np.nan\n",
    "            \n",
    "    dados_completos = dados_completos[colunas_finais]\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 5. SALVAMENTO NO ARQUIVO CSV (APPEND)\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Verifica se o diret√≥rio existe, se n√£o, cria\n",
    "    pasta = os.path.dirname(csv_dir)\n",
    "    if pasta and not os.path.exists(pasta):\n",
    "        os.makedirs(pasta)\n",
    "\n",
    "    if not os.path.exists(csv_dir):\n",
    "        print(f\"Arquivo {csv_dir} n√£o existe. Criando novo arquivo.\")\n",
    "        df_final = dados_completos\n",
    "    else:\n",
    "        print(f\"Adicionando resultados ao arquivo existente: {csv_dir}\")\n",
    "        df_antigo = pd.read_csv(csv_dir)\n",
    "        # Concatena antigo com novo\n",
    "        df_final = pd.concat([df_antigo, dados_completos], ignore_index=True)\n",
    "\n",
    "    # Salva\n",
    "    df_final.to_csv(csv_dir, index=False)\n",
    "    \n",
    "    clear_output()\n",
    "    print(f\"‚úÖ Sucesso! Previs√µes e m√©tricas salvas em: {csv_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b789a65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para ler hiperpar√¢metros de um arquivo YAML do PyTorch Lightning\n",
    "\n",
    "def get_hyperparameters_from_yaml(yaml_path: str) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    L√™ um arquivo hparams.yaml gerado pelo PyTorch Lightning e extrai hiperpar√¢metros.\n",
    "    \n",
    "    Args:\n",
    "        yaml_path (str): Caminho para o arquivo .yaml.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dicion√°rio com os par√¢metros. Retorna valores padr√£o (None) para chaves ausentes.\n",
    "        None: Se o arquivo n√£o existir ou estiver corrompido.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Verifica√ß√£o de exist√™ncia do arquivo antes de tentar abrir\n",
    "    if not os.path.exists(yaml_path):\n",
    "        print(f\"ERRO: Arquivo n√£o encontrado: {yaml_path}\")\n",
    "        return None\n",
    "\n",
    "    # 2. Leitura do YAML\n",
    "    try:\n",
    "        with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "            # UnsafeLoader √© necess√°rio pois o PL salva tags de objetos Python (!!python/object...)\n",
    "            config_data = yaml.load(f, Loader=yaml.UnsafeLoader)\n",
    "            \n",
    "        if not config_data:\n",
    "            print(f\"AVISO: O arquivo {yaml_path} est√° vazio.\")\n",
    "            return None\n",
    "            \n",
    "    except yaml.YAMLError as e:\n",
    "        print(f\"ERRO: YAML corrompido ou inv√°lido: {e}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"ERRO: Falha inesperada ao ler {yaml_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Tratamento especial para dicion√°rios aninhados (optimizer_kwargs)\n",
    "    opt_kwargs = config_data.get('optimizer_kwargs') or {} # Garante que seja dict se for None\n",
    "    \n",
    "    hyperparameters = {\n",
    "        # Par√¢metros Estruturais\n",
    "        'encoder_n_layers': config_data.get('encoder_n_layers'),\n",
    "        'encoder_hidden_size': config_data.get('encoder_hidden_size'),\n",
    "        'decoder_layers': config_data.get('decoder_layers'),\n",
    "        'decoder_hidden_size': config_data.get('decoder_hidden_size'),\n",
    "        'input_size': config_data.get('input_size'),\n",
    "        \n",
    "        # Par√¢metros de Treino\n",
    "        'learning_rate': config_data.get('learning_rate'),\n",
    "        'batch_size': config_data.get('batch_size'),\n",
    "        'steps': config_data.get('max_steps'),\n",
    "        \n",
    "        # Mapeamentos com renomea√ß√£o\n",
    "        'dropout': config_data.get('encoder_dropout'), # Renomeia encoder_dropout -> dropout\n",
    "        'weight_decay': opt_kwargs.get('weight_decay', 0.0), # Pega de dentro do kwargs ou retorna 0\n",
    "        \n",
    "        # IMPORTANTE: O script de teste usa 'h' (horizonte).\n",
    "        # Tenta pegar 'h' ou 'horizon'. Se n√£o achar, tenta inferir ou deixa None.\n",
    "        'h': config_data.get('h', config_data.get('horizon')) \n",
    "    }\n",
    "\n",
    "    return hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da19024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para calcular a M√©dia Simples do WMAPE por munic√≠pio\n",
    "\n",
    "def calcular_media_wmape_simples(df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Calcula a M√©dia Simples do WMAPE por munic√≠pio (unique_id).\n",
    "    \n",
    "    Esta fun√ß√£o agrupa os dados por munic√≠pio, calcula o erro individual de cada um\n",
    "    e, por fim, tira a m√©dia desses erros. Isso significa que todos os munic√≠pios\n",
    "    t√™m o mesmo peso na m√©trica final, independentemente do volume de produ√ß√£o.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame contendo as colunas 'unique_id', 'y' (real) e 'LSTM' (previsto).\n",
    "        \n",
    "    Returns:\n",
    "        float: O valor m√©dio do WMAPE entre todos os munic√≠pios.\n",
    "    \"\"\"\n",
    "    \n",
    "    # -------------------------------------------------------------------------\n",
    "    # 1. Defini√ß√£o da l√≥gica de c√°lculo (Fun√ß√£o Interna)\n",
    "    # -------------------------------------------------------------------------\n",
    "    def _calcular_wmape_individual(grupo):\n",
    "        \"\"\"Calcula o WMAPE para um √∫nico grupo (munic√≠pio).\"\"\"\n",
    "        \n",
    "        # Converte para numpy array para garantir performance e opera√ß√µes vetoriais\n",
    "        valores_reais = grupo['y'].values\n",
    "        valores_previstos = grupo['LSTM'].values\n",
    "        \n",
    "        # Calcula o denominador: Soma absoluta dos valores reais\n",
    "        soma_reais_abs = np.sum(np.abs(valores_reais))\n",
    "        \n",
    "        # PROTE√á√ÉO: Evita divis√£o por zero.\n",
    "        # Se a soma dos reais for 0 (munic√≠pio sem produ√ß√£o), o erro √© considerado 0.\n",
    "        if soma_reais_abs == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        # Calcula o numerador: Soma absoluta das diferen√ßas (erros)\n",
    "        soma_erros_abs = np.sum(np.abs(valores_previstos - valores_reais))\n",
    "        \n",
    "        # Retorna o WMAPE deste munic√≠pio espec√≠fico (Numerador / Denominador)\n",
    "        return soma_erros_abs / soma_reais_abs\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # 2. Processamento Principal\n",
    "    # -------------------------------------------------------------------------\n",
    "    \n",
    "    # Agrupa os dados pelo ID do munic√≠pio ('unique_id').\n",
    "    # O m√©todo .apply() executa a fun√ß√£o interna para CADA munic√≠pio separadamente.\n",
    "    # O resultado (wmapes_por_municipio) ser√° uma lista/Series com o erro de cada ID.\n",
    "    wmapes_por_municipio = df.groupby('unique_id')[['y', 'LSTM']].apply(_calcular_wmape_individual)\n",
    "    \n",
    "    # Calcula a m√©dia simples de todos os WMAPEs individuais encontrados.\n",
    "    # Ex: (WMAPE_Mun_A + WMAPE_Mun_B + ...) / Total_Municipios\n",
    "    media_final = wmapes_por_municipio.mean()\n",
    "    \n",
    "    return media_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7017431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para filtrar datasets por integridade dos dados (manter apenas munic√≠pios todos os registros no per√≠odo de valida√ß√£o)\n",
    "from typing import Tuple\n",
    "def filtrar_datasets_por_integridade(\n",
    "    train_df: pd.DataFrame, \n",
    "    val_df: pd.DataFrame, \n",
    "    test_df: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Filtra os datasets de treino, valida√ß√£o e teste para manter apenas os\n",
    "    munic√≠pios ('unique_id') que possuem dados completos no per√≠odo de valida√ß√£o.\n",
    "    Ou seja, se valida√ß√£o tem 12 meses, mant√©m apenas os munic√≠pios que t√™m os 12 meses completos.\n",
    "\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): DataFrame de treino.\n",
    "        val_df (pd.DataFrame): DataFrame de valida√ß√£o. Usado como refer√™ncia para a verifica√ß√£o.\n",
    "        test_df (pd.DataFrame): DataFrame de teste.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]: Uma tupla contendo os \n",
    "        DataFrames de treino, valida√ß√£o e teste devidamente filtrados.\n",
    "    \"\"\"\n",
    "    print(\"--- Iniciando verifica√ß√£o de integridade dos dados ---\")\n",
    "    \n",
    "    # Usa o DataFrame de valida√ß√£o como refer√™ncia para a integridade.\n",
    "    # 1. Conta o n√∫mero de timestamps √∫nicos que cada munic√≠pio DEVERIA ter.\n",
    "    timestamps_esperados = len(val_df['ds'].unique())\n",
    "    if timestamps_esperados == 0:\n",
    "        print(f\"{CoresTerminal.VERMELHO}ALERTA: O conjunto de valida√ß√£o est√° vazio. Nenhum filtro ser√° aplicado.{CoresTerminal.FIM}\")\n",
    "        return train_df, val_df, test_df\n",
    "\n",
    "    # 2. Conta quantos timestamps √∫nicos cada munic√≠pio realmente POSSUI.\n",
    "    contagem_por_id = val_df.groupby('unique_id')['ds'].nunique()\n",
    "    \n",
    "    # 3. Identifica os munic√≠pios com dados completos.\n",
    "    ids_completos = contagem_por_id[contagem_por_id == timestamps_esperados].index\n",
    "    \n",
    "    # 4. Compara com o total de munic√≠pios para ver se a filtragem √© necess√°ria.\n",
    "    ids_originais = val_df['unique_id'].nunique()\n",
    "    \n",
    "    if len(ids_completos) < ids_originais:\n",
    "        total_removido = ids_originais - len(ids_completos)\n",
    "        \n",
    "        print(f\"{CoresTerminal.VERMELHO}\"\n",
    "              \"----------------------------------------------------------------------\\n\"\n",
    "              \"ALERTA: Inconsist√™ncia de dados encontrada.\\n\"\n",
    "              f\"Foram encontrados {total_removido} de {ids_originais} munic√≠pios com dados INCOMPLETOS no per√≠odo de valida√ß√£o.\\n\"\n",
    "              \"Todos os datasets ser√£o filtrados para manter apenas os munic√≠pios com dados completos.\\n\"\n",
    "              f\"----------------------------------------------------------------------{CoresTerminal.FIM}\")\n",
    "        \n",
    "        # Filtra TODOS os dataframes para manter apenas os munic√≠pios com dados completos.\n",
    "        # O uso de .copy() evita o SettingWithCopyWarning do pandas.\n",
    "        train_filtrado = train_df[train_df['unique_id'].isin(ids_completos)].copy()\n",
    "        val_filtrado = val_df[val_df['unique_id'].isin(ids_completos)].copy()\n",
    "        test_filtrado = test_df[test_df['unique_id'].isin(ids_completos)].copy()\n",
    "        \n",
    "        # Se ap√≥s a filtragem n√£o sobrar nenhum dado, interrompe a execu√ß√£o.\n",
    "        if val_filtrado.empty:\n",
    "            print(f\"{CoresTerminal.VERMELHO}ALERTA: Ap√≥s a filtragem, n√£o restaram dados v√°lidos. Retornando DataFrames vazios.{CoresTerminal.FIM}\")\n",
    "            return train_filtrado, val_filtrado, test_filtrado\n",
    "        \n",
    "        print(f\"Filtragem conclu√≠da. {len(ids_completos)} munic√≠pios mantidos.\")\n",
    "        return train_filtrado, val_filtrado, test_filtrado\n",
    "        \n",
    "    else:\n",
    "        print(f\"{CoresTerminal.VERDE}Verifica√ß√£o conclu√≠da. Todos os {ids_originais} munic√≠pios possuem dados completos no per√≠odo de valida√ß√£o.{CoresTerminal.FIM}\")\n",
    "        return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01280f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para carregar e processar o dataset\n",
    "\n",
    "def get_dataset(dataset_file):\n",
    "    \"\"\"\n",
    "    Carrega, processa e limpa o dataset, garantindo que todas as s√©ries temporais\n",
    "    para cada 'unique_id' estejam completas no intervalo de datas do dataset.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(dataset_file)\n",
    "    # --- Aplica transforma√ß√£o logar√≠tmica ---\n",
    "    log_cols = [\n",
    "        '√Årea colhida (Hectares)',\n",
    "        'target',\n",
    "        'precomediocafe', # a partir de novembro 2025 (V37)\n",
    "        '√Årea destinada √† colheita (Hectares)'\n",
    "    ]\n",
    "    existing_log_cols = [col for col in log_cols if col in dataset.columns]\n",
    "    if existing_log_cols:\n",
    "        dataset[existing_log_cols] = dataset[existing_log_cols].apply(lambda x: np.log1p(x))\n",
    "\n",
    "    # --- Renomea√ß√£o e formata√ß√£o para o padr√£o NeuralForecast ---\n",
    "    dataset = dataset.rename(columns={\n",
    "        \"municipio\": \"unique_id\",\n",
    "        \"ano\": \"ds\",\n",
    "        \"target\": \"y\"\n",
    "    })\n",
    "    # Ordenar por unique_id e ds (ano)\n",
    "    dataset = dataset.sort_values(by=[\"unique_id\", \"ds\"]).reset_index(drop=True)\n",
    "    dataset['ds'] = pd.to_datetime(dataset['ds'].astype(str) + '-12-31')\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871ab2cb",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b05e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"Dataset/V43\"\n",
    "\n",
    "# sempre coloque a data no inicio do nome.\n",
    "Local_treino =  \"Teste_reprodutibilidade_2\" # Nome do arquivo final com todas as predi√ß√µes\n",
    "nome_dataset = \"V43\"\n",
    "anos_validacao = [2023] #  Anos usados para valida√ß√£o (caso for validar com mais de um ano, coloque apenas o ano de inicio, pois o ano de teste sera o ano_val +h)\n",
    "\n",
    "comentario = \"\"\"teste de reprodutibilidade\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19502439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de parametros usados no modelo\n",
    "hiperparametros = { \n",
    "    'h': 1,\n",
    "    'input_size': -1, # Use -1 para janela de contexto m√°xima\n",
    "    'batch_size': 32,\n",
    "    'dropout': 0.3,\n",
    "    'encoder_n_layers': 4,\n",
    "    'learning_rate': 0.00013034723280377263,\n",
    "    'encoder_hidden_size': 192,\n",
    "    'decoder_layers': 1,\n",
    "    'decoder_hidden_size': 64,\n",
    "    'weight_decay': 0.0001,\n",
    "    'steps': 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffe29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ano_val in anos_validacao:\n",
    "    ano_teste = ano_val + hiperparametros['h'] # Define o ano de teste com base no horizonte. \n",
    "    dataset_path_ano = f\"{dataset_path}/{ano_teste}\"\n",
    "    lista_datasets = sorted(os.listdir(dataset_path_ano)) # Garantir sempre a mesma ordem dos datasets\n",
    "    for dataset_file in lista_datasets:\n",
    "        \n",
    "        treino_id = f\"{dataset_file[:-4]}_{ano_teste}\"\n",
    "        dataset = get_dataset(f\"{dataset_path_ano}/{dataset_file}\")   \n",
    "\n",
    "        # --- Coleta a lista variaveis exogenas ---\n",
    "        exog_list = [col for col in dataset.columns.tolist() if col not in [\"ds\", \"y\", \"unique_id\"]]\n",
    "\n",
    "        # --- Prepara√ß√£o do dataset de treino, valida√ß√£o e teste ---\n",
    "        ano_teste = ano_val + hiperparametros['h'] # Define o ano de teste com base no horizonte. \n",
    "        train_ds = dataset[dataset['ds'].dt.year < ano_val].copy()\n",
    "        val_ds = dataset[(dataset['ds'].dt.year >= ano_val) & (dataset['ds'].dt.year < ano_teste)].copy()\n",
    "        test_ds = dataset[dataset['ds'].dt.year >= ano_teste].copy()\n",
    "\n",
    "        # --- Aplica a fun√ß√£o de filtragem para garantir a consist√™ncia ---\n",
    "        clear_output()\n",
    "        print(f\"\\n== Iniciando processamento para o dataset: {dataset_file} com valida√ß√£o em {ano_val} e teste em {ano_teste} ===\\n\")\n",
    "        train_ds, val_ds, test_ds = filtrar_datasets_por_integridade(\n",
    "            train_df=train_ds,\n",
    "            val_df=val_ds,\n",
    "            test_df=test_ds\n",
    "        )\n",
    "        print(f\"Dados de Treino entre os anos de {train_ds['ds'].dt.year.min()} e {train_ds['ds'].dt.year.max()}: {len(train_ds)} registros\")\n",
    "        print(f\"Dados de Valida√ß√£o entre os anos de {val_ds['ds'].dt.year.min() if val_ds is not None else 'N/A'} e {val_ds['ds'].dt.year.max() if val_ds is not None else 'N/A'}: {len(val_ds) if val_ds is not None else 0} registros\")\n",
    "        print(f\"Dados de Teste entre os anos de {test_ds['ds'].dt.year.min()} e {test_ds['ds'].dt.year.max()}: {len(test_ds)} registros\") \n",
    "\n",
    "\n",
    "        # -- Define o tamanho da janela de contexto (input_size) ---\n",
    "        if hiperparametros['input_size'] == -1:\n",
    "            hiperparametros['input_size'] = len(train_ds['ds'].unique()) - hiperparametros['h'] # Define o tamanho da janela de contexto como o total de anos de treino menos o horizonte\n",
    "\n",
    "        print(f\"\\nTamanho da janela de contexto (input_size): {hiperparametros['input_size']}.\\n\")\n",
    "        set_reproducibility(global_seed) # Reseta o estado global para garantir que este modelo come√ße do \"zero absoluto\", independente do modelo anterior.\n",
    "\n",
    "        model = LSTM(\n",
    "            h=hiperparametros['h'],\n",
    "            input_size= hiperparametros['input_size'],\n",
    "            batch_size=hiperparametros['batch_size'],\n",
    "            scaler_type=\"revin\",\n",
    "            encoder_dropout=hiperparametros['dropout'],\n",
    "            encoder_n_layers=hiperparametros['encoder_n_layers'],\n",
    "            encoder_hidden_size=hiperparametros['encoder_hidden_size'],\n",
    "            decoder_layers=hiperparametros['decoder_layers'],\n",
    "            decoder_hidden_size=hiperparametros['decoder_hidden_size'],\n",
    "            futr_exog_list=exog_list,\n",
    "            learning_rate=hiperparametros['learning_rate'],\n",
    "            max_steps=hiperparametros['steps'],\n",
    "            loss=HuberLoss(delta=1.0),\n",
    "            optimizer=AdamW,\n",
    "            optimizer_kwargs={\"weight_decay\": hiperparametros['weight_decay']},\n",
    "            lr_scheduler=StepLR,\n",
    "            lr_scheduler_kwargs={\"step_size\": int(hiperparametros['steps'] * 0.5), \"gamma\": 0.1},\n",
    "            random_seed=global_seed\n",
    "        )\n",
    "\n",
    "        nf = NeuralForecast(models=[model], freq=\"YE\")\n",
    "        nf.fit(df=train_ds)\n",
    "\n",
    "        # --- Validando o modelo ---\n",
    "        combined_df = evaluate_simple_forecast(\n",
    "            model=nf,\n",
    "            train_df=train_ds,\n",
    "            test_df=val_ds\n",
    "        )\n",
    "\n",
    "        actual = combined_df[\"y\"]\n",
    "        predicted = combined_df[\"LSTM\"]\n",
    "\n",
    "        # Calcula as tr√™s vers√µes da m√©trica\n",
    "        score_media_simples = calcular_media_wmape_simples(combined_df)\n",
    "        score_rmse = root_mean_squared_error(actual, predicted)\n",
    "    \n",
    "        # Imprime o score de valida√ß√£o\n",
    "        print(f\"\\n\\n --- Resultados da Valida√ß√£o ---\")\n",
    "        print(f\"  - WMAPE: {score_media_simples:.4f}\")\n",
    "        print(f\"  - RMSE:  {score_rmse:.4f}\\n\\n\")\n",
    "\n",
    "        # --- Salvando o modelo ---        \n",
    "        local_save = f\"./Treinos/{Local_treino}/Modelos/{treino_id}_({ano_teste})\"\n",
    "        csv_dir = f\"./Treinos/{Local_treino}/{Local_treino}.csv\"\n",
    "        nf.save(\n",
    "            path= local_save,\n",
    "            overwrite=True)\n",
    "        print(f\"Modelo salvo em: {local_save}\\n\")\n",
    "        \n",
    "        # --- Avalia√ß√£o ---\n",
    "        teste_modelo(\n",
    "            local = local_save,                   # Path onde o modelo treinado est√° salvo\n",
    "            csv_dir = csv_dir,                    # Path do arquivo CSV onde os resultados ser√£o acumulados\n",
    "            treino_id = treino_id,                # ID √∫nico do experimento/treino\n",
    "            dataset = dataset,                    # DataFrame completo (Treino + Valida√ß√£o + Teste)\n",
    "            train_ds = train_ds,                  # DataFrame apenas de Treino (usado para metadados)\n",
    "            val_ds = val_ds,                      # DataFrame de Valida√ß√£o (pode ser None)\n",
    "            test_ds = test_ds,                    # DataFrame de Teste\n",
    "            comentario = comentario,              # String com observa√ß√µes do analista\n",
    "            nome_dataset = nome_dataset,          # Nome do dataset (ex: 'V43')\n",
    "            hiperparametros = hiperparametros,    # Dicion√°rio com configs do modelo\n",
    "            incluir_treino=False                  # Se True, faz previs√£o no periodo de treino (In-Sample) (mais lento)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04a8044",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Resultado agregado dos clusters:\\n\")\n",
    "\n",
    "# Constantes\n",
    "SACA_KG = 60\n",
    "PATH_REF = \"Treinos/Modelo_Altitude_Ajustado/modelo_altitude_ajustado.csv\"\n",
    "PATH_AREA = \"Dataset/Area_colhida_V2.csv\"\n",
    "\n",
    "df_area = pd.read_csv(PATH_AREA)\n",
    "\n",
    "def processar_modelo(csv_path, df_area, colunas_para_converter):\n",
    "    \"\"\"\n",
    "    L√™ o CSV, filtra teste, faz merge com √°rea, converte para sacas e agrupa por ano.\n",
    "    \"\"\"\n",
    "    # Leitura e Pr√©-processamento b√°sico\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[df['flag'] == 'teste'].copy()\n",
    "    \n",
    "    # Ajuste de Datas e Nomes\n",
    "    df['Ano'] = pd.to_datetime(df['ds']).dt.year\n",
    "    df.rename(columns={\"unique_id\": \"Municipio\"}, inplace=True)\n",
    "    \n",
    "    # Merge com √Årea\n",
    "    df = df.merge(df_area, on=['Municipio', 'Ano'], how='left')\n",
    "    \n",
    "    # C√°lculo Vetorizado (Sem loops)\n",
    "    fator_conversao = (df['Area'] * 1000) / SACA_KG\n",
    "    \n",
    "    for col in colunas_para_converter:\n",
    "        valores = pd.to_numeric(df[col], errors='coerce')\n",
    "        df[col] = valores * fator_conversao\n",
    "        \n",
    "    # Agrupa por ano e soma\n",
    "    return df.groupby('Ano')[colunas_para_converter].sum()\n",
    "\n",
    "# 1. Processa Modelo Atual\n",
    "resultados_atual = processar_modelo(csv_dir, df_area, ['y', 'y_pred'])\n",
    "\n",
    "# 2. Processa Modelo de Refer√™ncia\n",
    "resultados_ref = processar_modelo(PATH_REF, df_area, ['y_pred'])\n",
    "resultados_ref = resultados_ref.rename(columns={'y_pred': 'Refer√™ncia'})\n",
    "\n",
    "# 3. Junta os resultados\n",
    "df_final = pd.concat([resultados_atual, resultados_ref], axis=1).round(3)\n",
    "\n",
    "# Fun√ß√£o de formata√ß√£o no padr√£o brasileiro\n",
    "def formatar_br(x):\n",
    "    if pd.isna(x):\n",
    "        return \"‚Äî\"\n",
    "    int_part = int(x)\n",
    "    dec_part = x - int_part\n",
    "    int_str = f\"{int_part:,}\".replace(\",\", \".\")\n",
    "    dec_str = f\"{abs(dec_part):.3f}\".split('.')[1]\n",
    "    return f\"{int_str},{dec_str}\"\n",
    "\n",
    "# Aplica estilo com formata√ß√£o personalizada\n",
    "styled_df = (\n",
    "    df_final\n",
    "    .style\n",
    "    .format(formatter=formatar_br, na_rep=\"‚Äî\")\n",
    "    .set_properties(**{'text-align': 'right'})\n",
    "    .set_table_styles([\n",
    "        {'selector': 'th', 'props': [('text-align', 'right')]},\n",
    "        {'selector': '', 'props': [('border', '1px solid #ccc')]}\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Exibe no notebook\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4cbfce",
   "metadata": {},
   "source": [
    "# Inferencia pos treino\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44493a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def converter_colunas_para_minusculo(df):\n",
    "    \"\"\"\n",
    "    Fun√ß√£o que converte os nomes das colunas de um dataframe para min√∫sculas\n",
    "\n",
    "    Par√¢metros:\n",
    "    - df: DataFrame que ter√° as colunas convertidas para min√∫sculas\n",
    "\n",
    "    Retorna:\n",
    "    - DataFrame com colunas em min√∫sculas\n",
    "    \"\"\"\n",
    "    df_renomeado = df.rename(columns=lambda col: col.lower())\n",
    "    return df_renomeado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efbdd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,200):\n",
    "    local = f\"Logs/trial_{i}/version_0\"\n",
    "    if not os.path.exists(local):\n",
    "        os.makedirs(f\"Logs/trial_{i}/version_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e9a35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame()\n",
    "ano = 2021\n",
    "for cluster in range(5):   \n",
    "    dataset = get_dataset(f\"Dataset/V29 - Cluster 5/dataset_V29_cluster_{cluster}_metodo_1.csv\") \n",
    "    model = NeuralForecast.load(path=f\"Treinos/Antigos/Antigos_Antigos/Teste_Clusters/Modelos_antigos/IBGE - Cluster V5 Cluster {cluster} (2025)\")\n",
    "\n",
    "    #if ano == 2025:\n",
    "    dataset = converter_colunas_para_minusculo(dataset)\n",
    "        \n",
    "    test_ds = dataset[dataset['ds'].dt.year == ano].copy()\n",
    "    predictions = evaluate_simple_forecast(\n",
    "        model=model,\n",
    "        train_df=dataset,\n",
    "        test_df=test_ds,\n",
    "        split = \"Teste\"\n",
    "    )\n",
    "\n",
    "    dados_teste = predictions[['unique_id', 'ds', 'y', 'LSTM', 'diferen√ßa_%']].copy()\n",
    "    dados_teste.columns = ['unique_id', 'ds', 'y', 'y_pred', 'diferen√ßa_%'] # Renomeando as colunas para o formato esperado\n",
    "    dados_teste['flag'] = 'teste'\n",
    "    dados_teste['cluster'] = cluster\n",
    "    dados_teste['ds'] = pd.to_datetime(dados_teste['ds']).fillna(dados_teste['ds'])\n",
    "    dados_teste['ds'] = (pd.to_datetime(dados_teste['ds'], errors='coerce').fillna(pd.Timestamp.now()).dt.strftime('%Y-%m-%dT%H:%M:%S'))\n",
    "    df_final = pd.concat([df_final, dados_teste], ignore_index=True)\n",
    "\n",
    "    \n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c446971",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_area = pd.read_csv(\"Dataset/Area_colhida_V2.csv\")\n",
    "#df_area = pd.read_csv(\"Dataset/Area_colhida_projesao_2026.csv\")\n",
    "#df_area = pd.read_excel(\"Dataset/Proje√ß√µes de √°reas colhidas v2 - 2025 e 2026.xlsx\")\n",
    "\n",
    "df_area.rename(columns={\"√Årea colhida (Hectares)\": \"Area\"}, inplace=True)\n",
    "for unique_id in df_final['unique_id'].unique():\n",
    "    # Filtrar as linhas correspondentes no df_soma\n",
    "    mask_final = df_final['unique_id'] == unique_id\n",
    "\n",
    "    # Buscar √°rea correspondente\n",
    "    area_row = df_area[(df_area['Municipio'] == unique_id) & (df_area['Ano'] == ano)]\n",
    "    \n",
    "    if area_row.empty:\n",
    "        print(f\"Aviso: √Årea n√£o encontrada para unique_id = {unique_id}\")\n",
    "        area_total = np.nan  # ou continue, ou defina um valor padr√£o\n",
    "    else:\n",
    "        area_total = area_row['Area'].iloc[0]\n",
    "\n",
    "    # Aplicar a transforma√ß√£o SOMENTE √†s linhas desse unique_id\n",
    "    if pd.notna(area_total):\n",
    "        df_final.loc[mask_final, 'y_pred'] = (\n",
    "            (df_final.loc[mask_final, 'y_pred'] * area_total) * 1000\n",
    "        ) / 60\n",
    "    else:\n",
    "        df_final.loc[mask_final, 'y_pred'] = np.nan  # ou 0, dependendo da l√≥gica\n",
    "\n",
    "print(f\"Ano: {ano} -> {df_final['y_pred'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baaf982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de parametros usados no modelo\n",
    "hiperparametros = { \n",
    "    'h': 1,\n",
    "    'input_size': 3, # Use -1 para janela de contexto m√°xima\n",
    "    'batch_size': 32,\n",
    "    'dropout': 0.3,\n",
    "    'encoder_n_layers': 64,\n",
    "    'learning_rate': 0.0001092083381402252,\n",
    "    'encoder_hidden_size': 128,\n",
    "    'decoder_layers': 2,\n",
    "    'decoder_hidden_size': 128,\n",
    "    'weight_decay': 0.0001,\n",
    "    'steps': 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dc88d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster = 0\n",
    "ano = 2021\n",
    "#for ano in [2020, 2021, 2022, 2023, 2024]:\n",
    "for cluster in range(5):\n",
    "    dataset = get_dataset(f\"Dataset/V29 - Cluster 5/dataset_V29_cluster_{cluster}_metodo_1.csv\") \n",
    "    #dataset = converter_colunas_para_minusculo(dataset)\n",
    "\n",
    "    local_modelo = f\"Treinos/Antigos/Antigos_Antigos/Teste_Clusters/Modelos_antigos/IBGE - Cluster V5 Metodo 1 Modelo unico (2021)\"\n",
    "    csv_dir = \"teste/IBGE_Cluster_V5_metodo_1_2021.csv\"\n",
    "    train_ds = dataset[dataset['ds'].dt.year < ano-1].copy()\n",
    "    val_ds = dataset[dataset['ds'].dt.year == ano-1].copy()\n",
    "    test_ds = dataset[dataset['ds'].dt.year == ano].copy()\n",
    "    comentario = \"dataset V29 com com as features escritas em minusculo. para padronizar de agora em diante.\"\n",
    "    nome_dataset = \"V29\"\n",
    "    treino_id = f\"IBGE - Cluster V5 Metodo 1 Cluster {cluster} ({ano})\"\n",
    "    teste_modelo(local_modelo, csv_dir, dataset, train_ds, val_ds, test_ds, comentario, nome_dataset, h=hiperparametros['h'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1680e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"teste/IBGE_Cluster_V5_metodo_1_2025.csv\")\n",
    "df_original = pd.read_csv(\"Treinoss/cluster_V5_Cluster_separados/cluster_V5_Cluster_separados.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66933b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_novo = pd.concat([df_original, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06afb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_novo[\"treino_id\"].unique()\n",
    "df_novo.to_csv(\"Treinoss/cluster_V5_Cluster_separados/cluster_V5_Cluster_separados_2024.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Educampo)",
   "language": "python",
   "name": "educampo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
